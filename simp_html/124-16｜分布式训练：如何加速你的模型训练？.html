
                <html lang="en" class="simpread-font simpread-theme-root" style='undefined'>
                    <head>
                        <meta charset="utf-8">
                        <meta http-equiv="content-type" content="text/html; charset=UTF-8;charset=utf-8">
                        <meta http-equiv="X-UA-Compatible" content="IE=Edge">
                        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1">
                        <meta name="author" content="Kenshin"/>
                        <meta name="description" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展" />
                        <meta name="keywords" content="Chrome extension, Chrome 扩展, 阅读模式, 沉浸式阅读, 简悦, 简阅, read mode, reading mode, reader view, firefox, firefox addon, userscript, safari, opera, tampermonkey"/>
                        <meta name="thumbnail" content="https://simpread-1254315611.cos.ap-shanghai.myqcloud.com/static/introduce-2.png"/>
                        <meta property="og:title" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展"/>
                        <meta property="og:type" content="website">
                        <meta property="og:local" content="zh_CN"/>
                        <meta property="og:url" content="http://ksria.com/simpread"/>
                        <meta property="og:image" content="https://simpread-1254315611.cos.ap-shanghai.myqcloud.com/static/introduce-2.png"/>
                        <meta property="og:image:type" content="image/png"/>
                        <meta property="og:image:width" content="960"/>
                        <meta property="og:image:height" content="355"/>
                        <meta property="og:site_name" content="http://ksria.com/simpread"/>
                        <meta property="og:description" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展"/>
                        <style type="text/css">.simpread-font{font:300 16px/1.8 -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#333;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased}.simpread-hidden{display:none}.simpread-read-root{display:-webkit-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;margin:0;top:-1000px;left:0;width:100%;z-index:2147483646;overflow-x:hidden;opacity:0;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}.simpread-read-root-show{top:0}.simpread-read-root-hide{top:1000px}sr-read{display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;margin:20px 20%;min-width:400px;min-height:400px;text-align:center}read-process{position:fixed;top:0;left:0;height:3px;width:100%;background-color:#64b5f6;-webkit-transition:width 2s;transition:width 2s;z-index:20000}sr-rd-content-error{display:block;position:relative;margin:0;margin-bottom:30px;padding:25px;background-color:rgba(0,0,0,.05)}sr-rd-footer{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;font-size:14px}sr-rd-footer,sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}sr-rd-footer-group{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}sr-rd-footer-text{min-width:150px}sr-rd-footer-copywrite{margin:10px 0 0;color:inherit}sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}sr-rd-footer-copywrite .second{margin:10px 0}sr-rd-footer-copywrite .third a:hover{border:none!important}sr-rd-footer-copywrite .third a:first-child{margin-right:50px}sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}sr-rd-footer-copywrite .sr-icon:hover{opacity:1}sr-rd-footer-copywrite a,sr-rd-footer-copywrite a:link,sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}sr-rd-footer-copywrite a:focus,sr-rd-footer-copywrite a:hover,sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.simpread-blocks{text-decoration:none!important}.simpread-blocks *{margin:0}.simpread-blocks a{padding:0;text-decoration:none!important}.simpread-blocks img{margin:0;padding:0;border:0;background:transparent;box-shadow:none}.simpread-focus-root{display:block;position:fixed;top:0;left:0;right:0;bottom:0;background-color:hsla(0,0%,92%,.9);z-index:2147483645;opacity:0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms}.simpread-focus-highlight{position:relative;box-shadow:0 0 0 20px #fff;background-color:#fff;overflow:visible;z-index:2147483646}.sr-controlbar-bg sr-rd-crlbar,.sr-controlbar-bg sr-rd-crlbar fab{z-index:2147483647}sr-rd-crlbar.controlbar{position:fixed;right:0;bottom:0;width:100px;height:100%;opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}sr-rd-crlbar.controlbar:hover{opacity:1}sr-rd-crlbar fap *{box-sizing:border-box}@media (max-height:620px){fab{zoom:.8}}@media (max-height:783px){dialog-gp dialog-content{max-height:580px}dialog-gp dialog-footer{border-top:1px solid #e0e0e0}}.simpread-highlight-selector{outline:3px dashed #1976d2!important;cursor:pointer!important}.simpread-highlight-controlbar,.simpread-highlight-selector{background-color:#fafafa!important;opacity:.8!important;-webkit-transition:opacity .5s ease!important;transition:opacity .5s ease!important}.simpread-highlight-controlbar{position:relative!important;border:3px dashed #1976d2!important}simpread-highlight,sr-snapshot-ctlbar{position:fixed;top:0;left:0;right:0;padding:15px;height:50px;background-color:rgba(50,50,50,.9);box-shadow:0 2px 5px rgba(0,0,0,.26);box-sizing:border-box;z-index:2147483640}simpread-highlight,sr-highlight-ctl,sr-snapshot-ctlbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-highlight-ctl{margin:0 5px;width:50px;height:20px;color:#fff;background-color:#1976d2;border-radius:4px;box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12);cursor:pointer}toc-bg{position:fixed;left:0;top:0;width:50px;height:200px;font-size:medium}toc-bg:hover{z-index:3}.toc-bg-hidden{opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}.toc-bg-hidden:hover{opacity:1;z-index:3}.toc-bg-hidden:hover toc{width:180px}toc *{all:unset}toc{position:fixed;left:0;top:100px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;padding:10px;width:0;max-width:200px;max-height:500px;overflow-x:hidden;overflow-y:hidden;cursor:pointer;border:1px solid hsla(0,0%,62%,.22);-webkit-transition:width .5s;transition:width .5s}toc:hover{overflow-y:auto}toc.mini:hover{width:200px!important}toc::-webkit-scrollbar{width:3px}toc::-webkit-scrollbar-thumb{border-radius:10px;background-color:hsla(36,2%,54%,.5)}toc outline{position:relative;display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;overflow:hidden;text-overflow:ellipsis;padding:2px 0;min-height:21px;line-height:21px;text-align:left}toc outline a,toc outline a:active,toc outline a:focus,toc outline a:visited{display:block;width:100%;color:inherit;font-size:11px;text-decoration:none!important;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}toc outline a:hover{font-weight:700!important}toc outline a.toc-outline-theme-dark,toc outline a.toc-outline-theme-night{color:#fff!important}.toc-level-h1{padding-left:5px}.toc-level-h2{padding-left:15px}.toc-level-h3{padding-left:25px}.toc-level-h4{padding-left:35px}.toc-outline-active{border-left:2px solid #f44336}toc outline active{position:absolute;left:0;top:0;bottom:0;padding:0 0 0 3px;border-left:2px solid #e8e8e8}sr-kbd{background:-webkit-gradient(linear,0 0,0 100%,from(#fff785),to(#ffc542));border:1px solid #e3be23;-o-border-image:none;border-image:none;-o-border-image:initial;border-image:initial;position:absolute;left:0;padding:1px 3px 0;font-size:11px!important;font-weight:700;box-shadow:0 3px 7px 0 rgba(0,0,0,.3);overflow:hidden;border-radius:3px}.sr-kbd-a{position:relative}kbd-mapping{position:fixed;left:5px;bottom:5px;-ms-flex-flow:row;flex-flow:row;width:250px;height:500px;background-color:#fff;border:1px solid hsla(0,0%,62%,.22);box-shadow:0 2px 5px rgba(0,0,0,.26);border-radius:3px}kbd-mapping,kbd-maps{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}kbd-maps{margin:40px 0 20px;width:100%;overflow-x:auto}kbd-maps::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}kbd-maps::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}kbd-mapping kbd-map-title{position:absolute;margin:5px 0;width:100%;font-size:14px;font-weight:700}kbd-maps-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}kbd-maps-title{margin:5px 0;padding-left:53px;font-size:12px;font-weight:700}kbd-map kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fafbfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}kbd-map kbd-name{display:inline-block;text-align:right;width:50px}kbd-map kbd-desc{padding-left:3px}sharecard-bg{position:fixed;top:0;left:0;width:100%;height:100%;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(0,0,0,.4);z-index:2147483647}sharecard{max-width:450px;background-color:#64b5f6}sharecard,sharecard-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-head{margin:25px;color:#fff;border-radius:10px;box-shadow:0 2px 6px 0 rgba(0,0,0,.2),0 25px 50px 0 rgba(0,0,0,.15)}sharecard-card{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-card,sharecard-top{display:-webkit-box;display:-ms-flexbox;display:flex}sharecard-top{-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:65px;background-color:#fff;color:#878787;font-size:25px;font-weight:500;border-top-left-radius:10px;border-top-right-radius:10px}sharecard-top span.logos{display:block;width:48px;height:48px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAMAAABg3Am1AAABU1BMVEUAAAAnNJMnNZI3Q5onNJInNJMnNJInNJMnNJI8SJ0tOZY/S55EUKAoNJI6RpwoNJNIU6InNJInNJImNJI7SJwmNJJ2fLUiMJFKVaNCTJ9faK1HUaJOWKVSXaUnNJNYY6pye7cmM5JXYKhwebMjMI8mL4719fW9vb0oNZP/UlLz8/QqN5TAwMAnNJPv7+/Pz8/q6+/p6enNzc3Kysry8vMsOJXc3env7/LU1uXo29vR0dHOzs7ExMTwjo73bW37XV3Aj1TCYELl5u3n5+fW2Obn6O7f4OrZ2+g0QJkxPpgvO5bh4uvS1OTP0ePCwsJQW6ZLVqTs7fHd3d3V1dXqv79VX6lET6A1TIxXUIBSgHxWQnpelHf+WVnopkXqbC7j5Ozi4uLDw8NGUaFATJ9SgH3r6+vGyd7BxNva2trX19ejqM2gpczHx8dze7Zha67Z2dlTgH1aXQeSAAAAJnRSTlMA6ff+497Y8NL+/fv49P379sqab/BeOiX06tzVy8m/tKqpalA7G6oKj0EAAAJlSURBVEjHndNXWxpBFIDhcS2ICRLAkt4Dx4WhLk0E6R0MYoIISrWX5P9f5cwSIRC2+T1czMV5n2FnZwn2eWONUqCAv3H2Uf5Ra1hx4+0WEXtDQW0fCPYJ1EffEfIV4CSROAE4jsePoTFsNmTJF/IeIHF2lgCIn57GodlqDWXBK7IwBYatVlMWFAildPKX7I3m74Z9fsCiQChoimoFQAz04Ad2gH1n9fv9n9hgMNDr9euLWD6fLxQKxaLfb7dTSlahbFVdEPwIQtrAihZQgyKCtCagbQe3xh0QFMgy5MR11+ewYY5/qlZ7vT2xu93ULKjbFLpiUxnIIwjgKmVTLDUFXMrAi2NJWCRLIthTBo4xyOLKpwyqU6CuDCI41hFBCVdOhyLw4FgJ1skCAiyl9BSHbCorgo6VJXTru5hrVCQS8Yr5xLzX59YJSFpVFwD9U0BGC3hGdFpATgRupTGe9R9I1b1ePBvXKDyvq/O/44LT4/E4BUbSCAwj8Evq6HlnOBprx6JhJz8Gktc7xeaP9ndY+0coQvCccFBD4JW60UIY50ciLOAODAQRVOeCHm4Q3Xks6uRDY+CQ+AR4T2wMYh6+jMCIQOp78CFoj0H7EQgIuhI3dGaHCrwgADwCPjJvA372GRigCJg49FUdk3D87pq3zp4SA5zc1Zh9DxfwkpjgUg5Mv+lbeE3McC8Lpu7SA3wk2xzcqL2tN5DfIsQC8HB7UamUy6FQOpTO5QKBQDZbKnWSyUzGjdWCwaDA8+7Le4BNgm3qQGWchYh9s5hNq6wVbBlbwhZYOp3OYOA4zmgEypnM2zj8ByIdedKrH8vDAAAAAElFTkSuQmCC");zoom:.8}sharecard-content{padding:15px;max-height:500px;font-size:20px;text-align:justify;background-color:#2196f3;overflow-x:hidden;overflow-y:auto}sharecard-via{padding:10px;font-size:10px;background-color:#2196f3}sharecard-footer{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:100px;background-color:#fff;color:#878787;font-size:15px;font-weight:500;border-bottom-left-radius:10px;border-bottom-right-radius:10px}sharecard-footer,sharecard-footer div{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sharecard-footer span.qrcode{display:block;width:100px;height:100px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,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")}sharecard-control{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:0 19px;height:80px;background-color:#fff}simpread-snapshot{width:100%;height:100%;cursor:move;z-index:2147483645}simpread-snapshot,sr-mask{position:fixed;left:0;top:0}sr-mask{background-color:rgba(0,0,0,.1)}.simpread-feedback,.simpread-urlscheme{position:fixed;right:20px;bottom:20px;z-index:2147483646}simpread-feedback,simpread-urlscheme{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:20px 20px 0;width:500px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:3px;box-shadow:0 0 2px rgba(0,0,0,.12),0 2px 2px rgba(0,0,0,.26);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease}simpread-feedback *,simpread-urlscheme *{font-size:12px!important;box-sizing:border-box}simpread-feedback.active,simpread-urlscheme.active{-webkit-animation-name:srFadeInUp;animation-name:srFadeInUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback.hide,simpread-urlscheme.hide{-webkit-animation-name:srFadeInDown;animation-name:srFadeInDown;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback sr-fb-label,simpread-urlscheme sr-urls-label{width:100%}simpread-feedback sr-fb-head,simpread-urlscheme sr-urls-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin-bottom:5px;width:100%}simpread-feedback sr-fb-content,simpread-urlscheme sr-urls-content{margin-bottom:5px;width:100%}simpread-feedback sr-urls-footer,simpread-urlscheme sr-urls-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-fb-a,simpread-urlscheme sr-urls-a{color:#2163f7;cursor:pointer}simpread-feedback text-field-state,simpread-urlscheme text-field-state{border-top:none rgba(34,101,247,.8)!important;border-left:none rgba(34,101,247,.8)!important;border-right:none rgba(34,101,247,.8)!important;border-bottom:2px solid rgba(34,101,247,.8)!important}simpread-feedback switch,simpread-urlscheme switch{margin-top:0!important}@-webkit-keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@-webkit-keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}@keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}simpread-feedback sr-fb-head{font-weight:700}simpread-feedback sr-fb-content{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}simpread-feedback sr-fb-content,simpread-feedback sr-fb-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}simpread-feedback sr-fb-footer{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-close{position:absolute;right:20px;cursor:pointer;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:200}simpread-feedback sr-close:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-top:10px}simpread-feedback sr-stars i{margin-right:10px;cursor:pointer}simpread-feedback sr-stars i svg{-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}simpread-feedback sr-stars i svg:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars i.active svg{-webkit-transform:rotate(0) scale(1);transform:rotate(0) scale(1)}simpread-feedback sr-emojis{display:block;height:100px;overflow:hidden}simpread-feedback sr-emoji{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:.3s;transition:.3s}simpread-feedback sr-emoji>svg{margin:15px 0;width:70px;height:70px;-ms-flex-negative:0;flex-shrink:0}simpread-feedback sr-stars-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin:10px 0 20px}</style>
                        <style type="text/css">.simpread-theme-root{font-size:62.5%!important}sr-rd-content,sr-rd-desc,sr-rd-title{width:100%}sr-rd-title{display:-webkit-box;margin:1em 0 .5em;overflow:hidden;text-overflow:ellipsis;text-rendering:optimizelegibility;-webkit-line-clamp:3;-webkit-box-orient:vertical}sr-rd-content{text-align:left;word-break:break-word}sr-rd-desc{text-align:justify;line-height:2.4;margin:0 0 1.2em;box-sizing:border-box}sr-rd-content{font-size:25.6px;font-size:1.6rem;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:25.6px;font-size:1.6rem}sr-rd-content div,sr-rd-content p,sr-rd-content pre,sr-rd-content sr-blockquote{margin:0 0 1.2em;word-break:break-word}sr-rd-content a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-rd-content a,sr-rd-content a:link{color:inherit;font-size:inherit;font-weight:inherit;border:none}sr-rd-content a:hover{background:transparent}sr-rd-content img{margin:10px;padding:5px;max-width:100%;background:#fff;border:1px solid #bbb;box-shadow:1px 1px 3px #d4d4d4}sr-rd-content figcaption{text-align:center;font-size:14px}sr-rd-content sr-blockquote{display:block;position:relative;padding:15px 25px;text-align:left;line-height:inherit}sr-rd-content sr-blockquote:before{position:absolute}sr-rd-content sr-blockquote *{margin:0;font-size:inherit}sr-rd-content table{width:100%;margin:0 0 1.2em;word-break:keep-all;word-break:normal;overflow:auto;border:none}sr-rd-content table td,sr-rd-content table th{border:none}sr-rd-content ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-rd-content ol{list-style:decimal;margin:0;padding:0}sr-rd-content ol li,sr-rd-content ul li{font-size:inherit;list-style:disc;margin:0 0 1.2em}sr-rd-content ol li{list-style:decimal;margin-left:1.3em}sr-rd-content ol li *,sr-rd-content ul li *{margin:0;text-align:left;text-align:initial}sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em}sr-rd-content li ul{list-style:circle}sr-rd-content pre{font-family:Consolas,Monaco,Andale Mono,Source Code Pro,Liberation Mono,Courier,monospace;display:block;padding:15px;line-height:1.5;word-break:break-all;word-wrap:break-word;white-space:pre;overflow:auto}sr-rd-content pre,sr-rd-content pre *,sr-rd-content pre div{font-size:17.6px;font-size:1.1rem}sr-rd-content li pre code,sr-rd-content p pre code,sr-rd-content pre{background-color:transparent;border:none}sr-rd-content pre code{margin:0;padding:0}sr-rd-content pre code,sr-rd-content pre code *{font-size:17.6px;font-size:1.1rem}sr-rd-content pre p{margin:0;padding:0;color:inherit;font-size:inherit;line-height:inherit}sr-rd-content li code,sr-rd-content p code{margin:0 4px;padding:2px 4px;font-size:17.6px;font-size:1.1rem}sr-rd-content mark{margin:0 5px;padding:2px;background:#fffdd1;border-bottom:1px solid #ffedce}.sr-rd-content-img{width:90%;height:auto}.sr-rd-content-img-load{width:48px;height:48px;margin:0;padding:0;border-style:none;border-width:0;background-repeat:no-repeat;background-image:url(data:image/gif;base64,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)}.sr-rd-content-center{text-align:center;display:-webkit-box;-webkit-box-align:center;-webkit-box-pack:center;-webkit-box-orient:vertical}.sr-rd-content-center-small{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.sr-rd-content-center-small img{margin:0;padding:0;border:0;box-shadow:none}img.simpread-img-broken{cursor:pointer}.sr-rd-content-nobeautify{margin:0;padding:0;border:0;box-shadow:0 0 0}sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-radius:4px;box-shadow:0 1px 2px 0 rgba(60,64,67,.3),0 2px 6px 2px rgba(60,64,67,.15)}sr-rd-mult:hover{-webkit-transition:all .45s 0ms;transition:all .45s 0ms;box-shadow:1px 1px 8px rgba(0,0,0,.16)}sr-rd-mult sr-rd-mult-content{padding:0 16px;overflow:auto}sr-rd-mult sr-rd-mult-avatar,sr-rd-mult sr-rd-mult-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sr-rd-mult sr-rd-mult-avatar{-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 15px}sr-rd-mult sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}sr-rd-mult sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}sr-rd-mult sr-rd-mult-content img{max-width:80%}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-page{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%}</style>
                        <style type="text/css">sr-rd-theme-github{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:57.6px;font-size:3.6rem;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;line-height:1.225}sr-rd-content h3{font-size:38.4px;font-size:2.4rem;line-height:1.43}sr-rd-content h4{font-size:32px;font-size:2rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}sr-rd-title{font-family:PT Sans,SF UI Display,\.PingFang SC,PingFang SC,Neue Haas Grotesk Text Pro,Arial Nova,Segoe UI,Microsoft YaHei,Microsoft JhengHei,Helvetica Neue,Source Han Sans SC,Noto Sans CJK SC,Source Han Sans CN,Noto Sans SC,Source Han Sans TC,Noto Sans CJK TC,Hiragino Sans GB,sans-serif;font-size:54.4px;font-size:3.4rem;font-weight:700;line-height:1.3}sr-rd-desc{position:relative;margin:0;margin-bottom:30px;padding:25px;padding-left:56px;font-size:28.8px;font-size:1.8rem;color:#777;background-color:rgba(0,0,0,.05);box-sizing:border-box}sr-rd-desc:before{content:"\201C";position:absolute;top:-28px;left:16px;font-size:80px;font-family:Arial;color:rgba(0,0,0,.15)}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content b *,sr-rd-content strong,sr-rd-content strong * sr-rd-content b{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}</style>
                        <style type="text/css"></style>
                        <style type="text/css">@media (pointer:coarse){sr-read{margin:20px 5%!important;min-width:0!important;max-width:90%!important}sr-rd-title{margin-top:0;font-size:2.7rem}sr-rd-content sr-blockquote,sr-rd-desc{margin:10 0!important;padding:0 0 0 10px!important;width:90%;font-size:1.8rem;font-style:normal;line-height:1.7;text-align:justify}sr-rd-content{font-size:1.75rem;font-weight:300}sr-rd-content figure{margin:0;padding:0;text-align:center}sr-rd-content a,sr-rd-content a:link,sr-rd-content li code,sr-rd-content p code{font-size:inherit}sr-rd-footer{margin-top:20px}sr-blockquote,sr-blockquote *{margin:5px!important;padding:5px!important}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6,sr-rd-title{font-family:PingFang SC,Verdana,Helvetica Neue,Microsoft Yahei,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#000;font-weight:100;line-height:1.35}sr-rd-content-h1,sr-rd-content-h2,sr-rd-content-h3,sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;line-height:1.35}sr-rd-content-h1,sr-rd-content h1{font-size:1.8em}sr-rd-content-h2,sr-rd-content h2{font-size:1.6em}sr-rd-content-h3,sr-rd-content h3{font-size:1.4em}sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:1.2em}sr-rd-content-ul,sr-rd-content ul{margin-left:1.3em!important;list-style:disc}sr-rd-content-ol,sr-rd-content ol{list-style:decimal;margin-left:1.9em!important}sr-rd-content-ol ol,sr-rd-content-ol ul,sr-rd-content-ul ol,sr-rd-content-ul ul,sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em!important}sr-rd-content img{margin:0;padding:0;border:0;max-width:100%!important;height:auto;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-rd-mult{min-width:0;background-color:#fff;box-shadow:0 1px 6px rgba(32,33,36,.28);border-radius:8px}sr-rd-mult sr-rd-mult-avatar div{margin:0}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center-small{margin:7px 0!important}sr-rd-mult sr-rd-mult-avatar span{display:block}sr-rd-mult sr-rd-mult-content{padding-left:0}@media only screen and (max-device-width:1024px){.simpread-theme-root,html.simpread-theme-root{font-size:80%!important}sr-rd-mult sr-rd-mult-avatar img{width:50px;height:50px;min-width:50px;min-height:50px}toc-bg toc{width:10px!important}toc-bg:hover toc{width:auto!important}}@media only screen and (max-device-width:414px){.simpread-theme-root,html.simpread-theme-root{font-size:70%!important}sr-rd-mult sr-rd-mult-avatar img{width:30px;height:30px;min-width:30px;min-height:30px}}@media only screen and (max-device-width:320px){.simpread-theme-root,html.simpread-theme-root{font-size:90%!important}sr-rd-content p{margin-bottom:.5em}}}</style>
                        <style type="text/css">sr-rd-content *, sr-rd-content p, sr-rd-content div {}sr-rd-content pre code, sr-rd-content pre code * {}sr-rd-desc {}sr-rd-content pre {}sr-rd-title {}</style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css">sr-rd-content *, sr-rd-content p, sr-rd-content div {
        font-size: 15px;
    }

    .annote-perview, .annote-perview * {
        color: rgb(85, 85, 85);
        font-weight: 400;
        line-height: 1.8;
    }</style>
                        
                        
                        <script>setTimeout(()=>{const e=location.hash.replace("#id=","");let t,a=!1;const n=t=>{for(let n of t){let t;if((t=e.length>6?n.getAttribute("data-id"):n.getAttribute("data-idx"))==e){n.scrollIntoView({behavior:"smooth",block:"start",inline:"nearest"}),a=!0;break}}};e&&(0==(t=document.getElementsByClassName("sr-unread-card")).length&&(t=document.getElementsByTagName("sr-annote")),n(t),a||n(t=document.getElementsByClassName("sr-annote")))},500);</script>
                        <script>document.addEventListener("DOMContentLoaded",function(){if("localhost"==location.hostname){const t=document.getElementsByTagName("img");for(let o of t){const t=o.src;t.startsWith("http")&&(o.src=location.origin+"/proxy?url="+t)}}},!1);</script>
                        <style>toc a {font-size: inherit!important;font-weight: 300!important;}</style><script>setTimeout(()=>{const max=document.getElementsByTagName('sr-annote-note-tip').length;for(let i=0;i<max;i++){const target=document.getElementsByTagName('sr-annote-note-tip')[i],value=target.dataset.value;value&&(target.innerText=value)}},1000);</script><title>简悦 | 16｜分布式训练：如何加速你的模型训练？</title>
                    </head>
                    <body>
                        <sr-read style='font-family: "LXGW WenKai Screen";'>
                            <sr-rd-title>16｜分布式训练：如何加速你的模型训练？</sr-rd-title>
                    <sr-rd-desc style="margin: 0;padding-top: 0;padding-bottom: 0;font-style: normal;font-size: 18px;">今天我们一起剖析分布式训练的原理，然后一起学习一个分布式训练的实战项目。</sr-rd-desc>
                    <sr-rd-content><h1 id="sr-toc-0">16｜分布式训练：如何加速你的模型训练？</h1><div><span>方远</span> <span> 2021-11-17</span></div><div><div><div class="sr-rd-content-center"><img class="sr-rd-content-img" src="https://static001.geekbang.org/resource/image/52/3c/52307a496de619b7e152d7768490b83c.jpg"></div><div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADEAAAABCAYAAAB35kaxAAAAAXNSR0IArs4c6QAAAChJREFUGFdjfPfu3X8GKBAUFASz3r9/DxNiQBbDxcamn1yzSLEDZi8A2agny86FR+IAAAAASUVORK5CYII="></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAABCAYAAAA8TpVcAAAAAXNSR0IArs4c6QAAAB1JREFUGFdjfPfu3X9BQUEGEHj//j2YBgFCYtjkAcT9D8ti7hUOAAAAAElFTkSuQmCC"></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD0AAAABCAYAAABt2qY/AAAAAXNSR0IArs4c6QAAACdJREFUGFdjfPfu3X9BQUEGEHj//j2YBgFkMULy2PQQK0YLewiZCQDhUS3LBhDf1QAAAABJRU5ErkJggg=="></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAABCAYAAAA4u0VhAAAAAXNSR0IArs4c6QAAAB1JREFUGFdjfPfu3X9BQUEGEHj//j2YBgFkMULyAF6lD8tbqYWOAAAAAElFTkSuQmCC"></div></div></div><div><div><div></div></div><div><div><div><div><div><div>00:00</div></div><div></div></div></div><a href="javascript:;"> 1.0x <i><span></span></i></a></div><div><span>讲述：方远</span><span>大小：14.93M</span><span> 时长：16:21</span></div></div><audio title="16｜分布式训练：如何加速你的模型训练？" src="https://res001.geekbang.org/media/audio/63/6b/63c454f503740212d725df0743a1a76b/ld/ld.m3u8"></audio></div><div><div><div><div data-slate-editor="true" data-key="0" autocorrect="off" spellcheck="false" data-gramm="false"><div data-slate-type="paragraph" data-slate-object="block" data-key="1"><span data-slate-object="text" data-key="2"><span data-slate-leaf="true" data-offset-key="2:0" data-first-offset="true"><span data-slate-string="true">你好，我是方远。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="3"><span data-slate-object="text" data-key="4"><span data-slate-leaf="true" data-offset-key="4:0" data-first-offset="true"><span data-slate-string="true">在之前的课程里，我们一起学习了深度学习必备的内容，包括构建网络、损失函数、优化方法等，这些环节掌握好了，我们就可以训练很多场景下的模型了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5"><span data-slate-object="text" data-key="6"><span data-slate-leaf="true" data-offset-key="6:0" data-first-offset="true"><span data-slate-string="true">但是有的时候，我们的模型比较大，或者训练数据比较多，训练起来就会比较慢，该怎么办呢？这时候牛气闪闪的分布式训练登场了，有了它，我们就可以极大地加速我们的训练过程。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7"><span data-slate-object="text" data-key="8"><span data-slate-leaf="true" data-offset-key="8:0" data-first-offset="true"><span data-slate-string="true">这节课我就带你入门分布式训练，让你吃透分布式训练的工作原理，最后我还会结合一个实战项目，带你小试牛刀，让你在动手过程中加深对这部分内容的理解。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="9" id="sr-toc-1"><span data-slate-object="text" data-key="10"><span data-slate-leaf="true" data-offset-key="10:0" data-first-offset="true"><span data-slate-string="true">分布式训练原理</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="11"><span data-slate-object="text" data-key="12"><span data-slate-leaf="true" data-offset-key="12:0" data-first-offset="true"><span data-slate-string="true">在具体介绍分布式训练之前，我们需要先简要了解一下为什么深度学习要使用 GPU。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="13"><span data-slate-object="text" data-key="14"><span data-slate-leaf="true" data-offset-key="14:0" data-first-offset="true"><span data-slate-string="true">在我们平时使用计算机的时候，程序都是将进程或者线程的数据资源放在内存中，然后在 CPU 进行计算。通常的程序中涉及到了大量的 if else 等分支逻辑操作，这也是 CPU 所擅长的计算方式。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="15"><span data-slate-object="text" data-key="16"><span data-slate-leaf="true" data-offset-key="16:0" data-first-offset="true"><span data-slate-string="true">而在深度学习中，模型的训练与计算过程则没有太多的分支，基本上都是矩阵或者向量的计算，而这种暴力又单纯的计算形式非常适合用 GPU 处理，GPU 的整个处理过程就是一个流式处理的过程。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="17"><span data-slate-object="text" data-key="18"><span data-slate-leaf="true" data-offset-key="18:0" data-first-offset="true"><span data-slate-string="true">但是再好的车子，一个缸的发动机也肯定比不过 12 个缸的，同理单单靠一个 GPU，速度肯定还是不够快，于是就有了多个 GPU 协同工作的办法，即分布式训练。分布式训练，顾名思义就是训练的过程是分布式的，重点就在于后面这两个问题：</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="19"><span data-slate-object="text" data-key="20"><span data-slate-leaf="true" data-offset-key="20:0" data-first-offset="true"><span data-slate-string="true">1. 谁分布了？答案有两个：数据与模型。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="21"><span data-slate-object="text" data-key="22"><span data-slate-leaf="true" data-offset-key="22:0" data-first-offset="true"><span data-slate-string="true">2. 怎么分布？答案也有两个：单机多卡与多机多卡。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="23"><span data-slate-object="text" data-key="24"><span data-slate-leaf="true" data-offset-key="24:0" data-first-offset="true"><span data-slate-string="true">也就是说，为了实现深度学习的分布式训练，我们需要采用单机多卡或者多机多卡的方式，让分布在不同 GPU 上的数据和模型协同训练。那么接下来，我们先从简单的单机单卡入手，了解一下 GPU 的训练过程。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="25" id="sr-toc-2"><span data-slate-object="text" data-key="26"><span data-slate-leaf="true" data-offset-key="26:0" data-first-offset="true"><span data-slate-string="true">单机单卡</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="27"><span data-slate-object="text" data-key="28"><span data-slate-leaf="true" data-offset-key="28:0" data-first-offset="true"><span data-slate-string="true">想象一下，如果让你把数据或者模型推送到 GPU 上，需要做哪几步操作呢？让我们先从单 GPU 的情况出发。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="29"><span data-slate-object="text" data-key="30"><span data-slate-leaf="true" data-offset-key="30:0" data-first-offset="true"><span data-slate-string="true">第一步，我们需要知道手头有多少 GPU。PyTorch 中使用 torch.cuda.is_available () 函数来判断当前的机器是否有可用的 GPU，而函数 torch.cuda.device_count () 则可以得到目前可用的 GPU 的数量。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="31"><span data-slate-object="text" data-key="32"><span data-slate-leaf="true" data-offset-key="32:0" data-first-offset="true"><span data-slate-string="true">第二步，获得 GPU 的一个实例。例如下面的语句：</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="33"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="34"><span data-slate-object="text" data-key="35"><span data-slate-leaf="true" data-offset-key="35:0" data-first-offset="true"><span data-slate-string="true">device = torch.device(</span></span></span><span data-slate-object="text" data-key="36"><span data-slate-leaf="true" data-offset-key="36:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"cuda:0"</span></span></span></span><span data-slate-object="text" data-key="37"><span data-slate-leaf="true" data-offset-key="37:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="38"><span data-slate-leaf="true" data-offset-key="38:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">if</span></span></span></span><span data-slate-object="text" data-key="39"><span data-slate-leaf="true" data-offset-key="39:0" data-first-offset="true"><span data-slate-string="true"> torch.cuda.is_available() </span></span></span><span data-slate-object="text" data-key="40"><span data-slate-leaf="true" data-offset-key="40:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">else</span></span></span></span><span data-slate-object="text" data-key="41"><span data-slate-leaf="true" data-offset-key="41:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="42"><span data-slate-leaf="true" data-offset-key="42:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"cpu"</span></span></span></span><span data-slate-object="text" data-key="43"><span data-slate-leaf="true" data-offset-key="43:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="44"><span data-slate-object="text" data-key="45"><span data-slate-leaf="true" data-offset-key="45:0" data-first-offset="true"><span data-slate-string="true">这里 torch.device 代表将 torch.Tensor 分配到的设备，是一个设备对象实例，也就是 GPU。其中 cuda: 0 表示我们使用的是第一块 GPU。当然你也可以不用声明 “:0”，默认就从第一块开始。如果没有 GPU（torch.cuda.is_available ()），那就只能使用 CPU 了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="46"><span data-slate-object="text" data-key="47"><span data-slate-leaf="true" data-offset-key="47:0" data-first-offset="true"><span data-slate-string="true">第三步，将数据或者模型推到 GPU 上去，这个过程我们称为</span></span></span><span data-slate-object="text" data-key="48"><span data-slate-leaf="true" data-offset-key="48:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">迁移</span></span></span></span><span data-slate-object="text" data-key="49"><span data-slate-leaf="true" data-offset-key="49:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="50"><span data-slate-object="text" data-key="51"><span data-slate-leaf="true" data-offset-key="51:0" data-first-offset="true"><span data-slate-string="true">在 PyTorch 中，这个过程的封装程度非常高，换句话说，我们只需要保证即将被推到 GPU 的内容是张量（Tensor）或者模型（Module），就可以用 to () 函数快速进行实现。例如：</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="52"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div><div data-code-line-number="6"></div><div data-code-line-number="7"></div><div data-code-line-number="8"></div><div data-code-line-number="9"></div><div data-code-line-number="10"></div><div data-code-line-number="11"></div><div data-code-line-number="12"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="53"><span data-slate-object="text" data-key="54"><span data-slate-leaf="true" data-offset-key="54:0" data-first-offset="true"><span data-slate-string="true">data = torch.ones((</span></span></span><span data-slate-object="text" data-key="55"><span data-slate-leaf="true" data-offset-key="55:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">3</span></span></span></span><span data-slate-object="text" data-key="56"><span data-slate-leaf="true" data-offset-key="56:0" data-first-offset="true"><span data-slate-string="true">, </span></span></span><span data-slate-object="text" data-key="57"><span data-slate-leaf="true" data-offset-key="57:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">3</span></span></span></span><span data-slate-object="text" data-key="58"><span data-slate-leaf="true" data-offset-key="58:0" data-first-offset="true"><span data-slate-string="true">))</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="59"><span data-slate-object="text" data-key="60"><span data-slate-leaf="true" data-offset-key="60:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">print</span></span></span></span><span data-slate-object="text" data-key="61"><span data-slate-leaf="true" data-offset-key="61:0" data-first-offset="true"><span data-slate-string="true">(data.device)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="62"><span data-slate-object="text" data-key="63"><span data-slate-leaf="true" data-offset-key="63:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># Get: cpu</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="64"></div><div data-slate-type="code-line" data-slate-object="block" data-key="65"><span data-slate-object="text" data-key="66"><span data-slate-leaf="true" data-offset-key="66:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 获得 device</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="67"><span data-slate-object="text" data-key="68"><span data-slate-leaf="true" data-offset-key="68:0" data-first-offset="true"><span data-slate-string="true">device = torch.device(</span></span></span><span data-slate-object="text" data-key="69"><span data-slate-leaf="true" data-offset-key="69:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"cuda: 0"</span></span></span></span><span data-slate-object="text" data-key="70"><span data-slate-leaf="true" data-offset-key="70:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="71"></div><div data-slate-type="code-line" data-slate-object="block" data-key="72"><span data-slate-object="text" data-key="73"><span data-slate-leaf="true" data-offset-key="73:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 将 data 推到 gpu 上</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="74"><span data-slate-object="text" data-key="75"><span data-slate-leaf="true" data-offset-key="75:0" data-first-offset="true"><span data-slate-string="true">data_gpu = data.to(device)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="76"><span data-slate-object="text" data-key="77"><span data-slate-leaf="true" data-offset-key="77:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">print</span></span></span></span><span data-slate-object="text" data-key="78"><span data-slate-leaf="true" data-offset-key="78:0" data-first-offset="true"><span data-slate-string="true">(data_gpu.device)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="79"><span data-slate-object="text" data-key="80"><span data-slate-leaf="true" data-offset-key="80:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># Get: cuda:0</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="81"></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="82"><span data-slate-object="text" data-key="83"><span data-slate-leaf="true" data-offset-key="83:0" data-first-offset="true"><span data-slate-string="true">在上面这段代码中，我们首先创建了一个常规的张量 data，通过 device 属性，可以看到 data 现在是在 CPU 上的。随后，我们通过 to () 函数将 data 迁移到 GPU 上，同样也能通过 device 属性看到 data 确实已经存在于 GPU 上了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="84"><span data-slate-object="text" data-key="85"><span data-slate-leaf="true" data-offset-key="85:0" data-first-offset="true"><span data-slate-string="true">那么对于模型，是否也是一样的操作呢？答案是肯定的，我们接下来看一个例子：</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="86"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="87"><span data-slate-object="text" data-key="88"><span data-slate-leaf="true" data-offset-key="88:0" data-first-offset="true"><span data-slate-string="true">net = nn.Sequential(nn.Linear(</span></span></span><span data-slate-object="text" data-key="89"><span data-slate-leaf="true" data-offset-key="89:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">3</span></span></span></span><span data-slate-object="text" data-key="90"><span data-slate-leaf="true" data-offset-key="90:0" data-first-offset="true"><span data-slate-string="true">, </span></span></span><span data-slate-object="text" data-key="91"><span data-slate-leaf="true" data-offset-key="91:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">3</span></span></span></span><span data-slate-object="text" data-key="92"><span data-slate-leaf="true" data-offset-key="92:0" data-first-offset="true"><span data-slate-string="true">))</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="93"><span data-slate-object="text" data-key="94"><span data-slate-leaf="true" data-offset-key="94:0" data-first-offset="true"><span data-slate-string="true">net.to(device)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="95"><span data-slate-object="text" data-key="96"><span data-slate-leaf="true" data-offset-key="96:0" data-first-offset="true"><span data-slate-string="true">这里仍旧使用 to () 函数即可。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="97"><span data-slate-object="text" data-key="98"><span data-slate-leaf="true" data-offset-key="98:0" data-first-offset="true"><span data-slate-string="true">单机单卡的模式，相当于有一批要处理加工的产品，只分给了一个工人和一台机器来完成，这种情况下数量少了还可以，但是一旦产品太多了，就得加人、加机器才能快速交工了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="99"><span data-slate-object="text" data-key="100"><span data-slate-leaf="true" data-offset-key="100:0" data-first-offset="true"><span data-slate-string="true">深度学习也是一样，在很多场景中，比如推荐算法模型、语言模型等，往往都有着百万、千万甚至上亿的训练数据，这样如果只用一张卡的话肯定是搞不定了。于是就有了单机多卡和多机多卡的解决方案。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="101" id="sr-toc-3"><span data-slate-object="text" data-key="102"><span data-slate-leaf="true" data-offset-key="102:0" data-first-offset="true"><span data-slate-string="true">单机多卡</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="103"><span data-slate-object="text" data-key="104"><span data-slate-leaf="true" data-offset-key="104:0" data-first-offset="true"><span data-slate-string="true">那么，在 PyTorch 中，单机多卡的训练是如何进行的呢？其实 PyTorch 提供了好几种解决方案，咱们先看一个最简单也是最常用的办法：nn.DataParallel ()。其具体定义如下：</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="105"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="106"><span data-slate-object="text" data-key="107"><span data-slate-leaf="true" data-offset-key="107:0" data-first-offset="true"><span data-slate-string="true">torch.nn.DataParallel(module, device_ids=</span></span></span><span data-slate-object="text" data-key="108"><span data-slate-leaf="true" data-offset-key="108:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">None</span></span></span></span><span data-slate-object="text" data-key="109"><span data-slate-leaf="true" data-offset-key="109:0" data-first-offset="true"><span data-slate-string="true">, output_device=</span></span></span><span data-slate-object="text" data-key="110"><span data-slate-leaf="true" data-offset-key="110:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">None</span></span></span></span><span data-slate-object="text" data-key="111"><span data-slate-leaf="true" data-offset-key="111:0" data-first-offset="true"><span data-slate-string="true">, dim=</span></span></span><span data-slate-object="text" data-key="112"><span data-slate-leaf="true" data-offset-key="112:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">0</span></span></span></span><span data-slate-object="text" data-key="113"><span data-slate-leaf="true" data-offset-key="113:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="114"><span data-slate-object="text" data-key="115"><span data-slate-leaf="true" data-offset-key="115:0" data-first-offset="true"><span data-slate-string="true">在这里，module 就是你定义的模型，device_ids 即为训练模型时用到的 GPU 设备号，output_device 表示输出结果的 device，默认为 0 也就是第一块卡。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="116"><span data-slate-object="text" data-key="117"><span data-slate-leaf="true" data-offset-key="117:0" data-first-offset="true"><span data-slate-string="true">我们可以使用 nvidia-smi 命令查看 GPU 使用情况。如果你足够细心就会发现，使用多个卡做训练的时候，output_device 的卡所占的显存明显大一些。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="118"><span data-slate-object="text" data-key="119"><span data-slate-leaf="true" data-offset-key="119:0" data-first-offset="true"><span data-slate-string="true">继续观察你还会发现，使用 DataParallel 时，数据的使用是并行的，每张卡获得的数据都一样多，但是输出的 loss 则是所有的卡的 loss 都会在第 output_device 块 GPU 进行计算，这导致了 output_device 卡的负载进一步增加。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="120"><div class="sr-rd-content-center"><img class="sr-rd-content-img" src="https://static001.geekbang.org/resource/image/7f/08/7f8e9a83fa6a91yyf931565c55f0a708.png?wh=1130x522"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="121"><span data-slate-object="text" data-key="122"><span data-slate-leaf="true" data-offset-key="122:0" data-first-offset="true"><span data-slate-string="true">就这么简单？对，就这么简单，只需要一个 DataParallel 函数就可以将模型分发到多个 GPU 上。但是我们还是需要了解这内部的运行逻辑，因为只有了解了这个逻辑，在以后的开发中遇到了诸如</span></span></span><span data-slate-object="text" data-key="123"><span data-slate-leaf="true" data-offset-key="123:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">时间计算、资源预估、优化调试问题</span></span></span></span><span data-slate-object="text" data-key="124"><span data-slate-leaf="true" data-offset-key="124:0" data-first-offset="true"><span data-slate-string="true">的时候，你才可以更好地运用 GPU，让多 GPU 的优势真正发挥出来。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="125"><span data-slate-object="text" data-key="126"><span data-slate-leaf="true" data-offset-key="126:0" data-first-offset="true"><span data-slate-string="true">在模型的前向计算过程中，数据会被划分为多个块，被推送到不同的 GPU 进行计算。但是不同的是，模型在每个 GPU 中都会复制一份。我们看一下后面的代码：</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="127"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div><div data-code-line-number="6"></div><div data-code-line-number="7"></div><div data-code-line-number="8"></div><div data-code-line-number="9"></div><div data-code-line-number="10"></div><div data-code-line-number="11"></div><div data-code-line-number="12"></div><div data-code-line-number="13"></div><div data-code-line-number="14"></div><div data-code-line-number="15"></div><div data-code-line-number="16"></div><div data-code-line-number="17"></div><div data-code-line-number="18"></div><div data-code-line-number="19"></div><div data-code-line-number="20"></div><div data-code-line-number="21"></div><div data-code-line-number="22"></div><div data-code-line-number="23"></div><div data-code-line-number="24"></div><div data-code-line-number="25"></div><div data-code-line-number="26"></div><div data-code-line-number="27"></div><div data-code-line-number="28"></div><div data-code-line-number="29"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="128"><span data-slate-object="text" data-key="129"><span data-slate-leaf="true" data-offset-key="129:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">class</span></span></span></span><span data-slate-object="text" data-key="130"><span data-slate-leaf="true" data-offset-key="130:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="131"><span data-slate-leaf="true" data-offset-key="131:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">ASimpleNet</span></span></span></span><span data-slate-object="text" data-key="132"><span data-slate-leaf="true" data-offset-key="132:0" data-first-offset="true"><span data-slate-string="true">(nn.Module):</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="133"><span data-slate-object="text" data-key="134"><span data-slate-leaf="true" data-offset-key="134:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="135"><span data-slate-leaf="true" data-offset-key="135:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">def</span></span></span></span><span data-slate-object="text" data-key="136"><span data-slate-leaf="true" data-offset-key="136:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="137"><span data-slate-leaf="true" data-offset-key="137:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">__init__</span></span></span></span><span data-slate-object="text" data-key="138"><span data-slate-leaf="true" data-offset-key="138:0" data-first-offset="true"><span data-slate-string="true">(</span></span></span><span data-slate-object="text" data-key="139"><span data-slate-leaf="true" data-offset-key="139:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">self, layers=</span></span></span></span><span data-slate-object="text" data-key="140"><span data-slate-leaf="true" data-offset-key="140:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">3</span></span></span></span></span><span data-slate-object="text" data-key="141"><span data-slate-leaf="true" data-offset-key="141:0" data-first-offset="true"><span data-slate-string="true">):</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="142"><span data-slate-object="text" data-key="143"><span data-slate-leaf="true" data-offset-key="143:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="144"><span data-slate-leaf="true" data-offset-key="144:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">super</span></span></span></span><span data-slate-object="text" data-key="145"><span data-slate-leaf="true" data-offset-key="145:0" data-first-offset="true"><span data-slate-string="true">(ASimpleNet, self).__init__()</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="146"><span data-slate-object="text" data-key="147"><span data-slate-leaf="true" data-offset-key="147:0" data-first-offset="true"><span data-slate-string="true">        self.linears = nn.ModuleList([nn.Linear(</span></span></span><span data-slate-object="text" data-key="148"><span data-slate-leaf="true" data-offset-key="148:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">3</span></span></span></span><span data-slate-object="text" data-key="149"><span data-slate-leaf="true" data-offset-key="149:0" data-first-offset="true"><span data-slate-string="true">, </span></span></span><span data-slate-object="text" data-key="150"><span data-slate-leaf="true" data-offset-key="150:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">3</span></span></span></span><span data-slate-object="text" data-key="151"><span data-slate-leaf="true" data-offset-key="151:0" data-first-offset="true"><span data-slate-string="true">, bias=</span></span></span><span data-slate-object="text" data-key="152"><span data-slate-leaf="true" data-offset-key="152:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">False</span></span></span></span><span data-slate-object="text" data-key="153"><span data-slate-leaf="true" data-offset-key="153:0" data-first-offset="true"><span data-slate-string="true">) </span></span></span><span data-slate-object="text" data-key="154"><span data-slate-leaf="true" data-offset-key="154:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">for</span></span></span></span><span data-slate-object="text" data-key="155"><span data-slate-leaf="true" data-offset-key="155:0" data-first-offset="true"><span data-slate-string="true"> i </span></span></span><span data-slate-object="text" data-key="156"><span data-slate-leaf="true" data-offset-key="156:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">in</span></span></span></span><span data-slate-object="text" data-key="157"><span data-slate-leaf="true" data-offset-key="157:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="158"><span data-slate-leaf="true" data-offset-key="158:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">range</span></span></span></span><span data-slate-object="text" data-key="159"><span data-slate-leaf="true" data-offset-key="159:0" data-first-offset="true"><span data-slate-string="true">(layers)])</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="160"><span data-slate-object="text" data-key="161"><span data-slate-leaf="true" data-offset-key="161:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="162"><span data-slate-leaf="true" data-offset-key="162:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">def</span></span></span></span><span data-slate-object="text" data-key="163"><span data-slate-leaf="true" data-offset-key="163:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="164"><span data-slate-leaf="true" data-offset-key="164:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">forward</span></span></span></span><span data-slate-object="text" data-key="165"><span data-slate-leaf="true" data-offset-key="165:0" data-first-offset="true"><span data-slate-string="true">(</span></span></span><span data-slate-object="text" data-key="166"><span data-slate-leaf="true" data-offset-key="166:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">self, x</span></span></span></span><span data-slate-object="text" data-key="167"><span data-slate-leaf="true" data-offset-key="167:0" data-first-offset="true"><span data-slate-string="true">):</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="168"><span data-slate-object="text" data-key="169"><span data-slate-leaf="true" data-offset-key="169:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="170"><span data-slate-leaf="true" data-offset-key="170:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">print</span></span></span></span><span data-slate-object="text" data-key="171"><span data-slate-leaf="true" data-offset-key="171:0" data-first-offset="true"><span data-slate-string="true">(</span></span></span><span data-slate-object="text" data-key="172"><span data-slate-leaf="true" data-offset-key="172:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"forward batchsize is: {}"</span></span></span></span><span data-slate-object="text" data-key="173"><span data-slate-leaf="true" data-offset-key="173:0" data-first-offset="true"><span data-slate-string="true">.</span></span></span><span data-slate-object="text" data-key="174"><span data-slate-leaf="true" data-offset-key="174:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">format</span></span></span></span><span data-slate-object="text" data-key="175"><span data-slate-leaf="true" data-offset-key="175:0" data-first-offset="true"><span data-slate-string="true">(x.size()[</span></span></span><span data-slate-object="text" data-key="176"><span data-slate-leaf="true" data-offset-key="176:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">0</span></span></span></span><span data-slate-object="text" data-key="177"><span data-slate-leaf="true" data-offset-key="177:0" data-first-offset="true"><span data-slate-string="true">]))</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="178"><span data-slate-object="text" data-key="179"><span data-slate-leaf="true" data-offset-key="179:0" data-first-offset="true"><span data-slate-string="true">        x = self.linears(x)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="180"><span data-slate-object="text" data-key="181"><span data-slate-leaf="true" data-offset-key="181:0" data-first-offset="true"><span data-slate-string="true">        x = torch.relu(x)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="182"><span data-slate-object="text" data-key="183"><span data-slate-leaf="true" data-offset-key="183:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="184"><span data-slate-leaf="true" data-offset-key="184:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">return</span></span></span></span><span data-slate-object="text" data-key="185"><span data-slate-leaf="true" data-offset-key="185:0" data-first-offset="true"><span data-slate-string="true"> x</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="186"><span data-slate-object="text" data-key="187"><span data-slate-leaf="true" data-offset-key="187:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="188"><span data-slate-object="text" data-key="189"><span data-slate-leaf="true" data-offset-key="189:0" data-first-offset="true"><span data-slate-string="true">batch_size = </span></span></span><span data-slate-object="text" data-key="190"><span data-slate-leaf="true" data-offset-key="190:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">16</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="191"><span data-slate-object="text" data-key="192"><span data-slate-leaf="true" data-offset-key="192:0" data-first-offset="true"><span data-slate-string="true">inputs = torch.randn(batch_size, </span></span></span><span data-slate-object="text" data-key="193"><span data-slate-leaf="true" data-offset-key="193:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">3</span></span></span></span><span data-slate-object="text" data-key="194"><span data-slate-leaf="true" data-offset-key="194:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="195"><span data-slate-object="text" data-key="196"><span data-slate-leaf="true" data-offset-key="196:0" data-first-offset="true"><span data-slate-string="true">labels = torch.randn(batch_size, </span></span></span><span data-slate-object="text" data-key="197"><span data-slate-leaf="true" data-offset-key="197:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">3</span></span></span></span><span data-slate-object="text" data-key="198"><span data-slate-leaf="true" data-offset-key="198:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="199"><span data-slate-object="text" data-key="200"><span data-slate-leaf="true" data-offset-key="200:0" data-first-offset="true"><span data-slate-string="true">inputs, labels = inputs.to(device), labels.to(device)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="201"><span data-slate-object="text" data-key="202"><span data-slate-leaf="true" data-offset-key="202:0" data-first-offset="true"><span data-slate-string="true">net = ASimpleNet()</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="203"><span data-slate-object="text" data-key="204"><span data-slate-leaf="true" data-offset-key="204:0" data-first-offset="true"><span data-slate-string="true">net = nn.DataParallel(net)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="205"><span data-slate-object="text" data-key="206"><span data-slate-leaf="true" data-offset-key="206:0" data-first-offset="true"><span data-slate-string="true">net.to(device)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="207"><span data-slate-object="text" data-key="208"><span data-slate-leaf="true" data-offset-key="208:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">print</span></span></span></span><span data-slate-object="text" data-key="209"><span data-slate-leaf="true" data-offset-key="209:0" data-first-offset="true"><span data-slate-string="true">(</span></span></span><span data-slate-object="text" data-key="210"><span data-slate-leaf="true" data-offset-key="210:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"CUDA_VISIBLE_DEVICES :{}"</span></span></span></span><span data-slate-object="text" data-key="211"><span data-slate-leaf="true" data-offset-key="211:0" data-first-offset="true"><span data-slate-string="true">.</span></span></span><span data-slate-object="text" data-key="212"><span data-slate-leaf="true" data-offset-key="212:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">format</span></span></span></span><span data-slate-object="text" data-key="213"><span data-slate-leaf="true" data-offset-key="213:0" data-first-offset="true"><span data-slate-string="true">(os.environ[</span></span></span><span data-slate-object="text" data-key="214"><span data-slate-leaf="true" data-offset-key="214:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"CUDA_VISIBLE_DEVICES"</span></span></span></span><span data-slate-object="text" data-key="215"><span data-slate-leaf="true" data-offset-key="215:0" data-first-offset="true"><span data-slate-string="true">]))</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="216"></div><div data-slate-type="code-line" data-slate-object="block" data-key="217"><span data-slate-object="text" data-key="218"><span data-slate-leaf="true" data-offset-key="218:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">for</span></span></span></span><span data-slate-object="text" data-key="219"><span data-slate-leaf="true" data-offset-key="219:0" data-first-offset="true"><span data-slate-string="true"> epoch </span></span></span><span data-slate-object="text" data-key="220"><span data-slate-leaf="true" data-offset-key="220:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">in</span></span></span></span><span data-slate-object="text" data-key="221"><span data-slate-leaf="true" data-offset-key="221:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="222"><span data-slate-leaf="true" data-offset-key="222:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">range</span></span></span></span><span data-slate-object="text" data-key="223"><span data-slate-leaf="true" data-offset-key="223:0" data-first-offset="true"><span data-slate-string="true">(</span></span></span><span data-slate-object="text" data-key="224"><span data-slate-leaf="true" data-offset-key="224:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">1</span></span></span></span><span data-slate-object="text" data-key="225"><span data-slate-leaf="true" data-offset-key="225:0" data-first-offset="true"><span data-slate-string="true">):</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="226"><span data-slate-object="text" data-key="227"><span data-slate-leaf="true" data-offset-key="227:0" data-first-offset="true"><span data-slate-string="true">    outputs = net(inputs)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="228"></div><div data-slate-type="code-line" data-slate-object="block" data-key="229"><span data-slate-object="text" data-key="230"><span data-slate-leaf="true" data-offset-key="230:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># Get:</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="231"><span data-slate-object="text" data-key="232"><span data-slate-leaf="true" data-offset-key="232:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># CUDA_VISIBLE_DEVICES : 3, 2, 1, 0</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="233"><span data-slate-object="text" data-key="234"><span data-slate-leaf="true" data-offset-key="234:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># forward batchsize is: 4</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="235"><span data-slate-object="text" data-key="236"><span data-slate-leaf="true" data-offset-key="236:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># forward batchsize is: 4</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="237"><span data-slate-object="text" data-key="238"><span data-slate-leaf="true" data-offset-key="238:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># forward batchsize is: 4</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="239"><span data-slate-object="text" data-key="240"><span data-slate-leaf="true" data-offset-key="240:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># forward batchsize is: 4</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="241"></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="242"><span data-slate-object="text" data-key="243"><span data-slate-leaf="true" data-offset-key="243:0" data-first-offset="true"><span data-slate-string="true">在上面的程序中，我们通过 CUDA_VISIBLE_DEVICES 得知了当前程序可见的 GPU 数量为 4，而我们的 batch size 为 16，输出每个 GPU 上模型 forward 函数内部的 print 内容，验证了每个 GPU 获得的数据量都是 4 个。这表示，DataParallel 会自动帮我们将数据切分、加载到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="244" id="sr-toc-4"><span data-slate-object="text" data-key="245"><span data-slate-leaf="true" data-offset-key="245:0" data-first-offset="true"><span data-slate-string="true">多机多卡</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="246"><span data-slate-object="text" data-key="247"><span data-slate-leaf="true" data-offset-key="247:0" data-first-offset="true"><span data-slate-string="true">多机多卡一般都是基于集群的方式进行大规模的训练，需要涉及非常多的方面，咱们这节课只讨论最基本的原理和方法。在具体实践中，你可能还会遇到其它网络或环境等问题，届时需要具体问题具体解决。</span></span></span></div><h4 data-slate-type="heading" data-slate-object="block" data-key="248" id="sr-toc-5"><span data-slate-object="text" data-key="249"><span data-slate-leaf="true" data-offset-key="249:0" data-first-offset="true"><span data-slate-string="true">DP 与 DDP</span></span></span></h4><div data-slate-type="paragraph" data-slate-object="block" data-key="250"><span data-slate-object="text" data-key="251"><span data-slate-leaf="true" data-offset-key="251:0" data-first-offset="true"><span data-slate-string="true">刚才我们已经提到，对于单机多卡训练，有一个最简单的办法：DataParallel。其实 PyTorch 的数据并行还有一个主要的 API，那就是 DistributedDataParallel。而 </span></span></span><span data-slate-object="text" data-key="252"><span data-slate-leaf="true" data-offset-key="252:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">DistributedDataParallel 也是我们实现多机多卡的关键 API</span></span></span></span><span data-slate-object="text" data-key="253"><span data-slate-leaf="true" data-offset-key="253:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="254"><span data-slate-object="text" data-key="255"><span data-slate-leaf="true" data-offset-key="255:0" data-first-offset="true"><span data-slate-string="true">DataParallel 简称为 DP，而 DistributedDataParallel 简称为 DDP。我们来详细看看 DP 与 DDP 的区别。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="256"><span data-slate-object="text" data-key="257"><span data-slate-leaf="true" data-offset-key="257:0" data-first-offset="true"><span data-slate-string="true">先看 DP，DP 是单进程控制多 GPU。从之前的程序中，我们也可以看出，DP 将输入的一个 batch 数据分成了 n 份（n 为实际使用的 GPU 数量），分别送到对应的 GPU 进行计算。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="258"><span data-slate-object="text" data-key="259"><span data-slate-leaf="true" data-offset-key="259:0" data-first-offset="true"><span data-slate-string="true">在网络前向传播时，模型会从主 GPU 复制到其它 GPU 上；在反向传播时，每个 GPU 上的梯度汇总到主 GPU 上，求得梯度均值更新模型参数后，再复制到其它 GPU，以此来实现并行。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="260"><span data-slate-object="text" data-key="261"><span data-slate-leaf="true" data-offset-key="261:0" data-first-offset="true"><span data-slate-string="true">由于主 GPU 要进行梯度汇总和模型更新，并将计算任务下发给其它 GPU，所以主 GPU 的负载与使用率会比其它 GPU 高，这就导致了 GPU 负载不均衡的现象。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="262"><span data-slate-object="text" data-key="263"><span data-slate-leaf="true" data-offset-key="263:0" data-first-offset="true"><span data-slate-string="true">再说说 DDP，DDP 多进程控制多 GPU。系统会为每个 GPU 创建一个进程，不再有主 GPU，每个 GPU 执行相同的任务。DDP 使用分布式数据采样器（DistributedSampler）加载数据，确保数据在各个进程之间没有重叠。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="264"><span data-slate-object="text" data-key="265"><span data-slate-leaf="true" data-offset-key="265:0" data-first-offset="true"><span data-slate-string="true">在反向传播时，各 GPU 梯度计算完成后，各进程以广播的方式将梯度进行汇总平均，然后每个进程在各自的 GPU 上进行梯度更新，从而确保每个 GPU 上的模型参数始终保持一致。由于无需在不同 GPU 之间复制模型，DPP 的传输数据量更少，因此速度更快。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="266"><span data-slate-object="text" data-key="267"><span data-slate-leaf="true" data-offset-key="267:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">DistributedDataParallel 既可用于单机多卡也可用于多机多卡</span></span></span></span><span data-slate-object="text" data-key="268"><span data-slate-leaf="true" data-offset-key="268:0" data-first-offset="true"><span data-slate-string="true">，它能够解决 DataParallel 速度慢、GPU 负载不均衡等问题。因此，官方更推荐使用 DistributedDataParallel 来进行分布式训练，也就是接下来要说的 DDP 训练。</span></span></span></div><h4 data-slate-type="heading" data-slate-object="block" data-key="269" id="sr-toc-6"><span data-slate-object="text" data-key="270"><span data-slate-leaf="true" data-offset-key="270:0" data-first-offset="true"><span data-slate-string="true">DDP 训练</span></span></span></h4><div data-slate-type="paragraph" data-slate-object="block" data-key="271"><span data-slate-object="text" data-key="272"><span data-slate-leaf="true" data-offset-key="272:0" data-first-offset="true"><span data-slate-string="true">DistributedDataParallel 主要是为多机多卡而设计的，不过单机上也同样可以使用。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="273"><span data-slate-object="text" data-key="274"><span data-slate-leaf="true" data-offset-key="274:0" data-first-offset="true"><span data-slate-string="true">想要弄明白 DPP 的训练机制，我们先要弄明白这几个分布式中的概念：</span></span></span></div><div data-slate-type="list" data-slate-object="block" data-key="275"><div data-slate-type="list-line" data-slate-object="block" data-key="276"><span data-slate-object="text" data-key="277"><span data-slate-leaf="true" data-offset-key="277:0" data-first-offset="true"><span data-slate-string="true">group：即进程组。默认情况下，只有一个组，即一个 world。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="278"><span data-slate-object="text" data-key="279"><span data-slate-leaf="true" data-offset-key="279:0" data-first-offset="true"><span data-slate-string="true">world_size ：表示全局进程个数。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="280"><span data-slate-object="text" data-key="281"><span data-slate-leaf="true" data-offset-key="281:0" data-first-offset="true"><span data-slate-string="true">rank：表示进程序号，用于进程间通讯，表示进程优先级。rank=0 的主机为主节点。</span></span></span></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="282"><span data-slate-object="text" data-key="283"><span data-slate-leaf="true" data-offset-key="283:0" data-first-offset="true"><span data-slate-string="true">使用 DDP 进行分布式训练的具体流程如下。接下来，我们就按步骤分别去实现。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="284"><div class="sr-rd-content-center"><img class="sr-rd-content-img" src="https://static001.geekbang.org/resource/image/27/7d/2730a8d7e7e1fe21574918a2dc48c67d.jpg?wh=1920x1009"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="285"><span data-slate-object="text" data-key="286"><span data-slate-leaf="true" data-offset-key="286:0" data-first-offset="true"><span data-slate-string="true">第一步，初始化进程组。我们使用 init_process_group 函数来进行分布式初始化，其定义如下：</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="287"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="288"><span data-slate-object="text" data-key="289"><span data-slate-leaf="true" data-offset-key="289:0" data-first-offset="true"><span data-slate-string="true">torch.distributed.init_process_group(backend, init_method=</span></span></span><span data-slate-object="text" data-key="290"><span data-slate-leaf="true" data-offset-key="290:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">None</span></span></span></span><span data-slate-object="text" data-key="291"><span data-slate-leaf="true" data-offset-key="291:0" data-first-offset="true"><span data-slate-string="true">,, world_size=-</span></span></span><span data-slate-object="text" data-key="292"><span data-slate-leaf="true" data-offset-key="292:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">1</span></span></span></span><span data-slate-object="text" data-key="293"><span data-slate-leaf="true" data-offset-key="293:0" data-first-offset="true"><span data-slate-string="true">, rank=-</span></span></span><span data-slate-object="text" data-key="294"><span data-slate-leaf="true" data-offset-key="294:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">1</span></span></span></span><span data-slate-object="text" data-key="295"><span data-slate-leaf="true" data-offset-key="295:0" data-first-offset="true"><span data-slate-string="true">, group_name=</span></span></span><span data-slate-object="text" data-key="296"><span data-slate-leaf="true" data-offset-key="296:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">''</span></span></span></span><span data-slate-object="text" data-key="297"><span data-slate-leaf="true" data-offset-key="297:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="298"><span data-slate-object="text" data-key="299"><span data-slate-leaf="true" data-offset-key="299:0" data-first-offset="true"><span data-slate-string="true">我们分别看看定义里的相关参数：</span></span></span></div><div data-slate-type="list" data-slate-object="block" data-key="300"><div data-slate-type="list-line" data-slate-object="block" data-key="301"><span data-slate-object="text" data-key="302"><span data-slate-leaf="true" data-offset-key="302:0" data-first-offset="true"><span data-slate-string="true">backend：是通信所用的后端，可以是 “nccl” 或 “gloo”。一般来说，nccl 用于 GPU 分布式训练，gloo 用于 CPU 进行分布式训练。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="303"><span data-slate-object="text" data-key="304"><span data-slate-leaf="true" data-offset-key="304:0" data-first-offset="true"><span data-slate-string="true">init_method：字符串类型，是一个 url，用于指定进程初始化方式，默认是 “env://”，表示从环境变量初始化，还可以使用 TCP 的方式或共享文件系统 &nbsp;。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="305"><span data-slate-object="text" data-key="306"><span data-slate-leaf="true" data-offset-key="306:0" data-first-offset="true"><span data-slate-string="true">world_size：执行训练的所有的进程数，表示一共有多少个节点（机器）。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="307"><span data-slate-object="text" data-key="308"><span data-slate-leaf="true" data-offset-key="308:0" data-first-offset="true"><span data-slate-string="true">rank：进程的编号，也是其优先级，表示当前节点（机器）的编号。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="309"><span data-slate-object="text" data-key="310"><span data-slate-leaf="true" data-offset-key="310:0" data-first-offset="true"><span data-slate-string="true">group_name：进程组的名字。</span></span></span></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="311"><span data-slate-object="text" data-key="312"><span data-slate-leaf="true" data-offset-key="312:0" data-first-offset="true"><span data-slate-string="true">使用 nccl 后端的代码如下。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="313"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="314"><span data-slate-object="text" data-key="315"><span data-slate-leaf="true" data-offset-key="315:0" data-first-offset="true"><span data-slate-string="true">torch.distributed.init_process_group(backend=</span></span></span><span data-slate-object="text" data-key="316"><span data-slate-leaf="true" data-offset-key="316:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"nccl"</span></span></span></span><span data-slate-object="text" data-key="317"><span data-slate-leaf="true" data-offset-key="317:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="318"><span data-slate-object="text" data-key="319"><span data-slate-leaf="true" data-offset-key="319:0" data-first-offset="true"><span data-slate-string="true">完成初始化以后，第二步就是模型并行化。正如前面讲过的，我们可以使用 DistributedDataParallel，将模型分发至多 GPU 上，其定义如下：</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="320"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="321"><span data-slate-object="text" data-key="322"><span data-slate-leaf="true" data-offset-key="322:0" data-first-offset="true"><span data-slate-string="true">torch.nn.parallel.DistributedDataParallel(module, device_ids=</span></span></span><span data-slate-object="text" data-key="323"><span data-slate-leaf="true" data-offset-key="323:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">None</span></span></span></span><span data-slate-object="text" data-key="324"><span data-slate-leaf="true" data-offset-key="324:0" data-first-offset="true"><span data-slate-string="true">, output_device=</span></span></span><span data-slate-object="text" data-key="325"><span data-slate-leaf="true" data-offset-key="325:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">None</span></span></span></span><span data-slate-object="text" data-key="326"><span data-slate-leaf="true" data-offset-key="326:0" data-first-offset="true"><span data-slate-string="true">, dim=</span></span></span><span data-slate-object="text" data-key="327"><span data-slate-leaf="true" data-offset-key="327:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">0</span></span></span></span><span data-slate-object="text" data-key="328"><span data-slate-leaf="true" data-offset-key="328:0" data-first-offset="true"><span data-slate-string="true">）</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="329"><span data-slate-object="text" data-key="330"><span data-slate-leaf="true" data-offset-key="330:0" data-first-offset="true"><span data-slate-string="true">DistributedDataParallel 的参数与 DataParallel 基本相同，因此模型并行化的用法只需将 DataParallel 函数替换成 DistributedDataParallel 即可，具体代码如下。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="331"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="332"><span data-slate-object="text" data-key="333"><span data-slate-leaf="true" data-offset-key="333:0" data-first-offset="true"><span data-slate-string="true">net = torch.nn.parallel.DistributedDataParallel(net)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="334"><span data-slate-object="text" data-key="335"><span data-slate-leaf="true" data-offset-key="335:0" data-first-offset="true"><span data-slate-string="true">最后就是创建分布式数据采样器。在多机多卡情况下，分布式训练数据的读取也是一个问题，不同的卡读取到的数据应该是不同的。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="336"><span data-slate-object="text" data-key="337"><span data-slate-leaf="true" data-offset-key="337:0" data-first-offset="true"><span data-slate-string="true">DP 是直接将一个 batch 的数据划分到不同的卡，但是多机多卡之间进行频繁的数据传输会严重影响效率，这时就要用到分布式数据采样器 DistributedSampler，它会为每个子进程划分出一部分数据集，从而使 DataLoader 只会加载特定的一个子数据集，以避免不同进程之间有数据重复。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="338"><span data-slate-object="text" data-key="339"><span data-slate-leaf="true" data-offset-key="339:0" data-first-offset="true"><span data-slate-string="true">创建与使用分布式数据采样器的代码如下。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="340"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="341"><span data-slate-object="text" data-key="342"><span data-slate-leaf="true" data-offset-key="342:0" data-first-offset="true"><span data-slate-string="true">train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="343"><span data-slate-object="text" data-key="344"><span data-slate-leaf="true" data-offset-key="344:0" data-first-offset="true"><span data-slate-string="true">data_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="345"><span data-slate-object="text" data-key="346"><span data-slate-leaf="true" data-offset-key="346:0" data-first-offset="true"><span data-slate-string="true">结合代码我给你解读一下。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="347"><span data-slate-object="text" data-key="348"><span data-slate-leaf="true" data-offset-key="348:0" data-first-offset="true"><span data-slate-string="true">首先，我们将 train_dataset 送到了 DistributedSampler 中，并创建了一个分布式数据采样器 train_sampler。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="349"><span data-slate-object="text" data-key="350"><span data-slate-leaf="true" data-offset-key="350:0" data-first-offset="true"><span data-slate-string="true">然后在构造 DataLoader 的时候，&nbsp; 参数中传入了一个 sampler=train_sampler，即可让不同的进程节点加载属于自己的那份子数据集。也就是说，使用 DDP 时，不再是从主 GPU 分发数据到其他 GPU 上，而是各 GPU 从自己的硬盘上读取属于自己的那份数据。</span></span></span></div><h1 data-slate-type="heading" data-slate-object="block" data-key="351" id="sr-toc-7"></h1><div data-slate-type="paragraph" data-slate-object="block" data-key="352"><span data-slate-object="text" data-key="353"><span data-slate-leaf="true" data-offset-key="353:0" data-first-offset="true"><span data-slate-string="true">为什么要使用分布式训练以及分布式训练的原理我们就讲到这里。相信你已经对数据并行与模型并行都有了一个初步的认识。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="354" id="sr-toc-8"><span data-slate-object="text" data-key="355"><span data-slate-leaf="true" data-offset-key="355:0" data-first-offset="true"><span data-slate-string="true">小试牛刀</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="356"><span data-slate-object="text" data-key="357"><span data-slate-leaf="true" data-offset-key="357:0" data-first-offset="true"><span data-slate-string="true">下面我们将会讲解一个</span></span></span><a data-slate-type="link" data-slate-object="inline" data-key="358"><span data-slate-object="text" data-key="359"><span data-slate-leaf="true" data-offset-key="359:0" data-first-offset="true"><span data-slate-string="true">官方的 ImageNet 的示例</span></span></span></a><span data-slate-object="text" data-key="360"><span data-slate-leaf="true" data-offset-key="360:0" data-first-offset="true"><span data-slate-string="true">，以后你可以把这个小项目当做分布式训练的一个模板来使用。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="361"><span data-slate-object="text" data-key="362"><span data-slate-leaf="true" data-offset-key="362:0" data-first-offset="true"><span data-slate-string="true">这个示例可对使用 DP 或 DDP 进行选配，下面我们就一起来看核心代码。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="363"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div><div data-code-line-number="6"></div><div data-code-line-number="7"></div><div data-code-line-number="8"></div><div data-code-line-number="9"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="364"><span data-slate-object="text" data-key="365"><span data-slate-leaf="true" data-offset-key="365:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">if</span></span></span></span><span data-slate-object="text" data-key="366"><span data-slate-leaf="true" data-offset-key="366:0" data-first-offset="true"><span data-slate-string="true"> args.distributed:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="367"><span data-slate-object="text" data-key="368"><span data-slate-leaf="true" data-offset-key="368:0" data-first-offset="true"><span data-slate-string="true">     </span></span></span><span data-slate-object="text" data-key="369"><span data-slate-leaf="true" data-offset-key="369:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">if</span></span></span></span><span data-slate-object="text" data-key="370"><span data-slate-leaf="true" data-offset-key="370:0" data-first-offset="true"><span data-slate-string="true"> args.dist_url == </span></span></span><span data-slate-object="text" data-key="371"><span data-slate-leaf="true" data-offset-key="371:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"env://"</span></span></span></span><span data-slate-object="text" data-key="372"><span data-slate-leaf="true" data-offset-key="372:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="373"><span data-slate-leaf="true" data-offset-key="373:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">and</span></span></span></span><span data-slate-object="text" data-key="374"><span data-slate-leaf="true" data-offset-key="374:0" data-first-offset="true"><span data-slate-string="true"> args.rank == -</span></span></span><span data-slate-object="text" data-key="375"><span data-slate-leaf="true" data-offset-key="375:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">1</span></span></span></span><span data-slate-object="text" data-key="376"><span data-slate-leaf="true" data-offset-key="376:0" data-first-offset="true"><span data-slate-string="true">:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="377"><span data-slate-object="text" data-key="378"><span data-slate-leaf="true" data-offset-key="378:0" data-first-offset="true"><span data-slate-string="true">         args.rank = </span></span></span><span data-slate-object="text" data-key="379"><span data-slate-leaf="true" data-offset-key="379:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">int</span></span></span></span><span data-slate-object="text" data-key="380"><span data-slate-leaf="true" data-offset-key="380:0" data-first-offset="true"><span data-slate-string="true">(os.environ[</span></span></span><span data-slate-object="text" data-key="381"><span data-slate-leaf="true" data-offset-key="381:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"RANK"</span></span></span></span><span data-slate-object="text" data-key="382"><span data-slate-leaf="true" data-offset-key="382:0" data-first-offset="true"><span data-slate-string="true">])</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="383"><span data-slate-object="text" data-key="384"><span data-slate-leaf="true" data-offset-key="384:0" data-first-offset="true"><span data-slate-string="true">     </span></span></span><span data-slate-object="text" data-key="385"><span data-slate-leaf="true" data-offset-key="385:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">if</span></span></span></span><span data-slate-object="text" data-key="386"><span data-slate-leaf="true" data-offset-key="386:0" data-first-offset="true"><span data-slate-string="true"> args.multiprocessing_distributed:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="387"><span data-slate-object="text" data-key="388"><span data-slate-leaf="true" data-offset-key="388:0" data-first-offset="true"><span data-slate-string="true">         </span></span></span><span data-slate-object="text" data-key="389"><span data-slate-leaf="true" data-offset-key="389:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># For multiprocessing distributed training, rank needs to be the</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="390"><span data-slate-object="text" data-key="391"><span data-slate-leaf="true" data-offset-key="391:0" data-first-offset="true"><span data-slate-string="true">         </span></span></span><span data-slate-object="text" data-key="392"><span data-slate-leaf="true" data-offset-key="392:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># global rank among all the processes</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="393"><span data-slate-object="text" data-key="394"><span data-slate-leaf="true" data-offset-key="394:0" data-first-offset="true"><span data-slate-string="true">         args.rank = args.rank * ngpus_per_node + gpu</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="395"><span data-slate-object="text" data-key="396"><span data-slate-leaf="true" data-offset-key="396:0" data-first-offset="true"><span data-slate-string="true">     dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="397"><span data-slate-object="text" data-key="398"><span data-slate-leaf="true" data-offset-key="398:0" data-first-offset="true"><span data-slate-string="true">                             world_size=args.world_size, rank=args.rank)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="399"><span data-slate-object="text" data-key="400"><span data-slate-leaf="true" data-offset-key="400:0" data-first-offset="true"><span data-slate-string="true">这里你可以重点关注示例代码中的 “args.distributed” 参数，args.distributed 为 True，表示使用 DDP，反之表示使用 DP。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="401"><span data-slate-object="text" data-key="402"><span data-slate-leaf="true" data-offset-key="402:0" data-first-offset="true"><span data-slate-string="true">我们来看 main_worker 函数中这段针对 DDP 的初始化代码，如果使用 DDP，那么使用 init_process_group 函数初始化进程组。ngpus_per_node 表示每个节点的 GPU 数量。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="403"><span data-slate-object="text" data-key="404"><span data-slate-leaf="true" data-offset-key="404:0" data-first-offset="true"><span data-slate-string="true">我们再来看 main_worker 函数中的这段逻辑代码。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="405"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div><div data-code-line-number="6"></div><div data-code-line-number="7"></div><div data-code-line-number="8"></div><div data-code-line-number="9"></div><div data-code-line-number="10"></div><div data-code-line-number="11"></div><div data-code-line-number="12"></div><div data-code-line-number="13"></div><div data-code-line-number="14"></div><div data-code-line-number="15"></div><div data-code-line-number="16"></div><div data-code-line-number="17"></div><div data-code-line-number="18"></div><div data-code-line-number="19"></div><div data-code-line-number="20"></div><div data-code-line-number="21"></div><div data-code-line-number="22"></div><div data-code-line-number="23"></div><div data-code-line-number="24"></div><div data-code-line-number="25"></div><div data-code-line-number="26"></div><div data-code-line-number="27"></div><div data-code-line-number="28"></div><div data-code-line-number="29"></div><div data-code-line-number="30"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="406"><span data-slate-object="text" data-key="407"><span data-slate-leaf="true" data-offset-key="407:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">if</span></span></span></span><span data-slate-object="text" data-key="408"><span data-slate-leaf="true" data-offset-key="408:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="409"><span data-slate-leaf="true" data-offset-key="409:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">not</span></span></span></span><span data-slate-object="text" data-key="410"><span data-slate-leaf="true" data-offset-key="410:0" data-first-offset="true"><span data-slate-string="true"> torch.cuda.is_available():</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="411"><span data-slate-object="text" data-key="412"><span data-slate-leaf="true" data-offset-key="412:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="413"><span data-slate-leaf="true" data-offset-key="413:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">print</span></span></span></span><span data-slate-object="text" data-key="414"><span data-slate-leaf="true" data-offset-key="414:0" data-first-offset="true"><span data-slate-string="true">(</span></span></span><span data-slate-object="text" data-key="415"><span data-slate-leaf="true" data-offset-key="415:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'using CPU, this will be slow'</span></span></span></span><span data-slate-object="text" data-key="416"><span data-slate-leaf="true" data-offset-key="416:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="417"><span data-slate-object="text" data-key="418"><span data-slate-leaf="true" data-offset-key="418:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">elif</span></span></span></span><span data-slate-object="text" data-key="419"><span data-slate-leaf="true" data-offset-key="419:0" data-first-offset="true"><span data-slate-string="true"> args.distributed:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="420"><span data-slate-object="text" data-key="421"><span data-slate-leaf="true" data-offset-key="421:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="422"><span data-slate-leaf="true" data-offset-key="422:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># For multiprocessing distributed, DistributedDataParallel constructor</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="423"><span data-slate-object="text" data-key="424"><span data-slate-leaf="true" data-offset-key="424:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="425"><span data-slate-leaf="true" data-offset-key="425:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># should always set the single device scope, otherwise,</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="426"><span data-slate-object="text" data-key="427"><span data-slate-leaf="true" data-offset-key="427:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="428"><span data-slate-leaf="true" data-offset-key="428:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># DistributedDataParallel will use all available devices.</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="429"><span data-slate-object="text" data-key="430"><span data-slate-leaf="true" data-offset-key="430:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="431"><span data-slate-leaf="true" data-offset-key="431:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">if</span></span></span></span><span data-slate-object="text" data-key="432"><span data-slate-leaf="true" data-offset-key="432:0" data-first-offset="true"><span data-slate-string="true"> args.gpu </span></span></span><span data-slate-object="text" data-key="433"><span data-slate-leaf="true" data-offset-key="433:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">is</span></span></span></span><span data-slate-object="text" data-key="434"><span data-slate-leaf="true" data-offset-key="434:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="435"><span data-slate-leaf="true" data-offset-key="435:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">not</span></span></span></span><span data-slate-object="text" data-key="436"><span data-slate-leaf="true" data-offset-key="436:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="437"><span data-slate-leaf="true" data-offset-key="437:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">None</span></span></span></span><span data-slate-object="text" data-key="438"><span data-slate-leaf="true" data-offset-key="438:0" data-first-offset="true"><span data-slate-string="true">:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="439"><span data-slate-object="text" data-key="440"><span data-slate-leaf="true" data-offset-key="440:0" data-first-offset="true"><span data-slate-string="true">        torch.cuda.set_device(args.gpu)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="441"><span data-slate-object="text" data-key="442"><span data-slate-leaf="true" data-offset-key="442:0" data-first-offset="true"><span data-slate-string="true">        model.cuda(args.gpu)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="443"><span data-slate-object="text" data-key="444"><span data-slate-leaf="true" data-offset-key="444:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="445"><span data-slate-leaf="true" data-offset-key="445:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># When using a single GPU per process and per</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="446"><span data-slate-object="text" data-key="447"><span data-slate-leaf="true" data-offset-key="447:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="448"><span data-slate-leaf="true" data-offset-key="448:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># DistributedDataParallel, we need to divide the batch size</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="449"><span data-slate-object="text" data-key="450"><span data-slate-leaf="true" data-offset-key="450:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="451"><span data-slate-leaf="true" data-offset-key="451:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># ourselves based on the total number of GPUs we have</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="452"><span data-slate-object="text" data-key="453"><span data-slate-leaf="true" data-offset-key="453:0" data-first-offset="true"><span data-slate-string="true">        args.batch_size = </span></span></span><span data-slate-object="text" data-key="454"><span data-slate-leaf="true" data-offset-key="454:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">int</span></span></span></span><span data-slate-object="text" data-key="455"><span data-slate-leaf="true" data-offset-key="455:0" data-first-offset="true"><span data-slate-string="true">(args.batch_size / ngpus_per_node)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="456"><span data-slate-object="text" data-key="457"><span data-slate-leaf="true" data-offset-key="457:0" data-first-offset="true"><span data-slate-string="true">        args.workers = </span></span></span><span data-slate-object="text" data-key="458"><span data-slate-leaf="true" data-offset-key="458:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">int</span></span></span></span><span data-slate-object="text" data-key="459"><span data-slate-leaf="true" data-offset-key="459:0" data-first-offset="true"><span data-slate-string="true">((args.workers + ngpus_per_node - </span></span></span><span data-slate-object="text" data-key="460"><span data-slate-leaf="true" data-offset-key="460:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">1</span></span></span></span><span data-slate-object="text" data-key="461"><span data-slate-leaf="true" data-offset-key="461:0" data-first-offset="true"><span data-slate-string="true">) / ngpus_per_node)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="462"><span data-slate-object="text" data-key="463"><span data-slate-leaf="true" data-offset-key="463:0" data-first-offset="true"><span data-slate-string="true">        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="464"><span data-slate-object="text" data-key="465"><span data-slate-leaf="true" data-offset-key="465:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="466"><span data-slate-leaf="true" data-offset-key="466:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">else</span></span></span></span><span data-slate-object="text" data-key="467"><span data-slate-leaf="true" data-offset-key="467:0" data-first-offset="true"><span data-slate-string="true">:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="468"><span data-slate-object="text" data-key="469"><span data-slate-leaf="true" data-offset-key="469:0" data-first-offset="true"><span data-slate-string="true">        model.cuda()</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="470"><span data-slate-object="text" data-key="471"><span data-slate-leaf="true" data-offset-key="471:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="472"><span data-slate-leaf="true" data-offset-key="472:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># DistributedDataParallel will divide and allocate batch_size to all</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="473"><span data-slate-object="text" data-key="474"><span data-slate-leaf="true" data-offset-key="474:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="475"><span data-slate-leaf="true" data-offset-key="475:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># available GPUs if device_ids are not set</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="476"><span data-slate-object="text" data-key="477"><span data-slate-leaf="true" data-offset-key="477:0" data-first-offset="true"><span data-slate-string="true">        model = torch.nn.parallel.DistributedDataParallel(model)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="478"><span data-slate-object="text" data-key="479"><span data-slate-leaf="true" data-offset-key="479:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">elif</span></span></span></span><span data-slate-object="text" data-key="480"><span data-slate-leaf="true" data-offset-key="480:0" data-first-offset="true"><span data-slate-string="true"> args.gpu </span></span></span><span data-slate-object="text" data-key="481"><span data-slate-leaf="true" data-offset-key="481:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">is</span></span></span></span><span data-slate-object="text" data-key="482"><span data-slate-leaf="true" data-offset-key="482:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="483"><span data-slate-leaf="true" data-offset-key="483:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">not</span></span></span></span><span data-slate-object="text" data-key="484"><span data-slate-leaf="true" data-offset-key="484:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="485"><span data-slate-leaf="true" data-offset-key="485:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">None</span></span></span></span><span data-slate-object="text" data-key="486"><span data-slate-leaf="true" data-offset-key="486:0" data-first-offset="true"><span data-slate-string="true">:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="487"><span data-slate-object="text" data-key="488"><span data-slate-leaf="true" data-offset-key="488:0" data-first-offset="true"><span data-slate-string="true">    torch.cuda.set_device(args.gpu)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="489"><span data-slate-object="text" data-key="490"><span data-slate-leaf="true" data-offset-key="490:0" data-first-offset="true"><span data-slate-string="true">    model = model.cuda(args.gpu)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="491"><span data-slate-object="text" data-key="492"><span data-slate-leaf="true" data-offset-key="492:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">else</span></span></span></span><span data-slate-object="text" data-key="493"><span data-slate-leaf="true" data-offset-key="493:0" data-first-offset="true"><span data-slate-string="true">:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="494"><span data-slate-object="text" data-key="495"><span data-slate-leaf="true" data-offset-key="495:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="496"><span data-slate-leaf="true" data-offset-key="496:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># DataParallel will divide and allocate batch_size to all available GPUs</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="497"><span data-slate-object="text" data-key="498"><span data-slate-leaf="true" data-offset-key="498:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="499"><span data-slate-leaf="true" data-offset-key="499:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">if</span></span></span></span><span data-slate-object="text" data-key="500"><span data-slate-leaf="true" data-offset-key="500:0" data-first-offset="true"><span data-slate-string="true"> args.arch.startswith(</span></span></span><span data-slate-object="text" data-key="501"><span data-slate-leaf="true" data-offset-key="501:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'alexnet'</span></span></span></span><span data-slate-object="text" data-key="502"><span data-slate-leaf="true" data-offset-key="502:0" data-first-offset="true"><span data-slate-string="true">) </span></span></span><span data-slate-object="text" data-key="503"><span data-slate-leaf="true" data-offset-key="503:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">or</span></span></span></span><span data-slate-object="text" data-key="504"><span data-slate-leaf="true" data-offset-key="504:0" data-first-offset="true"><span data-slate-string="true"> args.arch.startswith(</span></span></span><span data-slate-object="text" data-key="505"><span data-slate-leaf="true" data-offset-key="505:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'vgg'</span></span></span></span><span data-slate-object="text" data-key="506"><span data-slate-leaf="true" data-offset-key="506:0" data-first-offset="true"><span data-slate-string="true">):</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="507"><span data-slate-object="text" data-key="508"><span data-slate-leaf="true" data-offset-key="508:0" data-first-offset="true"><span data-slate-string="true">        model.features = torch.nn.DataParallel(model.features)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="509"><span data-slate-object="text" data-key="510"><span data-slate-leaf="true" data-offset-key="510:0" data-first-offset="true"><span data-slate-string="true">        model.cuda()</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="511"><span data-slate-object="text" data-key="512"><span data-slate-leaf="true" data-offset-key="512:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="513"><span data-slate-leaf="true" data-offset-key="513:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">else</span></span></span></span><span data-slate-object="text" data-key="514"><span data-slate-leaf="true" data-offset-key="514:0" data-first-offset="true"><span data-slate-string="true">:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="515"><span data-slate-object="text" data-key="516"><span data-slate-leaf="true" data-offset-key="516:0" data-first-offset="true"><span data-slate-string="true">        model = torch.nn.DataParallel(model).cuda()</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="517"><span data-slate-object="text" data-key="518"><span data-slate-leaf="true" data-offset-key="518:0" data-first-offset="true"><span data-slate-string="true">这段代码是对使用 CPU 还是使用 GPU、如果使用 GPU，是使用 DP 还是 DDP 进行了逻辑选择。我们可以看到，这里用到了 DistributedDataParallel 函数或 DataParallel 函数，对模型进行并行化。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="519"><span data-slate-object="text" data-key="520"><span data-slate-leaf="true" data-offset-key="520:0" data-first-offset="true"><span data-slate-string="true">并行化之后就是创建分布式数据采样器，具体代码如下。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="521"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div><div data-code-line-number="6"></div><div data-code-line-number="7"></div><div data-code-line-number="8"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="522"><span data-slate-object="text" data-key="523"><span data-slate-leaf="true" data-offset-key="523:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">if</span></span></span></span><span data-slate-object="text" data-key="524"><span data-slate-leaf="true" data-offset-key="524:0" data-first-offset="true"><span data-slate-string="true"> args.distributed:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="525"><span data-slate-object="text" data-key="526"><span data-slate-leaf="true" data-offset-key="526:0" data-first-offset="true"><span data-slate-string="true">    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="527"><span data-slate-object="text" data-key="528"><span data-slate-leaf="true" data-offset-key="528:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">else</span></span></span></span><span data-slate-object="text" data-key="529"><span data-slate-leaf="true" data-offset-key="529:0" data-first-offset="true"><span data-slate-string="true">:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="530"><span data-slate-object="text" data-key="531"><span data-slate-leaf="true" data-offset-key="531:0" data-first-offset="true"><span data-slate-string="true">    train_sampler = </span></span></span><span data-slate-object="text" data-key="532"><span data-slate-leaf="true" data-offset-key="532:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">None</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="533"></div><div data-slate-type="code-line" data-slate-object="block" data-key="534"><span data-slate-object="text" data-key="535"><span data-slate-leaf="true" data-offset-key="535:0" data-first-offset="true"><span data-slate-string="true">train_loader = torch.utils.data.DataLoader(</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="536"><span data-slate-object="text" data-key="537"><span data-slate-leaf="true" data-offset-key="537:0" data-first-offset="true"><span data-slate-string="true">    train_dataset, batch_size=args.batch_size, shuffle=(train_sampler </span></span></span><span data-slate-object="text" data-key="538"><span data-slate-leaf="true" data-offset-key="538:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">is</span></span></span></span><span data-slate-object="text" data-key="539"><span data-slate-leaf="true" data-offset-key="539:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="540"><span data-slate-leaf="true" data-offset-key="540:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">None</span></span></span></span><span data-slate-object="text" data-key="541"><span data-slate-leaf="true" data-offset-key="541:0" data-first-offset="true"><span data-slate-string="true">),</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="542"><span data-slate-object="text" data-key="543"><span data-slate-leaf="true" data-offset-key="543:0" data-first-offset="true"><span data-slate-string="true">    num_workers=args.workers, pin_memory=</span></span></span><span data-slate-object="text" data-key="544"><span data-slate-leaf="true" data-offset-key="544:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">True</span></span></span></span><span data-slate-object="text" data-key="545"><span data-slate-leaf="true" data-offset-key="545:0" data-first-offset="true"><span data-slate-string="true">, sampler=train_sample</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="546"><span data-slate-object="text" data-key="547"><span data-slate-leaf="true" data-offset-key="547:0" data-first-offset="true"><span data-slate-string="true">这里需要注意的是，</span></span></span><span data-slate-object="text" data-key="548"><span data-slate-leaf="true" data-offset-key="548:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">在建立 Dataloader 的过程中，如果 sampler 参数不为 None，那么 shuffle 参数不应该被设置</span></span></span></span><span data-slate-object="text" data-key="549"><span data-slate-leaf="true" data-offset-key="549:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="550"><span data-slate-object="text" data-key="551"><span data-slate-leaf="true" data-offset-key="551:0" data-first-offset="true"><span data-slate-string="true">最后，我们需要为每个机器节点上的每个 GPU 启动一个进程。PyTorch 提供了 torch.multiprocessing.spawn 函数，来在一个节点启动该节点所有进程，具体的代码如下。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="552"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div><div data-code-line-number="6"></div><div data-code-line-number="7"></div><div data-code-line-number="8"></div><div data-code-line-number="9"></div><div data-code-line-number="10"></div><div data-code-line-number="11"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="553"><span data-slate-object="text" data-key="554"><span data-slate-leaf="true" data-offset-key="554:0" data-first-offset="true"><span data-slate-string="true"> ngpus_per_node = torch.cuda.device_count()</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="555"><span data-slate-object="text" data-key="556"><span data-slate-leaf="true" data-offset-key="556:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="557"><span data-slate-leaf="true" data-offset-key="557:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">if</span></span></span></span><span data-slate-object="text" data-key="558"><span data-slate-leaf="true" data-offset-key="558:0" data-first-offset="true"><span data-slate-string="true"> args.multiprocessing_distributed:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="559"><span data-slate-object="text" data-key="560"><span data-slate-leaf="true" data-offset-key="560:0" data-first-offset="true"><span data-slate-string="true">     </span></span></span><span data-slate-object="text" data-key="561"><span data-slate-leaf="true" data-offset-key="561:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># Since we have ngpus_per_node processes per node, the total world_size</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="562"><span data-slate-object="text" data-key="563"><span data-slate-leaf="true" data-offset-key="563:0" data-first-offset="true"><span data-slate-string="true">     </span></span></span><span data-slate-object="text" data-key="564"><span data-slate-leaf="true" data-offset-key="564:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># needs to be adjusted accordingly</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="565"><span data-slate-object="text" data-key="566"><span data-slate-leaf="true" data-offset-key="566:0" data-first-offset="true"><span data-slate-string="true">     args.world_size = ngpus_per_node * args.world_size</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="567"><span data-slate-object="text" data-key="568"><span data-slate-leaf="true" data-offset-key="568:0" data-first-offset="true"><span data-slate-string="true">     </span></span></span><span data-slate-object="text" data-key="569"><span data-slate-leaf="true" data-offset-key="569:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># Use torch.multiprocessing.spawn to launch distributed processes: the</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="570"><span data-slate-object="text" data-key="571"><span data-slate-leaf="true" data-offset-key="571:0" data-first-offset="true"><span data-slate-string="true">     </span></span></span><span data-slate-object="text" data-key="572"><span data-slate-leaf="true" data-offset-key="572:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># main_worker process function</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="573"><span data-slate-object="text" data-key="574"><span data-slate-leaf="true" data-offset-key="574:0" data-first-offset="true"><span data-slate-string="true">     mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="575"><span data-slate-object="text" data-key="576"><span data-slate-leaf="true" data-offset-key="576:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="577"><span data-slate-leaf="true" data-offset-key="577:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">else</span></span></span></span><span data-slate-object="text" data-key="578"><span data-slate-leaf="true" data-offset-key="578:0" data-first-offset="true"><span data-slate-string="true">:</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="579"><span data-slate-object="text" data-key="580"><span data-slate-leaf="true" data-offset-key="580:0" data-first-offset="true"><span data-slate-string="true">     </span></span></span><span data-slate-object="text" data-key="581"><span data-slate-leaf="true" data-offset-key="581:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># Simply call main_worker function</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="582"><span data-slate-object="text" data-key="583"><span data-slate-leaf="true" data-offset-key="583:0" data-first-offset="true"><span data-slate-string="true">     main_worker(args.gpu, ngpus_per_node, args)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="584"><span data-slate-object="text" data-key="585"><span data-slate-leaf="true" data-offset-key="585:0" data-first-offset="true"><span data-slate-string="true">对照代码我们梳理一下其中的要点。之前我们提到的 main_worker 函数，就是每个进程中，需要执行的操作。ngpus_per_node 是每个节点的 GPU 数量（每个节点 GPU 数量相同），如果是多进程，ngpus_per_node * args.world_size 则表示所有的节点中一共有多少个 GPU，即总进程数。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="586"><span data-slate-object="text" data-key="587"><span data-slate-leaf="true" data-offset-key="587:0" data-first-offset="true"><span data-slate-string="true">一般情况下，进程 0 是主进程，比如我们会在主进程中保存模型或打印 log 信息。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="588"><span data-slate-object="text" data-key="589"><span data-slate-leaf="true" data-offset-key="589:0" data-first-offset="true"><span data-slate-string="true">当节点数为 1 时，实际上就是单机多卡，所以说 DDP 既可以支持多机多卡，也可以支持单机多卡。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="590"><span data-slate-object="text" data-key="591"><span data-slate-leaf="true" data-offset-key="591:0" data-first-offset="true"><span data-slate-string="true">main_worker 函数的调用方法如下。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="592"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="593"><span data-slate-object="text" data-key="594"><span data-slate-leaf="true" data-offset-key="594:0" data-first-offset="true"><span data-slate-string="true">main_worker(args.gpu, ngpus_per_node, args)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="595"><span data-slate-object="text" data-key="596"><span data-slate-leaf="true" data-offset-key="596:0" data-first-offset="true"><span data-slate-string="true">其中，args.gpu 表示当前所使用 GPU 的 id。而通过 mp.spawn 调用之后，会为每个节点上的每个 GPU 都启动一个进程，每个进程运行 main_worker (i, ngpus_per_node, args)，其中 i 是从 0 到 ngpus_per_node-1。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="597"><span data-slate-object="text" data-key="598"><span data-slate-leaf="true" data-offset-key="598:0" data-first-offset="true"><span data-slate-string="true">模型保存的代码如下。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="599"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div><div data-code-line-number="6"></div><div data-code-line-number="7"></div><div data-code-line-number="8"></div><div data-code-line-number="9"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="600"><span data-slate-object="text" data-key="601"><span data-slate-leaf="true" data-offset-key="601:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">if</span></span></span></span><span data-slate-object="text" data-key="602"><span data-slate-leaf="true" data-offset-key="602:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="603"><span data-slate-leaf="true" data-offset-key="603:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">not</span></span></span></span><span data-slate-object="text" data-key="604"><span data-slate-leaf="true" data-offset-key="604:0" data-first-offset="true"><span data-slate-string="true"> args.multiprocessing_distributed </span></span></span><span data-slate-object="text" data-key="605"><span data-slate-leaf="true" data-offset-key="605:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">or</span></span></span></span><span data-slate-object="text" data-key="606"><span data-slate-leaf="true" data-offset-key="606:0" data-first-offset="true"><span data-slate-string="true"> (args.multiprocessing_distributed</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="607"><span data-slate-object="text" data-key="608"><span data-slate-leaf="true" data-offset-key="608:0" data-first-offset="true"><span data-slate-string="true">         </span></span></span><span data-slate-object="text" data-key="609"><span data-slate-leaf="true" data-offset-key="609:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">and</span></span></span></span><span data-slate-object="text" data-key="610"><span data-slate-leaf="true" data-offset-key="610:0" data-first-offset="true"><span data-slate-string="true"> args.rank % ngpus_per_node == </span></span></span><span data-slate-object="text" data-key="611"><span data-slate-leaf="true" data-offset-key="611:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">0</span></span></span></span><span data-slate-object="text" data-key="612"><span data-slate-leaf="true" data-offset-key="612:0" data-first-offset="true"><span data-slate-string="true">):</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="613"><span data-slate-object="text" data-key="614"><span data-slate-leaf="true" data-offset-key="614:0" data-first-offset="true"><span data-slate-string="true">     save_checkpoint({</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="615"><span data-slate-object="text" data-key="616"><span data-slate-leaf="true" data-offset-key="616:0" data-first-offset="true"><span data-slate-string="true">         </span></span></span><span data-slate-object="text" data-key="617"><span data-slate-leaf="true" data-offset-key="617:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'epoch'</span></span></span></span><span data-slate-object="text" data-key="618"><span data-slate-leaf="true" data-offset-key="618:0" data-first-offset="true"><span data-slate-string="true">: epoch + </span></span></span><span data-slate-object="text" data-key="619"><span data-slate-leaf="true" data-offset-key="619:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">1</span></span></span></span><span data-slate-object="text" data-key="620"><span data-slate-leaf="true" data-offset-key="620:0" data-first-offset="true"><span data-slate-string="true">,</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="621"><span data-slate-object="text" data-key="622"><span data-slate-leaf="true" data-offset-key="622:0" data-first-offset="true"><span data-slate-string="true">         </span></span></span><span data-slate-object="text" data-key="623"><span data-slate-leaf="true" data-offset-key="623:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'arch'</span></span></span></span><span data-slate-object="text" data-key="624"><span data-slate-leaf="true" data-offset-key="624:0" data-first-offset="true"><span data-slate-string="true">: args.arch,</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="625"><span data-slate-object="text" data-key="626"><span data-slate-leaf="true" data-offset-key="626:0" data-first-offset="true"><span data-slate-string="true">         </span></span></span><span data-slate-object="text" data-key="627"><span data-slate-leaf="true" data-offset-key="627:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'state_dict'</span></span></span></span><span data-slate-object="text" data-key="628"><span data-slate-leaf="true" data-offset-key="628:0" data-first-offset="true"><span data-slate-string="true">: model.state_dict(),</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="629"><span data-slate-object="text" data-key="630"><span data-slate-leaf="true" data-offset-key="630:0" data-first-offset="true"><span data-slate-string="true">         </span></span></span><span data-slate-object="text" data-key="631"><span data-slate-leaf="true" data-offset-key="631:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'best_acc1'</span></span></span></span><span data-slate-object="text" data-key="632"><span data-slate-leaf="true" data-offset-key="632:0" data-first-offset="true"><span data-slate-string="true">: best_acc1,</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="633"><span data-slate-object="text" data-key="634"><span data-slate-leaf="true" data-offset-key="634:0" data-first-offset="true"><span data-slate-string="true">         </span></span></span><span data-slate-object="text" data-key="635"><span data-slate-leaf="true" data-offset-key="635:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'optimizer'</span></span></span></span><span data-slate-object="text" data-key="636"><span data-slate-leaf="true" data-offset-key="636:0" data-first-offset="true"><span data-slate-string="true"> : optimizer.state_dict(),</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="637"><span data-slate-object="text" data-key="638"><span data-slate-leaf="true" data-offset-key="638:0" data-first-offset="true"><span data-slate-string="true">     }, is_best)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="639"><span data-slate-object="text" data-key="640"><span data-slate-leaf="true" data-offset-key="640:0" data-first-offset="true"><span data-slate-string="true">这里需要注意的是，使用 DDP 意味着使用多进程，如果直接保存模型，每个进程都会执行一次保存操作，此时只使用主进程中的一个 GPU 来保存即可。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="641"><span data-slate-object="text" data-key="642"><span data-slate-leaf="true" data-offset-key="642:0" data-first-offset="true"><span data-slate-string="true">好，说到这，这个示例中有关分布式训练的重点内容我们就讲完了。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="643" id="sr-toc-9"><span data-slate-object="text" data-key="644"><span data-slate-leaf="true" data-offset-key="644:0" data-first-offset="true"><span data-slate-string="true">小结</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="645"><span data-slate-object="text" data-key="646"><span data-slate-leaf="true" data-offset-key="646:0" data-first-offset="true"><span data-slate-string="true">恭喜你走到这里，这节课我们一起完成了分布式训练的学习，最后咱们一起做个总结。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="647"><span data-slate-object="text" data-key="648"><span data-slate-leaf="true" data-offset-key="648:0" data-first-offset="true"><span data-slate-string="true">今天我们不但学习了为什么要使用分布式训练以及分布式训练的原理，还一起学习了一个分布式训练的实战项目。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="649"><span data-slate-object="text" data-key="650"><span data-slate-leaf="true" data-offset-key="650:0" data-first-offset="true"><span data-slate-string="true">在分布式训练中，主要有 DP 与 DDP 两种模式。其中 DP 并不是完整的分布式计算，只是将一部分计算放到了多张 GPU 卡上，在计算梯度的时候，仍然是 “一卡有难，八方围观”，因此 DP 会有负载不平衡、效率低等问题。而 DDP 刚好能够解决 DP 的上述问题，并且既可以用于单机多卡，也可以用于多机多卡，因此它是更好的分布式训练解决方案。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="651"><span data-slate-object="text" data-key="652"><span data-slate-leaf="true" data-offset-key="652:0" data-first-offset="true"><span data-slate-string="true">你可以将今天讲解的示例当做分布式训练的一个模板来使用。它包括了 DP 与 DPP 的完整使用过程，并且包含了如何在使用 DDP 时保存模型。不过这个示例中的代码里其实还有更多的细节，建议你留用课后空余时间，通过精读代码、查阅资料，多动手、多思考来巩固今天的学习成果。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="653" id="sr-toc-10"><span data-slate-object="text" data-key="654"><span data-slate-leaf="true" data-offset-key="654:0" data-first-offset="true"><span data-slate-string="true">每课一练</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="655"><span data-slate-object="text" data-key="656"><span data-slate-leaf="true" data-offset-key="656:0" data-first-offset="true"><span data-slate-string="true">在 torch.distributed.init_process_group (backend=“nccl”) 函数中，backend 参数可选哪些后端，它们分别有什么区别？</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="657"><span data-slate-object="text" data-key="658"><span data-slate-leaf="true" data-offset-key="658:0" data-first-offset="true"><span data-slate-string="true">推荐你好好研读今天的分布式训练 demo，也欢迎你记录自己的学习感悟或疑问，我在留言区等你。</span></span></span></div></div></div></div><div><div></div><div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><textarea placeholder="将学到的知识总结成笔记，方便日后快速查找及复习"></textarea></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div><div><div>确认放弃笔记？</div><div>放弃后所记笔记将不保留。</div></div><div><div>新功能上线，你的历史笔记已初始化为私密笔记，是否一键批量公开？</div><div>批量公开的笔记不会为你同步至部落</div></div><div></div></div><div><div><div>公开</div><div>同步至部落</div></div><div>取消</div><div>完成</div></div><div><span>0/2000</span></div></div><div><div><span>划线</span></div><div></div><div></div><div></div><div><span>笔记</span></div><div></div><div><span>复制</span></div></div></div><div><div><div><span></span></div><p><a href="javascript:void(0);">给文章提建议</a></p></div></div><div><span>©</span> 版权归极客邦科技所有，未经许可不得传播售卖。 页面已增加防盗追踪，如有侵权极客邦将依法追究其法律责任。 </div></div><div><div><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div><div>Geek__653581f21177</div></div><div><textarea placeholder="由作者筛选后的优质留言将会公开显示，欢迎踊跃留言。" rows="16"></textarea></div><div><div>Ctrl + Enter 发表</div><div>0/2000 字符</div><div>提交留言</div></div></div><div><h2 id="sr-toc-11">精选留言 (7)</h2><ul><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>马克图布</span></div></div><div>问：在 `torch.distributed.init_process_group (backend=“nccl”)` 函数中，backend 参数可选哪些后端，它们分别有什么区别？

答：根据文档，可选后端有 `gloo`, `nccl` 和 `mpi`。区别如下。

1. GLOO 支持绝大部分 CPU 上对于 tensor 的 communication function，但在 GPU 上只支持 broadcast 以及 all_reduced 函数；
2. NCCL 支持大部分 GPU 上对于 tensor 的 communication function，但不支持 CPU；
3. PyTorch 自带 NCCL 和 GLOO，但要使用 MPI 后端则需要在安装了 MPI 的 host 上从源码构建 PyTorch；
3. MPI 支持绝大部分 CPU 和 GPU 上的函数，但对于 CUDA 的支持必须是这个 host 上构建的 PyTorch 支持 CUDA 才可以。

关于使用哪个后端的经验法则：
- 使用 NCCL 进行分布式 GPU 训练
- 使用 GLOO 进行分布式 CPU 训练

参考自 PyTorch 文档：https://pytorch.org/docs/stable/distributed.html</div><div><p>作者回复: ^^，一万个赞，厉害。</p></div><div><div>2021-11-17</div><div><div><i></i></div><div><i></i><span>9</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div></div><div><div><div><span>hsiang271828</span></div></div><div>你好，我看到官方的 ImageNet 的示例中 299 行有如下代码：
# compute gradient and do SGD step
        optimizer.zero_grad ()
        loss.backward ()
        optimizer.step ()

此处使用的是 optimizer.zero_grad (), 跟之前课程中使用的 model.zero_grad () 有些许不同。请问这里应该使用哪个</div><div><p>作者回复：你好，hsiang271828，感谢你的留言。
如果 1 个优化器对应 1 个模型的时候 (1 个优化器可以对应多个模型)。optimizer.zero_grad () 与 model.zero_grad () 是相同的。
如果是多个模型的时候，就要具体情况具体分析了。如果直接 optimizer.zero_grad () 的话，就会把所有模型的梯度清零。</p></div><div><div>2021-11-17</div><div><div><i></i></div><div><i></i><span>7</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/1b/7f/d1/15a39351.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span> 小门神</span></div></div><div>老师，您好，请问利用 pytorch 对模型进行分布式训练后，在对模型进行推理时的处理和单机单卡有区别吗</div><div><p>作者回复：你好，感谢你的留言。没有区别 ^^</p></div><div><div>2021-11-18</div><div><div><i></i></div><div><i></i><span>3</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span> 马克图布</span></div></div><div>问题：

train_sampler = torch.utils.data.distributed.DistributedSampler (train_dataset)
data_loader = DataLoader (dataset, batch_size=batch_size, sampler=train_sampler)

在使用 DistributedSampler 时，DataLoader 中的 dataset 参数是否应该与 DistributedSampler 的 `train_dataset` 一致呀？</div><div><p>编辑回复：已修改，刷新可见。</p></div><div><div>2021-11-17</div><div><div><i></i></div><div><i></i><span>1</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span>John (易筋)</span></div></div><div>过去，我们经常被问到：“我应该使用哪个后端？”。

# 1. 经验法则

* 使用 NCCL 后端进行分布式 GPU 训练

* 使用 Gloo 后端进行分布式 CPU 训练。

# 2. 具有 InfiniBand 互连的 GPU 主机

* 使用 NCCL，因为它是当前唯一支持 InfiniBand 和 GPUDirect 的后端。

# 3. 具有以太网互连的 GPU 主机

* 使用 NCCL，因为它目前提供了最好的分布式 GPU 训练性能，特别是对于多进程单节点或多节点分布式训练。 如果您在使用 NCCL 时遇到任何问题，请使用 Gloo 作为备用选项。 （请注意，对于 GPU，Gloo 目前的运行速度比 NCCL 慢。）

# 4. 具有 InfiniBand 互连的 CPU 主机

* 如果您的 InfiniBand 启用了 IP over IB，请使用 Gloo，否则，请改用 MPI。 我们计划在即将发布的版本中添加对 Gloo 的 InfiniBand 支持。

# 5. 具有以太网互连的 CPU 主机

* 使用 Gloo，除非您有特定的理由使用 MPI。
官网： https://pytorch.org/docs/stable/distributed.html</div><div><div>2022-08-08</div><div><div><i></i><span></span></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/2c/8d/70/b0047299.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span>Zeurd</span></div></div><div>老师，使用 gup 的时候会报错，Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm) 为什么会同时推到 cup 和 gup 呢，我输入的就是 to（device）啊</div><div><p>作者回复: Zeurd, 你好，这个错误是由于参与运算的两个或多个变量，有的在 CPU 上，有的在 GPU 上。
我们在模型训练时，模型、数据和 loss 必须都放到同一种设备上。

你提到说，都是使用的 "to (device)"，可以看下下面的代码：
device = torch.device ('cuda' if torch.cuda.is_available () else 'cpu')
mage = image.to (device)
是不是 device 的赋值有所不同呢？

可以参考下面的步骤来 debug 你的代码：
首先找到报错的行，看看计算时都用到哪些变量或者数据，然后使用. is_cuda 这个属性去查看到底哪些是在 GPU 上，哪些是在 CPU 上，然后把它们统一都放在 CUP，或者统一放在 GPU 上。</p></div><div><div>2022-07-25</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>亚林</span></div></div><div>感觉现在购买一个 mac 炼丹炉，好贵</div><div><div>2022-05-23</div><div><div><i></i><span></span></div><div><i></i><span></span></div></div></div></div></div></li></ul><div> 收起评论<span></span></div></div></sr-rd-content>
                            <toc-bg><toc style="width: initial;overflow: auto!important;"class="simpread-font simpread-theme-root" data-reactid=".1"><outline class="toc-level-h1" data-reactid=".1.0" style="width: 175px;"><active data-reactid=".1.0.0" ></active><a class="toc-outline-theme-github" href="#sr-toc-0" data-reactid=".1.0.1"><span data-reactid=".1.0.1.0">16｜分布式训练：如何加速你的模型训练？</span></a></outline><outline class="toc-level-h2" data-reactid=".1.1" style="width: 165px;"><active data-reactid=".1.1.0"></active><a class="toc-outline-theme-github" href="#sr-toc-1" data-reactid=".1.1.1"><span data-reactid=".1.1.1.0">分布式训练原理</span></a></outline><outline class="toc-level-h3" data-reactid=".1.2" style="width: 155px;"><active data-reactid=".1.2.0"></active><a class="toc-outline-theme-github" href="#sr-toc-2" data-reactid=".1.2.1"><span data-reactid=".1.2.1.0">单机单卡</span></a></outline><outline class="toc-level-h3" data-reactid=".1.3" style="width: 155px;"><active data-reactid=".1.3.0"></active><a class="toc-outline-theme-github" href="#sr-toc-3" data-reactid=".1.3.1"><span data-reactid=".1.3.1.0">单机多卡</span></a></outline><outline class="toc-level-h3" data-reactid=".1.4" style="width: 155px;"><active data-reactid=".1.4.0"></active><a class="toc-outline-theme-github" href="#sr-toc-4" data-reactid=".1.4.1"><span data-reactid=".1.4.1.0">多机多卡</span></a></outline><outline class="toc-level-h4" data-reactid=".1.5" style="width: 145px;"><active data-reactid=".1.5.0"></active><pangu> </pangu><a class="toc-outline-theme-github" href="#sr-toc-5" data-reactid=".1.5.1"><span data-reactid=".1.5.1.0">DP 与 DDP</span></a></outline><outline class="toc-level-h4" data-reactid=".1.6" style="width: 145px;"><active data-reactid=".1.6.0"></active><a class="toc-outline-theme-github" href="#sr-toc-6" data-reactid=".1.6.1"><span data-reactid=".1.6.1.0">DDP 训练</span></a></outline><outline class="toc-level-h2" data-reactid=".1.7" style="width: 165px;"><active data-reactid=".1.7.0"></active><a class="toc-outline-theme-github" href="#sr-toc-8" data-reactid=".1.7.1"><span data-reactid=".1.7.1.0">小试牛刀</span></a></outline><outline class="toc-level-h2" data-reactid=".1.8" style="width: 165px;"><active data-reactid=".1.8.0"></active><a class="toc-outline-theme-github" href="#sr-toc-9" data-reactid=".1.8.1"><span data-reactid=".1.8.1.0">小结</span></a></outline><outline class="toc-level-h2" data-reactid=".1.9" style="width: 165px;"><active data-reactid=".1.9.0"></active><a class="toc-outline-theme-github" href="#sr-toc-10" data-reactid=".1.9.1"><span data-reactid=".1.9.1.0">每课一练</span></a></outline><outline class="toc-level-h2" data-reactid=".1.a" style="width: 165px;"><active data-reactid=".1.a.0"></active><a class="toc-outline-theme-github" href="#sr-toc-11" data-reactid=".1.a.1"><span data-reactid=".1.a.1.0">精选留言 (7)</span></a></outline></toc></toc-bg>
                            <sr-rd-footer>
                                <sr-rd-footer-group>
                                    <sr-rd-footer-line></sr-rd-footer-line>
                                    <sr-rd-footer-text>全文完</sr-rd-footer-text>
                                    <sr-rd-footer-line></sr-rd-footer-line>
                                </sr-rd-footer-group>
                                <sr-rd-footer-copywrite>
                                    <div>本文由 <a href="http://ksria.com/simpread" target="_blank">简悦 SimpRead</a> 转码，用以提升阅读体验，<a href="https://time.geekbang.org/column/article/445886" target="_blank">原文地址 </a></div>
                                </sr-rd-footer-copywrite>
                            </sr-rd-footer>
                        </sr-read>
                    </body>
                </html>