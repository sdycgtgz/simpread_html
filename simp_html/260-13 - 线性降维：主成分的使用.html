
                <html lang="en" class="simpread-font simpread-theme-root" style=''>
                    <head>
                        <meta charset="utf-8">
                        <meta http-equiv="content-type" content="text/html; charset=UTF-8;charset=utf-8">
                        <meta http-equiv="X-UA-Compatible" content="IE=Edge">
                        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1">
                        <meta name="author" content="Kenshin"/>
                        <meta name="description" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展" />
                        <meta name="keywords" content="Chrome extension, Chrome 扩展, 阅读模式, 沉浸式阅读, 简悦, 简阅, read mode, reading mode, reader view, firefox, firefox addon, userscript, safari, opera, tampermonkey"/>
                        <meta name="thumbnail" content="https://simpread-1254315611.cos.ap-shanghai.myqcloud.com/static/introduce-2.png"/>
                        <meta property="og:title" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展"/>
                        <meta property="og:type" content="website">
                        <meta property="og:local" content="zh_CN"/>
                        <meta property="og:url" content="http://ksria.com/simpread"/>
                        <meta property="og:image" content="https://simpread-1254315611.cos.ap-shanghai.myqcloud.com/static/introduce-2.png"/>
                        <meta property="og:image:type" content="image/png"/>
                        <meta property="og:image:width" content="960"/>
                        <meta property="og:image:height" content="355"/>
                        <meta property="og:site_name" content="http://ksria.com/simpread"/>
                        <meta property="og:description" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展"/>
                        <style type="text/css">.simpread-font{font:300 16px/1.8 -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#333;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased}.simpread-hidden{display:none}.simpread-read-root{display:-webkit-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;margin:0;top:-1000px;left:0;width:100%;z-index:2147483646;overflow-x:hidden;opacity:0;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}.simpread-read-root-show{top:0}.simpread-read-root-hide{top:1000px}sr-read{display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;margin:20px 20%;min-width:400px;min-height:400px;text-align:center}read-process{position:fixed;top:0;left:0;height:3px;width:100%;background-color:#64b5f6;-webkit-transition:width 2s;transition:width 2s;z-index:20000}sr-rd-content-error{display:block;position:relative;margin:0;margin-bottom:30px;padding:25px;background-color:rgba(0,0,0,.05)}sr-rd-footer{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;font-size:14px}sr-rd-footer,sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}sr-rd-footer-group{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}sr-rd-footer-text{min-width:150px}sr-rd-footer-copywrite{margin:10px 0 0;color:inherit}sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}sr-rd-footer-copywrite .second{margin:10px 0}sr-rd-footer-copywrite .third a:hover{border:none!important}sr-rd-footer-copywrite .third a:first-child{margin-right:50px}sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}sr-rd-footer-copywrite .sr-icon:hover{opacity:1}sr-rd-footer-copywrite a,sr-rd-footer-copywrite a:link,sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}sr-rd-footer-copywrite a:focus,sr-rd-footer-copywrite a:hover,sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.simpread-blocks{text-decoration:none!important}.simpread-blocks *{margin:0}.simpread-blocks a{padding:0;text-decoration:none!important}.simpread-blocks img{margin:0;padding:0;border:0;background:transparent;box-shadow:none}.simpread-focus-root{display:block;position:fixed;top:0;left:0;right:0;bottom:0;background-color:hsla(0,0%,92%,.9);z-index:2147483645;opacity:0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms}.simpread-focus-highlight{position:relative;box-shadow:0 0 0 20px #fff;background-color:#fff;overflow:visible;z-index:2147483646}.sr-controlbar-bg sr-rd-crlbar,.sr-controlbar-bg sr-rd-crlbar fab{z-index:2147483647}sr-rd-crlbar.controlbar{position:fixed;right:0;bottom:0;width:100px;height:100%;opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}sr-rd-crlbar.controlbar:hover{opacity:1}sr-rd-crlbar fap *{box-sizing:border-box}@media (max-height:620px){fab{zoom:.8}}@media (max-height:783px){dialog-gp dialog-content{max-height:580px}dialog-gp dialog-footer{border-top:1px solid #e0e0e0}}.simpread-highlight-selector{outline:3px dashed #1976d2!important;cursor:pointer!important}.simpread-highlight-controlbar,.simpread-highlight-selector{background-color:#fafafa!important;opacity:.8!important;-webkit-transition:opacity .5s ease!important;transition:opacity .5s ease!important}.simpread-highlight-controlbar{position:relative!important;border:3px dashed #1976d2!important}simpread-highlight,sr-snapshot-ctlbar{position:fixed;top:0;left:0;right:0;padding:15px;height:50px;background-color:rgba(50,50,50,.9);box-shadow:0 2px 5px rgba(0,0,0,.26);box-sizing:border-box;z-index:2147483640}simpread-highlight,sr-highlight-ctl,sr-snapshot-ctlbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-highlight-ctl{margin:0 5px;width:50px;height:20px;color:#fff;background-color:#1976d2;border-radius:4px;box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12);cursor:pointer}toc-bg{position:fixed;left:0;top:0;width:50px;height:200px;font-size:medium}toc-bg:hover{z-index:3}.toc-bg-hidden{opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}.toc-bg-hidden:hover{opacity:1;z-index:3}.toc-bg-hidden:hover toc{width:180px}toc *{all:unset}toc{position:fixed;left:0;top:100px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;padding:10px;width:0;max-width:200px;max-height:500px;overflow-x:hidden;overflow-y:hidden;cursor:pointer;border:1px solid hsla(0,0%,62%,.22);-webkit-transition:width .5s;transition:width .5s}toc:hover{overflow-y:auto}toc.mini:hover{width:200px!important}toc::-webkit-scrollbar{width:3px}toc::-webkit-scrollbar-thumb{border-radius:10px;background-color:hsla(36,2%,54%,.5)}toc outline{position:relative;display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;overflow:hidden;text-overflow:ellipsis;padding:2px 0;min-height:21px;line-height:21px;text-align:left}toc outline a,toc outline a:active,toc outline a:focus,toc outline a:visited{display:block;width:100%;color:inherit;font-size:11px;text-decoration:none!important;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}toc outline a:hover{font-weight:700!important}toc outline a.toc-outline-theme-dark,toc outline a.toc-outline-theme-night{color:#fff!important}.toc-level-h1{padding-left:5px}.toc-level-h2{padding-left:15px}.toc-level-h3{padding-left:25px}.toc-level-h4{padding-left:35px}.toc-outline-active{border-left:2px solid #f44336}toc outline active{position:absolute;left:0;top:0;bottom:0;padding:0 0 0 3px;border-left:2px solid #e8e8e8}sr-kbd{background:-webkit-gradient(linear,0 0,0 100%,from(#fff785),to(#ffc542));border:1px solid #e3be23;-o-border-image:none;border-image:none;-o-border-image:initial;border-image:initial;position:absolute;left:0;padding:1px 3px 0;font-size:11px!important;font-weight:700;box-shadow:0 3px 7px 0 rgba(0,0,0,.3);overflow:hidden;border-radius:3px}.sr-kbd-a{position:relative}kbd-mapping{position:fixed;left:5px;bottom:5px;-ms-flex-flow:row;flex-flow:row;width:250px;height:500px;background-color:#fff;border:1px solid hsla(0,0%,62%,.22);box-shadow:0 2px 5px rgba(0,0,0,.26);border-radius:3px}kbd-mapping,kbd-maps{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}kbd-maps{margin:40px 0 20px;width:100%;overflow-x:auto}kbd-maps::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}kbd-maps::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}kbd-mapping kbd-map-title{position:absolute;margin:5px 0;width:100%;font-size:14px;font-weight:700}kbd-maps-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}kbd-maps-title{margin:5px 0;padding-left:53px;font-size:12px;font-weight:700}kbd-map kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fafbfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}kbd-map kbd-name{display:inline-block;text-align:right;width:50px}kbd-map kbd-desc{padding-left:3px}sharecard-bg{position:fixed;top:0;left:0;width:100%;height:100%;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(0,0,0,.4);z-index:2147483647}sharecard{max-width:450px;background-color:#64b5f6}sharecard,sharecard-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-head{margin:25px;color:#fff;border-radius:10px;box-shadow:0 2px 6px 0 rgba(0,0,0,.2),0 25px 50px 0 rgba(0,0,0,.15)}sharecard-card{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-card,sharecard-top{display:-webkit-box;display:-ms-flexbox;display:flex}sharecard-top{-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:65px;background-color:#fff;color:#878787;font-size:25px;font-weight:500;border-top-left-radius:10px;border-top-right-radius:10px}sharecard-top span.logos{display:block;width:48px;height:48px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAMAAABg3Am1AAABU1BMVEUAAAAnNJMnNZI3Q5onNJInNJMnNJInNJMnNJI8SJ0tOZY/S55EUKAoNJI6RpwoNJNIU6InNJInNJImNJI7SJwmNJJ2fLUiMJFKVaNCTJ9faK1HUaJOWKVSXaUnNJNYY6pye7cmM5JXYKhwebMjMI8mL4719fW9vb0oNZP/UlLz8/QqN5TAwMAnNJPv7+/Pz8/q6+/p6enNzc3Kysry8vMsOJXc3env7/LU1uXo29vR0dHOzs7ExMTwjo73bW37XV3Aj1TCYELl5u3n5+fW2Obn6O7f4OrZ2+g0QJkxPpgvO5bh4uvS1OTP0ePCwsJQW6ZLVqTs7fHd3d3V1dXqv79VX6lET6A1TIxXUIBSgHxWQnpelHf+WVnopkXqbC7j5Ozi4uLDw8NGUaFATJ9SgH3r6+vGyd7BxNva2trX19ejqM2gpczHx8dze7Zha67Z2dlTgH1aXQeSAAAAJnRSTlMA6ff+497Y8NL+/fv49P379sqab/BeOiX06tzVy8m/tKqpalA7G6oKj0EAAAJlSURBVEjHndNXWxpBFIDhcS2ICRLAkt4Dx4WhLk0E6R0MYoIISrWX5P9f5cwSIRC2+T1czMV5n2FnZwn2eWONUqCAv3H2Uf5Ra1hx4+0WEXtDQW0fCPYJ1EffEfIV4CSROAE4jsePoTFsNmTJF/IeIHF2lgCIn57GodlqDWXBK7IwBYatVlMWFAildPKX7I3m74Z9fsCiQChoimoFQAz04Ad2gH1n9fv9n9hgMNDr9euLWD6fLxQKxaLfb7dTSlahbFVdEPwIQtrAihZQgyKCtCagbQe3xh0QFMgy5MR11+ewYY5/qlZ7vT2xu93ULKjbFLpiUxnIIwjgKmVTLDUFXMrAi2NJWCRLIthTBo4xyOLKpwyqU6CuDCI41hFBCVdOhyLw4FgJ1skCAiyl9BSHbCorgo6VJXTru5hrVCQS8Yr5xLzX59YJSFpVFwD9U0BGC3hGdFpATgRupTGe9R9I1b1ePBvXKDyvq/O/44LT4/E4BUbSCAwj8Evq6HlnOBprx6JhJz8Gktc7xeaP9ndY+0coQvCccFBD4JW60UIY50ciLOAODAQRVOeCHm4Q3Xks6uRDY+CQ+AR4T2wMYh6+jMCIQOp78CFoj0H7EQgIuhI3dGaHCrwgADwCPjJvA372GRigCJg49FUdk3D87pq3zp4SA5zc1Zh9DxfwkpjgUg5Mv+lbeE3McC8Lpu7SA3wk2xzcqL2tN5DfIsQC8HB7UamUy6FQOpTO5QKBQDZbKnWSyUzGjdWCwaDA8+7Le4BNgm3qQGWchYh9s5hNq6wVbBlbwhZYOp3OYOA4zmgEypnM2zj8ByIdedKrH8vDAAAAAElFTkSuQmCC");zoom:.8}sharecard-content{padding:15px;max-height:500px;font-size:20px;text-align:justify;background-color:#2196f3;overflow-x:hidden;overflow-y:auto}sharecard-via{padding:10px;font-size:10px;background-color:#2196f3}sharecard-footer{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:100px;background-color:#fff;color:#878787;font-size:15px;font-weight:500;border-bottom-left-radius:10px;border-bottom-right-radius:10px}sharecard-footer,sharecard-footer div{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sharecard-footer span.qrcode{display:block;width:100px;height:100px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,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")}sharecard-control{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:0 19px;height:80px;background-color:#fff}simpread-snapshot{width:100%;height:100%;cursor:move;z-index:2147483645}simpread-snapshot,sr-mask{position:fixed;left:0;top:0}sr-mask{background-color:rgba(0,0,0,.1)}.simpread-feedback,.simpread-urlscheme{position:fixed;right:20px;bottom:20px;z-index:2147483646}simpread-feedback,simpread-urlscheme{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:20px 20px 0;width:500px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:3px;box-shadow:0 0 2px rgba(0,0,0,.12),0 2px 2px rgba(0,0,0,.26);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease}simpread-feedback *,simpread-urlscheme *{font-size:12px!important;box-sizing:border-box}simpread-feedback.active,simpread-urlscheme.active{-webkit-animation-name:srFadeInUp;animation-name:srFadeInUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback.hide,simpread-urlscheme.hide{-webkit-animation-name:srFadeInDown;animation-name:srFadeInDown;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback sr-fb-label,simpread-urlscheme sr-urls-label{width:100%}simpread-feedback sr-fb-head,simpread-urlscheme sr-urls-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin-bottom:5px;width:100%}simpread-feedback sr-fb-content,simpread-urlscheme sr-urls-content{margin-bottom:5px;width:100%}simpread-feedback sr-urls-footer,simpread-urlscheme sr-urls-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-fb-a,simpread-urlscheme sr-urls-a{color:#2163f7;cursor:pointer}simpread-feedback text-field-state,simpread-urlscheme text-field-state{border-top:none rgba(34,101,247,.8)!important;border-left:none rgba(34,101,247,.8)!important;border-right:none rgba(34,101,247,.8)!important;border-bottom:2px solid rgba(34,101,247,.8)!important}simpread-feedback switch,simpread-urlscheme switch{margin-top:0!important}@-webkit-keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@-webkit-keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}@keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}simpread-feedback sr-fb-head{font-weight:700}simpread-feedback sr-fb-content{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}simpread-feedback sr-fb-content,simpread-feedback sr-fb-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}simpread-feedback sr-fb-footer{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-close{position:absolute;right:20px;cursor:pointer;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:200}simpread-feedback sr-close:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-top:10px}simpread-feedback sr-stars i{margin-right:10px;cursor:pointer}simpread-feedback sr-stars i svg{-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}simpread-feedback sr-stars i svg:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars i.active svg{-webkit-transform:rotate(0) scale(1);transform:rotate(0) scale(1)}simpread-feedback sr-emojis{display:block;height:100px;overflow:hidden}simpread-feedback sr-emoji{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:.3s;transition:.3s}simpread-feedback sr-emoji>svg{margin:15px 0;width:70px;height:70px;-ms-flex-negative:0;flex-shrink:0}simpread-feedback sr-stars-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin:10px 0 20px}</style>
                        <style type="text/css">.simpread-theme-root{font-size:62.5%!important}sr-rd-content,sr-rd-desc,sr-rd-title{width:100%}sr-rd-title{display:-webkit-box;margin:1em 0 .5em;overflow:hidden;text-overflow:ellipsis;text-rendering:optimizelegibility;-webkit-line-clamp:3;-webkit-box-orient:vertical}sr-rd-content{text-align:left;word-break:break-word}sr-rd-desc{text-align:justify;line-height:2.4;margin:0 0 1.2em;box-sizing:border-box}sr-rd-content{font-size:25.6px;font-size:1.6rem;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:25.6px;font-size:1.6rem}sr-rd-content div,sr-rd-content p,sr-rd-content pre,sr-rd-content sr-blockquote{margin:0 0 1.2em;word-break:break-word}sr-rd-content a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-rd-content a,sr-rd-content a:link{color:inherit;font-size:inherit;font-weight:inherit;border:none}sr-rd-content a:hover{background:transparent}sr-rd-content img{margin:10px;padding:5px;max-width:100%;background:#fff;border:1px solid #bbb;box-shadow:1px 1px 3px #d4d4d4}sr-rd-content figcaption{text-align:center;font-size:14px}sr-rd-content sr-blockquote{display:block;position:relative;padding:15px 25px;text-align:left;line-height:inherit}sr-rd-content sr-blockquote:before{position:absolute}sr-rd-content sr-blockquote *{margin:0;font-size:inherit}sr-rd-content table{width:100%;margin:0 0 1.2em;word-break:keep-all;word-break:normal;overflow:auto;border:none}sr-rd-content table td,sr-rd-content table th{border:none}sr-rd-content ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-rd-content ol{list-style:decimal;margin:0;padding:0}sr-rd-content ol li,sr-rd-content ul li{font-size:inherit;list-style:disc;margin:0 0 1.2em}sr-rd-content ol li{list-style:decimal;margin-left:1.3em}sr-rd-content ol li *,sr-rd-content ul li *{margin:0;text-align:left;text-align:initial}sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em}sr-rd-content li ul{list-style:circle}sr-rd-content pre{font-family:Consolas,Monaco,Andale Mono,Source Code Pro,Liberation Mono,Courier,monospace;display:block;padding:15px;line-height:1.5;word-break:break-all;word-wrap:break-word;white-space:pre;overflow:auto}sr-rd-content pre,sr-rd-content pre *,sr-rd-content pre div{font-size:17.6px;font-size:1.1rem}sr-rd-content li pre code,sr-rd-content p pre code,sr-rd-content pre{background-color:transparent;border:none}sr-rd-content pre code{margin:0;padding:0}sr-rd-content pre code,sr-rd-content pre code *{font-size:17.6px;font-size:1.1rem}sr-rd-content pre p{margin:0;padding:0;color:inherit;font-size:inherit;line-height:inherit}sr-rd-content li code,sr-rd-content p code{margin:0 4px;padding:2px 4px;font-size:17.6px;font-size:1.1rem}sr-rd-content mark{margin:0 5px;padding:2px;background:#fffdd1;border-bottom:1px solid #ffedce}.sr-rd-content-img{width:90%;height:auto}.sr-rd-content-img-load{width:48px;height:48px;margin:0;padding:0;border-style:none;border-width:0;background-repeat:no-repeat;background-image:url(data:image/gif;base64,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)}.sr-rd-content-center{text-align:center;display:-webkit-box;-webkit-box-align:center;-webkit-box-pack:center;-webkit-box-orient:vertical}.sr-rd-content-center-small{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.sr-rd-content-center-small img{margin:0;padding:0;border:0;box-shadow:none}img.simpread-img-broken{cursor:pointer}.sr-rd-content-nobeautify{margin:0;padding:0;border:0;box-shadow:0 0 0}sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-radius:4px;box-shadow:0 1px 2px 0 rgba(60,64,67,.3),0 2px 6px 2px rgba(60,64,67,.15)}sr-rd-mult:hover{-webkit-transition:all .45s 0ms;transition:all .45s 0ms;box-shadow:1px 1px 8px rgba(0,0,0,.16)}sr-rd-mult sr-rd-mult-content{padding:0 16px;overflow:auto}sr-rd-mult sr-rd-mult-avatar,sr-rd-mult sr-rd-mult-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sr-rd-mult sr-rd-mult-avatar{-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 15px}sr-rd-mult sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}sr-rd-mult sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}sr-rd-mult sr-rd-mult-content img{max-width:80%}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-page{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%}</style>
                        <style type="text/css">sr-rd-theme-github{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:57.6px;font-size:3.6rem;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;line-height:1.225}sr-rd-content h3{font-size:38.4px;font-size:2.4rem;line-height:1.43}sr-rd-content h4{font-size:32px;font-size:2rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}sr-rd-title{font-family:PT Sans,SF UI Display,\.PingFang SC,PingFang SC,Neue Haas Grotesk Text Pro,Arial Nova,Segoe UI,Microsoft YaHei,Microsoft JhengHei,Helvetica Neue,Source Han Sans SC,Noto Sans CJK SC,Source Han Sans CN,Noto Sans SC,Source Han Sans TC,Noto Sans CJK TC,Hiragino Sans GB,sans-serif;font-size:54.4px;font-size:3.4rem;font-weight:700;line-height:1.3}sr-rd-desc{position:relative;margin:0;margin-bottom:30px;padding:25px;padding-left:56px;font-size:28.8px;font-size:1.8rem;color:#777;background-color:rgba(0,0,0,.05);box-sizing:border-box}sr-rd-desc:before{content:"\201C";position:absolute;top:-28px;left:16px;font-size:80px;font-family:Arial;color:rgba(0,0,0,.15)}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content b *,sr-rd-content strong,sr-rd-content strong * sr-rd-content b{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}</style>
                        <style type="text/css"></style>
                        <style type="text/css">@media (pointer:coarse){sr-read{margin:20px 5%!important;min-width:0!important;max-width:90%!important}sr-rd-title{margin-top:0;font-size:2.7rem}sr-rd-content sr-blockquote,sr-rd-desc{margin:10 0!important;padding:0 0 0 10px!important;width:90%;font-size:1.8rem;font-style:normal;line-height:1.7;text-align:justify}sr-rd-content{font-size:1.75rem;font-weight:300}sr-rd-content figure{margin:0;padding:0;text-align:center}sr-rd-content a,sr-rd-content a:link,sr-rd-content li code,sr-rd-content p code{font-size:inherit}sr-rd-footer{margin-top:20px}sr-blockquote,sr-blockquote *{margin:5px!important;padding:5px!important}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6,sr-rd-title{font-family:PingFang SC,Verdana,Helvetica Neue,Microsoft Yahei,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#000;font-weight:100;line-height:1.35}sr-rd-content-h1,sr-rd-content-h2,sr-rd-content-h3,sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;line-height:1.35}sr-rd-content-h1,sr-rd-content h1{font-size:1.8em}sr-rd-content-h2,sr-rd-content h2{font-size:1.6em}sr-rd-content-h3,sr-rd-content h3{font-size:1.4em}sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:1.2em}sr-rd-content-ul,sr-rd-content ul{margin-left:1.3em!important;list-style:disc}sr-rd-content-ol,sr-rd-content ol{list-style:decimal;margin-left:1.9em!important}sr-rd-content-ol ol,sr-rd-content-ol ul,sr-rd-content-ul ol,sr-rd-content-ul ul,sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em!important}sr-rd-content img{margin:0;padding:0;border:0;max-width:100%!important;height:auto;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-rd-mult{min-width:0;background-color:#fff;box-shadow:0 1px 6px rgba(32,33,36,.28);border-radius:8px}sr-rd-mult sr-rd-mult-avatar div{margin:0}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center-small{margin:7px 0!important}sr-rd-mult sr-rd-mult-avatar span{display:block}sr-rd-mult sr-rd-mult-content{padding-left:0}@media only screen and (max-device-width:1024px){.simpread-theme-root,html.simpread-theme-root{font-size:80%!important}sr-rd-mult sr-rd-mult-avatar img{width:50px;height:50px;min-width:50px;min-height:50px}toc-bg toc{width:10px!important}toc-bg:hover toc{width:auto!important}}@media only screen and (max-device-width:414px){.simpread-theme-root,html.simpread-theme-root{font-size:70%!important}sr-rd-mult sr-rd-mult-avatar img{width:30px;height:30px;min-width:30px;min-height:30px}}@media only screen and (max-device-width:320px){.simpread-theme-root,html.simpread-theme-root{font-size:90%!important}sr-rd-content p{margin-bottom:.5em}}}</style>
                        <style type="text/css">undefinedsr-rd-content *, sr-rd-content p, sr-rd-content div {}sr-rd-content pre code, sr-rd-content pre code * {}sr-rd-desc {}sr-rd-content pre {}sr-rd-title {}</style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css">sr-rd-content *, sr-rd-content p, sr-rd-content div {
        font-size: 15px;
    }

    .annote-perview, .annote-perview * {
        color: rgb(85, 85, 85);
        font-weight: 400;
        line-height: 1.8;
    }</style>
                        
                        
                        <script>setTimeout(()=>{const e=location.hash.replace("#id=","");let t,a=!1;const n=t=>{for(let n of t){let t;if((t=e.length>6?n.getAttribute("data-id"):n.getAttribute("data-idx"))==e){n.scrollIntoView({behavior:"smooth",block:"start",inline:"nearest"}),a=!0;break}}};e&&(0==(t=document.getElementsByClassName("sr-unread-card")).length&&(t=document.getElementsByTagName("sr-annote")),n(t),a||n(t=document.getElementsByClassName("sr-annote")))},500);</script>
                        <script>document.addEventListener("DOMContentLoaded",function(){if("localhost"==location.hostname){const t=document.getElementsByTagName("img");for(let o of t){const t=o.src;t.startsWith("http")&&(o.src=location.origin+"/proxy?url="+t)}}},!1);</script>
                        <style>toc a {font-size: inherit!important;font-weight: 300!important;}</style><script>setTimeout(()=>{const max=document.getElementsByTagName('sr-annote-note-tip').length;for(let i=0;i<max;i++){const target=document.getElementsByTagName('sr-annote-note-tip')[i],value=target.dataset.value;value&&(target.innerText=value)}},1000);</script><title>简悦 | 13 | 线性降维：主成分的使用</title>
                    </head>
                    <body>
                        <sr-read style='font-family: "LXGW WenKai Screen";'>
                            <sr-rd-title>13 | 线性降维：主成分的使用</sr-rd-title>
                    <sr-rd-desc style="margin: 0;padding-top: 0;padding-bottom: 0;font-style: normal;font-size: 18px;">新一季的主题是机器学习，我会帮你把握不同模型之间的内在关联，让你形成观察机器学习的宏观视角，找准进一步理解与创新的方向。</sr-rd-desc>
                    <sr-rd-content><h1 id="sr-toc-0">13 | 线性降维：主成分的使用</h1><div><span>王天一</span> <span> 2018-07-03</span></div><div><div><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/be/07/be5c6ee89c4167b362f5f6c831606d07.jpeg"></div><div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADEAAAABCAYAAAB35kaxAAAAAXNSR0IArs4c6QAAAChJREFUGFdjfPfu3X8GKBAUFASz3r9/DxNiQBbDxcamn1yzSLEDZi8A2agny86FR+IAAAAASUVORK5CYII="></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAABCAYAAAA8TpVcAAAAAXNSR0IArs4c6QAAAB1JREFUGFdjfPfu3X9BQUEGEHj//j2YBgFCYtjkAcT9D8ti7hUOAAAAAElFTkSuQmCC"></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD0AAAABCAYAAABt2qY/AAAAAXNSR0IArs4c6QAAACdJREFUGFdjfPfu3X9BQUEGEHj//j2YBgFkMULy2PQQK0YLewiZCQDhUS3LBhDf1QAAAABJRU5ErkJggg=="></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAABCAYAAAA4u0VhAAAAAXNSR0IArs4c6QAAAB1JREFUGFdjfPfu3X9BQUEGEHj//j2YBgFkMULyAF6lD8tbqYWOAAAAAElFTkSuQmCC"></div></div></div><div><div><div></div></div><div><div><div><div><div><div>00:00</div></div><div></div></div></div><a href="javascript:;"> 1.0x <i><span></span></i></a></div><div><span>讲述：王天一</span><span>大小：6.09M</span><span> 时长：21:17</span></div></div><audio title="13 | 线性降维：主成分的使用" src="https://res001.geekbang.org/media/audio/b8/a8/b872f7b08e157a33c2707ca734d5b8a8/ld/ld.m3u8"></audio></div><div><div><div><div data-slate-editor="true" data-key="2422" autocorrect="off" spellcheck="false" data-gramm="false"><div data-slate-type="paragraph" data-slate-object="block" data-key="2423"><span data-slate-object="text" data-key="2424"><span data-slate-leaf="true" data-offset-key="2424:0" data-first-offset="true"><span data-slate-string="true">在前一篇文章中，我以岭回归和 LASSO 为例介绍了线性回归的正则化处理。这两种方法都属于</span></span></span><span data-slate-object="text" data-key="2425"><span data-slate-leaf="true" data-offset-key="2425:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">收缩方法</span></span></span></span><span data-slate-object="text" data-key="2426"><span data-slate-leaf="true" data-offset-key="2426:0" data-first-offset="true"><span data-slate-string="true">（shrinkage method），它们能够使线性回归的系数连续变化。但和岭回归不同的是，LASSO 可以将一部分属性的系数收缩为 0，事实上起到了筛选属性的作用。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2427"><span data-slate-object="text" data-key="2428"><span data-slate-leaf="true" data-offset-key="2428:0" data-first-offset="true"><span data-slate-string="true">和 LASSO 这种间接去除属性的收缩方法相对应的是</span></span></span><span data-slate-object="text" data-key="2429"><span data-slate-leaf="true" data-offset-key="2429:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">维度规约</span></span></span></span><span data-slate-object="text" data-key="2430"><span data-slate-leaf="true" data-offset-key="2430:0" data-first-offset="true"><span data-slate-string="true">。维度规约这个听起来个高大上的名称是数据挖掘中常用的术语，它有一个更接地气的同义词，就是</span></span></span><span data-slate-object="text" data-key="2431"><span data-slate-leaf="true" data-offset-key="2431:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">降维</span></span></span></span><span data-slate-object="text" data-key="2432"><span data-slate-leaf="true" data-offset-key="2432:0" data-first-offset="true"><span data-slate-string="true">（dimensionality reduction），也就是直接降低输入属性的数目来削减数据的维度。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2433"><span data-slate-object="text" data-key="2434"><span data-slate-leaf="true" data-offset-key="2434:0" data-first-offset="true"><span data-slate-string="true">对数据维度的探讨来源于 “</span></span></span><span data-slate-object="text" data-key="2435"><span data-slate-leaf="true" data-offset-key="2435:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">维数灾难</span></span></span></span><span data-slate-object="text" data-key="2436"><span data-slate-leaf="true" data-offset-key="2436:0" data-first-offset="true"><span data-slate-string="true">”（curse of dimensionality），这个概念是数学家理查德・贝尔曼（Richard Bellman）在动态优化问题的研究中提出的。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2437"><span data-slate-object="text" data-key="2438"><span data-slate-leaf="true" data-offset-key="2438:0" data-first-offset="true"><span data-slate-string="true">发表于《IEEE 模式分析与机器智能汇刊》（IEEE Transactions on Pattern Analysis and Machine Intelligence）第 1 卷第 3 期的论文《维数问题：一个简单实例（A Problem of Dimensionality: A Simple Example）》在数学上证明了当所有参数都已知时，属性维数的增加可以让分类问题的错误率渐进为 0；可当未知的参数只能根据数量有限的样本来估计时，属性维数的增加会使错误率先降低再升高，最终收敛到 0.5。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2439"><span data-slate-object="text" data-key="2440"><span data-slate-leaf="true" data-offset-key="2440:0" data-first-offset="true"><span data-slate-string="true">这就像一群谋士七嘴八舌在支招，当领导的要是对每个人的意见都深入考虑再来拍板的话，这样的决策也没什么准确性可言了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2441"><span data-slate-object="text" data-key="2442"><span data-slate-leaf="true" data-offset-key="2442:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">维数灾难深层次的原因在于数据样本的有限</span></span></span></span><span data-slate-object="text" data-key="2443"><span data-slate-leaf="true" data-offset-key="2443:0" data-first-offset="true"><span data-slate-string="true">。当属性的维数增加时，每个属性每个可能取值的组合就会以指数形式增长。对于二值属性来说，2 个属性所有可能的取值组合共有 4 种，可每增加一个属性，可能的组合数目就会翻番。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2444"><span data-slate-object="text" data-key="2445"><span data-slate-leaf="true" data-offset-key="2445:0" data-first-offset="true"><span data-slate-string="true">一般的经验法则是每个属性维度需要对应至少 5 个数据样本，可当属性维数增加而样本数目不变时，过少的数据就不足以体现出属性背后的趋势，从而导致过拟合的发生。当然，这只是维数灾难的一种解释方式，另一种解释方式来源于几何角度的数据稀疏性，这里暂且按下不表。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2446"><span data-slate-object="text" data-key="2447"><span data-slate-leaf="true" data-offset-key="2447:0" data-first-offset="true"><span data-slate-string="true">在数据有限的前提下解决维数灾难问题，化繁为简的降维是必经之路。降维的对象通常是那些 “食之无味，弃之可惜” 的鸡肋属性。食之无味是因为它们或者和结果的相关性不强，或者和其他属性之间有较强的关联，使用这样的属性没有多大必要；弃之可惜则是因为它们终究还包含一些独有的信息，就这么断舍离又会心有不甘。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2448"><span data-slate-object="text" data-key="2449"><span data-slate-leaf="true" data-offset-key="2449:0" data-first-offset="true"><span data-slate-string="true">如果像亚历山大剑斩戈尔迪之结一般直接砍掉鸡肋属性，这种 “简单粗暴” 的做法就是</span></span></span><span data-slate-object="text" data-key="2450"><span data-slate-leaf="true" data-offset-key="2450:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">特征选择</span></span></span></span><span data-slate-object="text" data-key="2451"><span data-slate-leaf="true" data-offset-key="2451:0" data-first-offset="true"><span data-slate-string="true">（feature selection）。另一种更加稳妥的办法是破旧立新，将所有原始属性的信息一点儿不浪费地整合到脱胎换骨的新属性中，这就是</span></span></span><span data-slate-object="text" data-key="2452"><span data-slate-leaf="true" data-offset-key="2452:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">特征提取</span></span></span></span><span data-slate-object="text" data-key="2453"><span data-slate-leaf="true" data-offset-key="2453:0" data-first-offset="true"><span data-slate-string="true">（feature extraction）的方法。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2454"><span data-slate-object="text" data-key="2455"><span data-slate-leaf="true" data-offset-key="2455:0" data-first-offset="true"><span data-slate-string="true">无论是特征选择还是特征提取，在 “人工智能基础课” 中都有相应的介绍。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2456"><span data-slate-object="text" data-key="2457"><span data-slate-leaf="true" data-offset-key="2457:0" data-first-offset="true"><span data-slate-string="true">今天我要换个角度，先从刚刚介绍过的岭回归说起。假设数据集中有 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2458"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>N</span></span></span></span></span></span><span data-slate-object="text" data-key="2460"><span data-slate-leaf="true" data-offset-key="2460:0" data-first-offset="true"><span data-slate-string="true"> 个样本，每个样本都有 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2461"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span></span><span data-slate-object="text" data-key="2463"><span data-slate-leaf="true" data-offset-key="2463:0" data-first-offset="true"><span data-slate-string="true"> 个属性，则数据矩阵 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2464"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2466"><span data-slate-leaf="true" data-offset-key="2466:0" data-first-offset="true"><span data-slate-string="true"> 的维度就是 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2467"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>×</span><span></span></span><span><span></span><span>p</span></span></span></span></span></span><span data-slate-object="text" data-key="2469"><span data-slate-leaf="true" data-offset-key="2469:0" data-first-offset="true"><span data-slate-string="true">。将中心化处理后，也就是减去每个属性平均值的 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2470"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2472"><span data-slate-leaf="true" data-offset-key="2472:0" data-first-offset="true"><span data-slate-string="true"> 进行奇异值分解（singular value decomposition）可以得到</span></span></span></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-slate-type="block-katex" data-slate-object="block" data-key="2473"><span><span><span aria-hidden="true"><span><span></span><span><span><span>X</span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>U</span></span></span><span><span><span>D</span></span></span><span><span><span><span>V</span></span></span><span><span><span><span><span><span></span><span><span>T</span></span></span></span></span></span></span></span></span></span></span></span></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2475"><span data-slate-object="text" data-key="2476"><span data-slate-leaf="true" data-offset-key="2476:0" data-first-offset="true"><span data-slate-string="true">其中的 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2477"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>U</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2479"><span data-slate-leaf="true" data-offset-key="2479:0" data-first-offset="true"><span data-slate-string="true"> 和 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2480"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>V</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2482"><span data-slate-leaf="true" data-offset-key="2482:0" data-first-offset="true"><span data-slate-string="true"> 分别是 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2483"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>×</span><span></span></span><span><span></span><span>p</span></span></span></span></span></span><span data-slate-object="text" data-key="2485"><span data-slate-leaf="true" data-offset-key="2485:0" data-first-offset="true"><span data-slate-string="true"> 维和 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2486"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>p</span><span></span><span>×</span><span></span></span><span><span></span><span>p</span></span></span></span></span></span><span data-slate-object="text" data-key="2488"><span data-slate-leaf="true" data-offset-key="2488:0" data-first-offset="true"><span data-slate-string="true"> 维的正交矩阵，其各自的所有列向量可以张成一个子空间；</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2489"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>D</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2491"><span data-slate-leaf="true" data-offset-key="2491:0" data-first-offset="true"><span data-slate-string="true"> 则是对角矩阵，对角线上的各个元素是数据矩阵 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2492"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2494"><span data-slate-leaf="true" data-offset-key="2494:0" data-first-offset="true"><span data-slate-string="true"> 按从大到小顺序排列的奇异值（singular value）</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2495"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2497"><span data-slate-leaf="true" data-offset-key="2497:0" data-first-offset="true"><span data-slate-string="true">。可以证明，岭回归求出的最优系数可以写成 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2498"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>β</span></span><span><span></span><span>^</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>(</span><span><span><span><span>X</span></span></span><span><span><span><span><span><span></span><span><span>T</span></span></span></span></span></span></span></span><span><span><span>X</span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>λ</span><span><span><span>I</span></span></span><span><span>)</span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span><span><span><span><span>X</span></span></span><span><span><span><span><span><span></span><span><span>T</span></span></span></span></span></span></span></span><span><span><span>y</span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2500"><span data-slate-leaf="true" data-offset-key="2500:0" data-first-offset="true"><span data-slate-string="true">。将 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2501"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2503"><span data-slate-leaf="true" data-offset-key="2503:0" data-first-offset="true"><span data-slate-string="true"> 的奇异值分解代入岭回归的预测输出中，就可以得到：</span></span></span></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-slate-type="block-katex" data-slate-object="block" data-key="2504"><span><span><span aria-hidden="true"><span><span></span><span><span><span>X</span></span></span><span><span><span><span><span><span><span></span><span>β</span></span><span><span></span><span>^</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>p</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span><span>u</span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>d</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span>λ</span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>d</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span><span><span><span>u</span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>y</span></span></span></span></span></span></span></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2506"><span data-slate-object="text" data-key="2507"><span data-slate-leaf="true" data-offset-key="2507:0" data-first-offset="true"><span data-slate-string="true">其中的 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2508"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span>u</span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2510"><span data-slate-leaf="true" data-offset-key="2510:0" data-first-offset="true"><span data-slate-string="true"> 是矩阵 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2511"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>U</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2513"><span data-slate-leaf="true" data-offset-key="2513:0" data-first-offset="true"><span data-slate-string="true"> 的列向量，也是 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2514"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2516"><span data-slate-leaf="true" data-offset-key="2516:0" data-first-offset="true"><span data-slate-string="true"> 的列空间的一组正交基，而岭回归计算出的结果正是将训练数据的输出 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2517"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>y</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2519"><span data-slate-leaf="true" data-offset-key="2519:0" data-first-offset="true"><span data-slate-string="true"> 投影到以 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2520"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span>u</span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2522"><span data-slate-leaf="true" data-offset-key="2522:0" data-first-offset="true"><span data-slate-string="true"> 为正交基的子空间上所得到的坐标。除了空间变换之外，岭回归的收缩特性也有体现，那就是上式中的系数。当正则化参数 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2523"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>λ</span></span></span></span></span></span><span data-slate-object="text" data-key="2525"><span data-slate-leaf="true" data-offset-key="2525:0" data-first-offset="true"><span data-slate-string="true"> 一定时，奇异值 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2526"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2528"><span data-slate-leaf="true" data-offset-key="2528:0" data-first-offset="true"><span data-slate-string="true"> 越小，它对应的坐标被衰减地就越厉害。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2529"><span data-slate-object="text" data-key="2530"><span data-slate-leaf="true" data-offset-key="2530:0" data-first-offset="true"><span data-slate-string="true">除了经历不同的收缩外，奇异值 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2531"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2533"><span data-slate-leaf="true" data-offset-key="2533:0" data-first-offset="true"><span data-slate-string="true"> 还有什么意义呢？</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2534"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2536"><span data-slate-leaf="true" data-offset-key="2536:0" data-first-offset="true"><span data-slate-string="true"> 的平方可以写成对角矩阵 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2537"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>D</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2539"><span data-slate-leaf="true" data-offset-key="2539:0" data-first-offset="true"><span data-slate-string="true"> 的平方，利用奇异值分解又可以推导出 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2540"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span>D</span></span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2542"><span data-slate-leaf="true" data-offset-key="2542:0" data-first-offset="true"><span data-slate-string="true"> 和数据矩阵 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2543"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2545"><span data-slate-leaf="true" data-offset-key="2545:0" data-first-offset="true"><span data-slate-string="true"> 如下的关系</span></span></span></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-slate-type="block-katex" data-slate-object="block" data-key="2546"><span><span><span aria-hidden="true"><span><span></span><span><span><span><span>X</span></span></span><span><span><span><span><span><span></span><span><span>T</span></span></span></span></span></span></span></span><span><span><span>X</span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>V</span></span></span><span><span><span><span>D</span></span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span><span><span><span>V</span></span></span><span><span><span><span><span><span></span><span><span>T</span></span></span></span></span></span></span></span></span></span></span></span></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2548"><span data-slate-object="text" data-key="2549"><span data-slate-leaf="true" data-offset-key="2549:0" data-first-offset="true"><span data-slate-string="true">这个表达式实际上就是矩阵的</span></span></span><span data-slate-object="text" data-key="2550"><span data-slate-leaf="true" data-offset-key="2550:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">特征分解</span></span></span></span><span data-slate-object="text" data-key="2551"><span data-slate-leaf="true" data-offset-key="2551:0" data-first-offset="true"><span data-slate-string="true">（eigen decomposition）：等式左侧的表达式实际上就是数据的协方差矩阵（covariance matrix）乘以维度 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2552"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>N</span></span></span></span></span></span><span data-slate-object="text" data-key="2554"><span data-slate-leaf="true" data-offset-key="2554:0" data-first-offset="true"><span data-slate-string="true">，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2555"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>V</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2557"><span data-slate-leaf="true" data-offset-key="2557:0" data-first-offset="true"><span data-slate-string="true"> 中的每一列 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2558"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2560"><span data-slate-leaf="true" data-offset-key="2560:0" data-first-offset="true"><span data-slate-string="true"> 都是协方差矩阵的特征向量，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2561"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span>D</span></span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2563"><span data-slate-leaf="true" data-offset-key="2563:0" data-first-offset="true"><span data-slate-string="true"> 中的每个对角元素 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2564"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2566"><span data-slate-leaf="true" data-offset-key="2566:0" data-first-offset="true"><span data-slate-string="true"> 则是对应的特征值。如果你对主成分分析还有印象，就不难发现每一个 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2567"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span>X</span></span></span><span><span><span><span>v</span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2569"><span data-slate-leaf="true" data-offset-key="2569:0" data-first-offset="true"><span data-slate-string="true"> 都是一个主成分（principal component），第 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2570"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>j</span></span></span></span></span></span><span data-slate-object="text" data-key="2572"><span data-slate-leaf="true" data-offset-key="2572:0" data-first-offset="true"><span data-slate-string="true"> 个主成分上数据的方差就是 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2573"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>/</span><span>N</span></span></span></span></span></span><span data-slate-object="text" data-key="2575"><span data-slate-leaf="true" data-offset-key="2575:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2576"><span data-slate-object="text" data-key="2577"><span data-slate-leaf="true" data-offset-key="2577:0" data-first-offset="true"><span data-slate-string="true">解释到这儿，就能够看出岭回归的作用了：</span></span></span><span data-slate-object="text" data-key="2578"><span data-slate-leaf="true" data-offset-key="2578:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">岭回归收缩系数的对象并非每个单独的属性，而是由属性的线性组合计算出来的互不相关的主成分，主成分上数据的方差越小，其系数收缩地就越明显</span></span></span></span><span data-slate-object="text" data-key="2579"><span data-slate-leaf="true" data-offset-key="2579:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2580"><span data-slate-object="text" data-key="2581"><span data-slate-leaf="true" data-offset-key="2581:0" data-first-offset="true"><span data-slate-string="true">数据在一个主成分上波动较大意味着主成分的取值对数据有较高的区分度，也就是上一季中提到的 “最大方差原理”。反之，数据在另一个主成分上方差较小就说明不同数据的取值较为集中，而聚成一团的数据显然是不容易区分的。岭回归正是通过削弱方差较小的主成分、保留方差较大的主成分来简化模型，实现正则化的。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="2582"><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/ea/64/ea5287f918ab983e30607dc988839264.png?wh=720*720"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2583"><span data-slate-object="text" data-key="2584"><span data-slate-leaf="true" data-offset-key="2584:0" data-first-offset="true"><span data-slate-type="secondary" data-slate-object="mark"><span data-slate-string="true">﻿﻿不同方差的主成分示意图，2 点钟方向的主成分方差较大，11 点钟方向的主成分方差较小（图片来自维基百科）</span></span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2585"><span data-slate-object="text" data-key="2586"><span data-slate-leaf="true" data-offset-key="2586:0" data-first-offset="true"><span data-slate-string="true">看到这里你可能就想到了：既然主成分都已经算出来了，与其用岭回归兜一个圈子，莫不如直接使用它们作为自变量来计算线性回归的系数，这种思路得到的就是</span></span></span><span data-slate-object="text" data-key="2587"><span data-slate-leaf="true" data-offset-key="2587:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">主成分回归</span></span></span></span><span data-slate-object="text" data-key="2588"><span data-slate-leaf="true" data-offset-key="2588:0" data-first-offset="true"><span data-slate-string="true">（principal component regression）。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2589"><span data-slate-object="text" data-key="2590"><span data-slate-leaf="true" data-offset-key="2590:0" data-first-offset="true"><span data-slate-string="true">主成分回归以每个主成分 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2591"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span>z</span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>X</span></span></span><span><span><span><span>v</span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2593"><span data-slate-leaf="true" data-offset-key="2593:0" data-first-offset="true"><span data-slate-string="true"> 作为输入计算回归参数。由于不同的主成分是两两正交的，因此这个看似多元线性回归的问题实质上是多个独立的简单线性回归的组合，每个主成分的权重系数可以表示为</span></span></span></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-slate-type="block-katex" data-slate-object="block" data-key="2594"><span><span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span></span><span>θ</span></span><span><span></span><span>^</span></span></span></span></span></span><span><span><span><span><span><span></span><span><span>m</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>&lt;</span><span></span><span><span><span><span>z</span></span></span><span><span><span><span><span><span></span><span><span>m</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span><span>z</span></span></span><span><span><span><span><span><span></span><span><span>m</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>&gt;</span></span></span><span><span></span><span></span></span><span><span></span><span><span>&lt;</span><span></span><span><span><span><span>z</span></span></span><span><span><span><span><span><span></span><span><span>m</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>y</span></span></span><span></span><span>&gt;</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2596"><span data-slate-object="text" data-key="2597"><span data-slate-leaf="true" data-offset-key="2597:0" data-first-offset="true"><span data-slate-string="true">其中 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2598"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>&lt;</span></span><span><span></span><span>&gt;</span></span></span></span></span></span><span data-slate-object="text" data-key="2600"><span data-slate-leaf="true" data-offset-key="2600:0" data-first-offset="true"><span data-slate-string="true"> 表示内积运算。需要注意的是这里的 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2601"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>y</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2603"><span data-slate-leaf="true" data-offset-key="2603:0" data-first-offset="true"><span data-slate-string="true"> 和数据矩阵的每一列 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2604"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span>x</span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2606"><span data-slate-leaf="true" data-offset-key="2606:0" data-first-offset="true"><span data-slate-string="true"> 都要做去均值的处理，主成分回归的常数项就是 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2607"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>y</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2609"><span data-slate-leaf="true" data-offset-key="2609:0" data-first-offset="true"><span data-slate-string="true">，也就是所有数据输出结果的均值 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2610"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>y</span></span><span><span></span><span>ˉ</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2612"><span data-slate-leaf="true" data-offset-key="2612:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2613"><span data-slate-object="text" data-key="2614"><span data-slate-leaf="true" data-offset-key="2614:0" data-first-offset="true"><span data-slate-string="true">当主成分回归中使用的主成分数目等于数据的属性数目 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2615"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span></span><span data-slate-object="text" data-key="2617"><span data-slate-leaf="true" data-offset-key="2617:0" data-first-offset="true"><span data-slate-string="true"> 时，主成分回归和岭回归的结果是一致的。可如果放弃方差最小的若干个主成分，得到的就是约化的回归结果，从而更加清晰地体现出主成分分析的思想。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2618"><span data-slate-object="text" data-key="2619"><span data-slate-leaf="true" data-offset-key="2619:0" data-first-offset="true"><span data-slate-string="true">主成分分析是典型的特征提取方法，它和收缩方法的本质区别在于将原始的共线性特征转化为人为生成的正交特征，从而带来了数据维度的约简和数据压缩的可能性。数字图像处理中的特征脸方法是主成分回归最典型的应用之一。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2620"><span data-slate-object="text" data-key="2621"><span data-slate-leaf="true" data-offset-key="2621:0" data-first-offset="true"><span data-slate-string="true">所谓 “</span></span></span><span data-slate-object="text" data-key="2622"><span data-slate-leaf="true" data-offset-key="2622:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">特征脸</span></span></span></span><span data-slate-object="text" data-key="2623"><span data-slate-leaf="true" data-offset-key="2623:0" data-first-offset="true"><span data-slate-string="true">”（eigenface）实际上就是用于人脸识别的主成分。用特征脸方法处理的人脸图像都具有相同的空间维度，假定图像的像素数目都是 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2624"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>1</span><span>0</span><span>0</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span>0</span><span>0</span></span></span></span></span></span><span data-slate-object="text" data-key="2626"><span data-slate-leaf="true" data-offset-key="2626:0" data-first-offset="true"><span data-slate-string="true">，那么每一个像素点都是一个属性，数字图像就变成了 10000 维空间中的一个点。可一般数字图像慢变特性决定了这 10000 个特征之间具有很强的关联，直接处理的话运算量较大不说，还未必有好的效果，可谓事倍功半。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="2627"><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/e6/37/e6fd81d7312849205e57e1007c792037.png?wh=1200*900"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2628"><span data-slate-object="text" data-key="2629"><span data-slate-leaf="true" data-offset-key="2629:0" data-first-offset="true"><span data-slate-type="secondary" data-slate-object="mark"><span data-slate-string="true">﻿﻿根据 AT&amp;T Laboratories Cambridge Facedatabase 人脸数据库生成的特征脸</span></span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2630"><span data-slate-object="text" data-key="2631"><span data-slate-leaf="true" data-offset-key="2631:0" data-first-offset="true"><span data-slate-type="secondary" data-slate-object="mark"><span data-slate-string="true">图片来自 https://www.bytefish.de/blog/eigenfaces/</span></span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2632"><span data-slate-object="text" data-key="2633"><span data-slate-leaf="true" data-offset-key="2633:0" data-first-offset="true"><span data-slate-string="true">引入主成分分析后，情况就不一样了。主成分分析可以将 10000 个相互关联的像素维度精炼成 100～150 个特征脸，再用这些特征脸来重构相同形状的人脸图像。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2634"><span data-slate-object="text" data-key="2635"><span data-slate-leaf="true" data-offset-key="2635:0" data-first-offset="true"><span data-slate-string="true">上图是真实计算出的一组特征脸图像，如果是晚上一个人在家玩手机的话，那这组惊悚的特征脸很可能让你吓得不轻。可如果你能想明白一个问题：这只是一组被打成正方形的 10000 多维的相互正交的主成分，而原始图像让它们碰巧具有了人脸的轮廓，这些人不人鬼不鬼的东西就没有那么恐怖了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2636"><span data-slate-object="text" data-key="2637"><span data-slate-leaf="true" data-offset-key="2637:0" data-first-offset="true"><span data-slate-string="true">这些主成分可以用来分解任意一张面孔，说不定我的一寸照片就可以表示成 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2638"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>2</span><span>7</span></span></span></span></span></span><span data-slate-object="text" data-key="2640"><span data-slate-leaf="true" data-offset-key="2640:0" data-first-offset="true"><span data-slate-string="true"> 的组合呢。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2641"><span data-slate-object="text" data-key="2642"><span data-slate-leaf="true" data-offset-key="2642:0" data-first-offset="true"><span data-slate-string="true">前面对主成分分析的解释都是从降维的角度出发的。换个角度，主成分分析可以看成</span></span></span><span data-slate-object="text" data-key="2643"><span data-slate-leaf="true" data-offset-key="2643:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">对高斯隐变量的概率描述</span></span></span></span><span data-slate-object="text" data-key="2644"><span data-slate-leaf="true" data-offset-key="2644:0" data-first-offset="true"><span data-slate-string="true">。隐变量（latent variable）是不能直接观测但可以间接推断的变量，虽然数据本身处在高维空间之中，但决定其变化特点的可能只是有限个参数，这些幕后的操纵者就是隐变量，寻找隐变量的过程就是对数据进行降维的过程。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2645"><span data-slate-object="text" data-key="2646"><span data-slate-leaf="true" data-offset-key="2646:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">概率主成分分析</span></span></span></span><span data-slate-object="text" data-key="2647"><span data-slate-leaf="true" data-offset-key="2647:0" data-first-offset="true"><span data-slate-string="true">（probabilistic principal component analysis）体现的就是高斯型观测结果和高斯隐变量之间线性的相关关系，它是因子分析（factor analysis）的一个特例。概率主成分分析的数学推导比较复杂，在这里不妨直接给出结论：</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2648"><span data-slate-object="text" data-key="2649"><span data-slate-leaf="true" data-offset-key="2649:0" data-first-offset="true"><span data-slate-string="true">假定一组数据观测值构成了 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2650"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>D</span></span></span></span></span></span><span data-slate-object="text" data-key="2652"><span data-slate-leaf="true" data-offset-key="2652:0" data-first-offset="true"><span data-slate-string="true"> 维向量 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2653"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2655"><span data-slate-leaf="true" data-offset-key="2655:0" data-first-offset="true"><span data-slate-string="true">，另外一组隐变量构成了 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2656"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>Q</span></span></span></span></span></span><span data-slate-object="text" data-key="2658"><span data-slate-leaf="true" data-offset-key="2658:0" data-first-offset="true"><span data-slate-string="true"> 维向量 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2659"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>Z</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2661"><span data-slate-leaf="true" data-offset-key="2661:0" data-first-offset="true"><span data-slate-string="true">，两者之间的线性关系就可以表示为</span></span></span></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-slate-type="block-katex" data-slate-object="block" data-key="2662"><span><span><span aria-hidden="true"><span><span></span><span><span><span>X</span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>W</span></span></span><span><span><span>Z</span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>μ</span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>ϵ</span></span></span></span></span></span></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2664"><span data-slate-object="text" data-key="2665"><span data-slate-leaf="true" data-offset-key="2665:0" data-first-offset="true"><span data-slate-string="true">其中关联矩阵 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2666"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>W</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2668"><span data-slate-leaf="true" data-offset-key="2668:0" data-first-offset="true"><span data-slate-string="true"> 是由标准正交基构成的矩阵，非零向量 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2669"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>μ</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2671"><span data-slate-leaf="true" data-offset-key="2671:0" data-first-offset="true"><span data-slate-string="true"> 表示观测值的均值，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2672"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>ϵ</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2674"><span data-slate-leaf="true" data-offset-key="2674:0" data-first-offset="true"><span data-slate-string="true"> 则是服从标准多维正态分布 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2675"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>N</span><span>(</span><span><span><span>0</span></span></span><span>,</span><span></span><span><span>σ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span><span><span>I</span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="2677"><span data-slate-leaf="true" data-offset-key="2677:0" data-first-offset="true"><span data-slate-string="true"> 的各向同性的噪声。如果假设隐变量 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2678"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>Z</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2680"><span data-slate-leaf="true" data-offset-key="2680:0" data-first-offset="true"><span data-slate-string="true"> 具有多元标准正态形式的先验分布 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2681"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span><span>Z</span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="2683"><span data-slate-leaf="true" data-offset-key="2683:0" data-first-offset="true"><span data-slate-string="true">，去均值观测数据 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2684"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span>X</span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2686"><span data-slate-leaf="true" data-offset-key="2686:0" data-first-offset="true"><span data-slate-string="true"> 的对数似然概率可以写成</span></span></span></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-slate-type="block-katex" data-slate-object="block" data-key="2687"><span><span><span aria-hidden="true"><span><span></span><span>lo<span>g</span></span><span></span><span>p</span><span>(</span><span><span><span>X</span></span></span><span>∣</span><span><span><span>W</span></span></span><span>,</span><span></span><span><span>σ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>−</span><span><span></span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span></span></span><span><span></span><span><span>N</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>ln</span><span></span><span>∣</span><span><span><span>C</span></span></span><span>∣</span><span></span><span>−</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>N</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span><span>x</span></span></span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span><span>C</span></span></span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span><span><span><span><span>x</span></span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2689"><span data-slate-object="text" data-key="2690"><span data-slate-leaf="true" data-offset-key="2690:0" data-first-offset="true"><span data-slate-string="true">其中 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2691"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span>C</span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>W</span></span></span><span><span><span><span>W</span></span></span><span><span><span><span><span><span></span><span><span>T</span></span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>σ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span><span><span>I</span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2693"><span data-slate-leaf="true" data-offset-key="2693:0" data-first-offset="true"><span data-slate-string="true">。计算可得，让似然概率取得最大值的参数值为 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2694"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span><span><span>W</span></span></span></span><span><span></span><span>^</span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>V</span></span></span><span>(</span><span><span><span><span>D</span></span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span><span>σ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span><span><span>I</span></span></span><span><span>)</span><span><span><span><span><span><span></span><span><span><span>1</span><span>/</span><span>2</span></span></span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2696"><span data-slate-leaf="true" data-offset-key="2696:0" data-first-offset="true"><span data-slate-string="true">。根据这个 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2697"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span><span><span>W</span></span></span></span><span><span></span><span>^</span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2699"><span data-slate-leaf="true" data-offset-key="2699:0" data-first-offset="true"><span data-slate-string="true"> 又可以计算出超参数 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2700"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>σ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2702"><span data-slate-leaf="true" data-offset-key="2702:0" data-first-offset="true"><span data-slate-string="true"> 得最大似然估计值 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2703"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span></span><span>σ</span></span><span><span></span><span>^</span></span></span></span></span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>D</span><span></span><span>−</span><span></span><span>Q</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>=</span><span>Q</span><span>+</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>D</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>d</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2705"><span data-slate-leaf="true" data-offset-key="2705:0" data-first-offset="true"><span data-slate-string="true">，这说明噪声方差就是所有被丢弃的主成分方差的均值。而当 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2706"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>ϵ</span></span><span></span><span>→</span><span></span></span><span><span></span><span>0</span></span></span></span></span></span><span data-slate-object="text" data-key="2708"><span data-slate-leaf="true" data-offset-key="2708:0" data-first-offset="true"><span data-slate-string="true"> 时，概率主成分分析的线性系数 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2709"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span><span><span>W</span></span></span></span><span><span></span><span>^</span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2711"><span data-slate-leaf="true" data-offset-key="2711:0" data-first-offset="true"><span data-slate-string="true"> 就会退化为经典的主成分分析中的 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2712"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>V</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2714"><span data-slate-leaf="true" data-offset-key="2714:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2715"><span data-slate-object="text" data-key="2716"><span data-slate-leaf="true" data-offset-key="2716:0" data-first-offset="true"><span data-slate-string="true">除了似然概率外，根据正态分布的性质也可以计算出隐变量的后验概率 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2717"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span><span>Z</span></span></span><span>∣</span><span><span><span>X</span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="2719"><span data-slate-leaf="true" data-offset-key="2719:0" data-first-offset="true"><span data-slate-string="true">。令 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2720"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span><span><span>F</span></span></span></span><span><span></span><span>^</span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span><span></span><span><span><span>W</span></span></span></span><span><span></span><span>^</span></span></span></span></span></span><span><span><span><span><span><span></span><span><span>T</span></span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>W</span></span></span></span><span><span></span><span>^</span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span><span><span><span><span><span></span><span>σ</span></span><span><span></span><span>^</span></span></span></span></span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span><span><span>I</span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2722"><span data-slate-leaf="true" data-offset-key="2722:0" data-first-offset="true"><span data-slate-string="true">，后验概率满足的就是以 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2723"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span></span><span><span><span>F</span></span></span></span><span><span></span><span>^</span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>W</span></span></span></span><span><span></span><span>^</span></span></span></span></span></span><span><span><span>X</span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2725"><span data-slate-leaf="true" data-offset-key="2725:0" data-first-offset="true"><span data-slate-string="true"> 为均值，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2726"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>σ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span><span><span><span><span><span><span></span><span><span><span>F</span></span></span></span><span><span></span><span>^</span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2728"><span data-slate-leaf="true" data-offset-key="2728:0" data-first-offset="true"><span data-slate-string="true"> 为方差的正态分布。当 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2729"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>ϵ</span></span><span></span><span>→</span><span></span></span><span><span></span><span>0</span></span></span></span></span></span><span data-slate-object="text" data-key="2731"><span data-slate-leaf="true" data-offset-key="2731:0" data-first-offset="true"><span data-slate-string="true"> 时，隐变量的最优值就会收敛为经典主成分 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2732"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span>X</span><span>V</span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2734"><span data-slate-leaf="true" data-offset-key="2734:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2735"><span data-slate-object="text" data-key="2736"><span data-slate-leaf="true" data-offset-key="2736:0" data-first-offset="true"><span data-slate-string="true">在实际问题中，使用的主成分数目是个超参数，需要通过模型选择确定，而概率主成分分析对测试数据的处理就可以完成模型选择的任务。从重构误差的角度出发，经典主成分分析一般会让被选中的主成分的特征值之和占所有特征值之和的 95% 以上。在贝叶斯框架下，计算最优的主成分数目需要对所有未知的参数超参数进行积分，其过程非常复杂，在这里就不讨论了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2737"><span data-slate-object="text" data-key="2738"><span data-slate-leaf="true" data-offset-key="2738:0" data-first-offset="true"><span data-slate-string="true">同其他隐变量模型一样，概率主成分分析也是个生成模型，其生成机制如下图所示。首先从服从一维正态分布的隐变量 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2739"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>z</span></span></span></span></span></span><span data-slate-object="text" data-key="2741"><span data-slate-leaf="true" data-offset-key="2741:0" data-first-offset="true"><span data-slate-string="true"> 中得到采样值 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2742"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>z</span></span><span><span></span><span>^</span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="2744"><span data-slate-leaf="true" data-offset-key="2744:0" data-first-offset="true"><span data-slate-string="true">，以 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2745"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span><span>w</span></span></span><span>z</span><span></span><span>+</span><span></span></span><span><span></span><span><span>μ</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2747"><span data-slate-leaf="true" data-offset-key="2747:0" data-first-offset="true"><span data-slate-string="true"> 为均值的单位方差二维正态分布就是数据 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2748"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>x</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2750"><span data-slate-leaf="true" data-offset-key="2750:0" data-first-offset="true"><span data-slate-string="true"> 的似然分布，将先验分布与似然分布相乘，得到的就是最右侧的二维分布 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="2751"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>x</span><span>)</span></span></span></span></span></span></span><span data-slate-object="text" data-key="2753"><span data-slate-leaf="true" data-offset-key="2753:0" data-first-offset="true"><span data-slate-string="true"> 了。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="2754"><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/92/e9/92ea88cc40155f4fa2e50b4e24c891e9.png?wh=769*255"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2755"><span data-slate-object="text" data-key="2756"><span data-slate-leaf="true" data-offset-key="2756:0" data-first-offset="true"><span data-slate-type="secondary" data-slate-object="mark"><span data-slate-string="true">﻿﻿概率主成分分析表示的数据生成机制</span></span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2757"><span data-slate-object="text" data-key="2758"><span data-slate-leaf="true" data-offset-key="2758:0" data-first-offset="true"><span data-slate-type="secondary" data-slate-object="mark"><span data-slate-string="true">图片来自 Machine Learning: A Probabilistic Perspective, 图 12.1</span></span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2759"><span data-slate-object="text" data-key="2760"><span data-slate-leaf="true" data-offset-key="2760:0" data-first-offset="true"><span data-slate-string="true">在 Scikit-learn 中，主成分分析对应的类是 PCA，它在 decomposition 的模块种。还是以英超数据集为例，对多元线性回归的数据进行主成分分析，可以得到 10 个主成分的方差，以及它们占总方差的比例：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="2761"><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/39/91/390f6ca36bd154f9d0d61e58f79b8891.jpeg?wh=652*130"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2762"><span data-slate-object="text" data-key="2763"><span data-slate-leaf="true" data-offset-key="2763:0" data-first-offset="true"><span data-slate-type="secondary" data-slate-object="mark"><span data-slate-string="true">英超数据集上所有主成分的方差及其比例</span></span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2764"><span data-slate-object="text" data-key="2765"><span data-slate-leaf="true" data-offset-key="2765:0" data-first-offset="true"><span data-slate-string="true">从结果中可以看出，方差最大的主成分占据了近 4/5 的总方差，前两个主成分的方差之和的比例则超过了 90%。在对数据进行降维时，如果将方差的比例阈值设定为 95%，保留的主成分数目就是 2 个，这说明 2 个主成分已经足以解释输出结果中 90% 的变化。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="2766"><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/31/48/31f4e937adea5a8ed8ce4edb19a95d48.png?wh=640*480"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2767"><span data-slate-object="text" data-key="2768"><span data-slate-leaf="true" data-offset-key="2768:0" data-first-offset="true"><span data-slate-type="secondary" data-slate-object="mark"><span data-slate-string="true">使用前两个主成分对英超数据集进行变换的结果</span></span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2769"><span data-slate-object="text" data-key="2770"><span data-slate-leaf="true" data-offset-key="2770:0" data-first-offset="true"><span data-slate-string="true">为了对主成分分析后的数据分布产生直观的认识，可以将变换后的数据点显示在低维空间中，以观察它们的集中程度。出于观察方便的考虑，在可视化时只选择了方差最大的前两个主成分，虽然这样做会造成较大的误差，但变换后的数据就可以在平面直角坐标系上显示出来，如上图所示。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2771"><span data-slate-object="text" data-key="2772"><span data-slate-leaf="true" data-offset-key="2772:0" data-first-offset="true"><span data-slate-string="true">可以看出，经过变换后的数据点依然分散在整个二维平面上，但根据它们在横轴上的取值已经可以近似地将数据划分为两个类别，其原因很可能是蓝线两侧的数据代表了两种类型的球队风格，就像来自两个高斯分布的随机数。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2773"><span data-slate-object="text" data-key="2774"><span data-slate-leaf="true" data-offset-key="2774:0" data-first-offset="true"><span data-slate-string="true">今天我和你分享了从岭回归到主成分回归的推导过程，以及作为降维方法和特征提取技术的主成分分析，其要点如下：</span></span></span></div><div data-slate-type="list" data-slate-object="block" data-key="2775"><div data-slate-type="list-line" data-slate-object="block" data-key="2776"><span data-slate-object="text" data-key="2777"><span data-slate-leaf="true" data-offset-key="2777:0" data-first-offset="true"><span data-slate-type="primary" data-slate-object="mark"><span data-slate-string="true"> 在有限的数据集下，数据维度过高会导致维数灾难；</span></span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="2778"><span data-slate-object="text" data-key="2779"><span data-slate-leaf="true" data-offset-key="2779:0" data-first-offset="true"><span data-slate-type="primary" data-slate-object="mark"><span data-slate-string="true">降维的方法包括特征选择和特征提取；</span></span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="2780"><span data-slate-object="text" data-key="2781"><span data-slate-leaf="true" data-offset-key="2781:0" data-first-offset="true"><span data-slate-type="primary" data-slate-object="mark"><span data-slate-string="true">主成分分析将原始的共线性特征转化为新的正交特征，从而实现特征提取；</span></span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="2782"><span data-slate-object="text" data-key="2783"><span data-slate-leaf="true" data-offset-key="2783:0" data-first-offset="true"><span data-slate-type="primary" data-slate-object="mark"><span data-slate-string="true">概率主成分分析是因子分析的一种，是数据的生成模型。</span></span></span></span></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2784"><span data-slate-object="text" data-key="2785"><span data-slate-leaf="true" data-offset-key="2785:0" data-first-offset="true"><span data-slate-string="true">在机器学习中，还有一种和主成分分析名字相似的方法，叫作</span></span></span><span data-slate-object="text" data-key="2786"><span data-slate-leaf="true" data-offset-key="2786:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">独立成分分析</span></span></span></span><span data-slate-object="text" data-key="2787"><span data-slate-leaf="true" data-offset-key="2787:0" data-first-offset="true"><span data-slate-string="true">（independent component analysis）。那么这两者之间到底有什么区别和联系呢？</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="2788"><span data-slate-object="text" data-key="2789"><span data-slate-leaf="true" data-offset-key="2789:0" data-first-offset="true"><span data-slate-string="true">你可以查阅资料加以了解，并在这里分享你的理解。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="2790"><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/58/d9/58c45c5e94e8d8c8f05e4f355790cbd9.jpg?wh=2379*2408"></div></div></div></div></div><div><div></div><div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><textarea placeholder="将学到的知识总结成笔记，方便日后快速查找及复习"></textarea></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div><div><div>确认放弃笔记？</div><div>放弃后所记笔记将不保留。</div></div><div><div>新功能上线，你的历史笔记已初始化为私密笔记，是否一键批量公开？</div><div>批量公开的笔记不会为你同步至部落</div></div><div></div></div><div><div><div>公开</div><div>同步至部落</div></div><div>取消</div><div>完成</div></div><div><span>0/2000</span></div></div><div><div><span>划线</span></div><div></div><div></div><div></div><div><span>笔记</span></div><div></div><div><span>复制</span></div></div></div><div><div><div><span></span></div><p><a href="javascript:void(0);">给文章提建议</a></p></div></div><div><span>©</span> 版权归极客邦科技所有，未经许可不得传播售卖。 页面已增加防盗追踪，如有侵权极客邦将依法追究其法律责任。 </div></div><div><div><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div><div>Geek__653581f21177</div></div><div><textarea placeholder="由作者筛选后的优质留言将会公开显示，欢迎踊跃留言。" rows="16"></textarea></div><div><div>Ctrl + Enter 发表</div><div>0/2000 字符</div><div>提交留言</div></div></div><div><h2 id="sr-toc-1">精选留言 (6)</h2><ul><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>林彦</span></div></div><div>PCA 和 ICA 都是把原始特征线性组合转换成新的不相关的特征，PCA 里转换后的特征是正交的。网上搜索到的 ICA 会在数据白化预处理 (data whitening) 用到 PCA，我的理解 ICA 转换产生的特征也是正交的。

PCA 和 LDA 都是以观测数据点呈高斯分布为假设前提，而 ICA 假设观测信号是非高斯分布的信号源的线性组合，信号源分量都不是高斯分布或者最多只有一个是高斯分布。

ICA 生成的新特征分量不仅是不相关的，而且要求是统计独立的。我的理解是这个要求比 PCA 的不相关或正交要求更高，包含更多信息。PCA 的数据有可能不是由一些互相统计独立的特征分量生成的。如何区别 ICA 中的统计独立和 PCA 中的不相关 / 正交我不懂。有文章提到在原始随机信号 x 是高斯随机向量，其 PCA 变换得到的 y 也是高斯随机向量，y 的各个分量不仅是线性无关的，它们还是独立的。这段描述令我对线性无关和独立的区别更加困惑。

PCA 选择新特征时用方差作为衡量标准，ICA 根据网上部分描述会用到 “非高斯性度量 “来作为衡量标准。这里是不是衡量标准一定会有区别我不确定。</div><div><p>作者回复: ICA 是盲源分离的一种手段，它假设接收到的数据来源于统计独立的不同分量的线性叠加，所以它的独立性是解决问题的前提。典型的例子是鸡尾酒会问题：酒会上人声嘈杂，不同的声音混在一起，ICA 就要实现解混，分解出每个人的声音。
统计独立的概念要强于不相关。不相关只需要协方差为 0，统计独立则要求联合分布等于各自分布的乘积。所以在评价 ICA 时，指标的核心在于不同成分之间是不是真的独立，方差这些则不在关注范围。
之所以关注非高斯性是由于中心极限定理说明了大量随机独立分布的叠加是高斯分布。独立成分的非高斯性可以保证分离结果的可辨识性。从机器学习角度看，ICA 应该属于一种隐变量模型。</p></div><div><div>2018-07-04</div><div><div><i></i><span>2</span></div><div><i></i><span>4</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/account/avatar/00/12/57/6e/ae14e53b.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>Howard.Wundt</span></div></div><div>老师的文章排版非常优美，值得学习。
目前极客时间导出到印象笔记时，版面会发生变化，公式与文字之间错位严重，各位同学有何好办法处理之？</div><div><div>2018-10-03</div><div><div><i></i><span></span></div><div><i></i><span>1</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132"></div></div><div><div><div><span> 杨家荣</span></div></div><div>极客时间
21 天打卡行动 51/21
&lt;&lt;机器学习 40 讲 / 13&gt;&gt; 线性降维：主成分的使用
回答老师问题
在机器学习中，还有一种和主成分分析名字相似的方法，叫作独立成分分析（independent component analysis）。那么这两者之间到底有什么区别和联系呢？
1. 主成分分析假设源信号间彼此非相关，独立成分分析假设源信号间彼此独立。
2. 主成分分析认为主元之间彼此正交，样本呈高斯分布；独立成分分析则不要求样本呈高斯分布。
来源:[https://blog.csdn.net/shenziheng1/article/details/53547401]
今日所学:
1, 维数灾难深层次的原因在于数据样本的有限。
2, 特征选择（feature selection）;
3, 岭回归收缩系数的对象并非每个单独的属性，而是由属性的线性组合计算出来的互不相关的主成分，主成分上数据的方差越小，其系数收缩地就越明显。
4, 主成分回归；
5, 主成分分析是典型的特征提取方法，它和收缩方法的本质区别在于将原始的共线性特征转化为人为生成的正交特征，从而带来了数据维度的约简和数据压缩的可能性；
6, 主成分分析可以看成对高斯隐变量的概率描述，
7, 隐变量（latent variable）是不能直接观测但可以间接推断的变量；
8, 概率主成分分析（probabilistic principal component analysis）体现的就是高斯型观测结果和高斯隐变量之间线性的相关关系，它是因子分析（factor analysis）的一个特例；
重点:
降维方法和特征提取技术要点:
 在有限的数据集下，数据维度过高会导致维数灾难；
降维的方法包括特征选择和特征提取；
主成分分析将原始的共线性特征转化为新的正交特征，从而实现特征提取；
概率主成分分析是因子分析的一种，是数据的生成模型。</div><div><div>2020-02-07</div><div><div><i></i><span></span></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/account/avatar/00/11/c4/eb/0cd6d6ff.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>zhoujie</span></div></div><div>收缩方法可以使系数连续变化，这里 “连续变化” 怎么理解，收缩方法可以使系数缩小或者带来稀疏可以理解</div><div><p>作者回复：意思是不会从 1 跳变到 0，而是按 1 0.9 0.8 0.7 这样地变化</p></div><div><div>2018-09-10</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/10/3c/1f/3948a3c6.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>paradox</span></div></div><div>老师，您好
对于用 SVD 解释 PCA
是不是
行数表示特征数，列数表示数据样本的个数，这样 SVD 后，就是 U 矩阵用作降维了。
如果是行数表示数据样本的个数，列数表示特征数，SVD 后，就是 V 矩阵用作降维了。</div><div><p>作者回复：一般都是你说的后一种情况，就是把同一个数据写成矩阵的一个行，很少有写成列的。像 sklearn 这些成熟的库也是这样处理。</p></div><div><div>2018-08-10</div><div><div><i></i><span>3</span></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/fd/e3/f40eaddd.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>兆熊</span></div></div><div>和第一季相比，第二季每篇文章的篇幅长了很多。建议老师将长文章一分为二，将每篇文章的语音控制在十分钟左右，以达到更好的学习效果。</div><div><p>作者回复：这个我和极客时间的团队反映一下。</p></div><div><div>2018-07-03</div><div><div><i></i><span>2</span></div><div><i></i><span>1</span></div></div></div></div></div></li></ul><div> 收起评论<span></span></div></div></sr-rd-content>
                            <toc-bg><toc style="width: initial;overflow: auto!important;"class="simpread-font simpread-theme-root" data-reactid=".17"><outline class="toc-level-h1" data-reactid=".17.0" style="width: 175px;"><active data-reactid=".17.0.0" ></active><a class="toc-outline-theme-github" href="#sr-toc-0" data-reactid=".17.0.1"><span data-reactid=".17.0.1.0">13 | 线性降维：主成分的使用</span></a></outline><outline class="toc-level-h2" data-reactid=".17.1" style="width: 165px;"><active data-reactid=".17.1.0"></active><a class="toc-outline-theme-github" href="#sr-toc-1" data-reactid=".17.1.1"><span data-reactid=".17.1.1.0">精选留言 (6)</span></a></outline></toc></toc-bg>
                            <sr-rd-footer>
                                <sr-rd-footer-group>
                                    <sr-rd-footer-line></sr-rd-footer-line>
                                    <sr-rd-footer-text>全文完</sr-rd-footer-text>
                                    <sr-rd-footer-line></sr-rd-footer-line>
                                </sr-rd-footer-group>
                                <sr-rd-footer-copywrite>
                                    <div>本文由 <a href="http://ksria.com/simpread" target="_blank">简悦 SimpRead</a> 转码，用以提升阅读体验，<a href="https://time.geekbang.org/column/article/10160" target="_blank">原文地址 </a></div>
                                </sr-rd-footer-copywrite>
                            </sr-rd-footer>
                        </sr-read>
                    </body>
                </html>