
                <html lang="en" class="simpread-font simpread-theme-root" style=''>
                    <head>
                        <meta charset="utf-8">
                        <meta http-equiv="content-type" content="text/html; charset=UTF-8;charset=utf-8">
                        <meta http-equiv="X-UA-Compatible" content="IE=Edge">
                        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1">
                        <meta name="author" content="Kenshin"/>
                        <meta name="description" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展" />
                        <meta name="keywords" content="Chrome extension, Chrome 扩展, 阅读模式, 沉浸式阅读, 简悦, 简阅, read mode, reading mode, reader view, firefox, firefox addon, userscript, safari, opera, tampermonkey"/>
                        <meta name="thumbnail" content="https://simpread-1254315611.cos.ap-shanghai.myqcloud.com/static/introduce-2.png"/>
                        <meta property="og:title" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展"/>
                        <meta property="og:type" content="website">
                        <meta property="og:local" content="zh_CN"/>
                        <meta property="og:url" content="http://ksria.com/simpread"/>
                        <meta property="og:image" content="https://simpread-1254315611.cos.ap-shanghai.myqcloud.com/static/introduce-2.png"/>
                        <meta property="og:image:type" content="image/png"/>
                        <meta property="og:image:width" content="960"/>
                        <meta property="og:image:height" content="355"/>
                        <meta property="og:site_name" content="http://ksria.com/simpread"/>
                        <meta property="og:description" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展"/>
                        <style type="text/css">.simpread-font{font:300 16px/1.8 -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#333;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased}.simpread-hidden{display:none}.simpread-read-root{display:-webkit-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;margin:0;top:-1000px;left:0;width:100%;z-index:2147483646;overflow-x:hidden;opacity:0;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}.simpread-read-root-show{top:0}.simpread-read-root-hide{top:1000px}sr-read{display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;margin:20px 20%;min-width:400px;min-height:400px;text-align:center}read-process{position:fixed;top:0;left:0;height:3px;width:100%;background-color:#64b5f6;-webkit-transition:width 2s;transition:width 2s;z-index:20000}sr-rd-content-error{display:block;position:relative;margin:0;margin-bottom:30px;padding:25px;background-color:rgba(0,0,0,.05)}sr-rd-footer{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;font-size:14px}sr-rd-footer,sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}sr-rd-footer-group{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}sr-rd-footer-text{min-width:150px}sr-rd-footer-copywrite{margin:10px 0 0;color:inherit}sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}sr-rd-footer-copywrite .second{margin:10px 0}sr-rd-footer-copywrite .third a:hover{border:none!important}sr-rd-footer-copywrite .third a:first-child{margin-right:50px}sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}sr-rd-footer-copywrite .sr-icon:hover{opacity:1}sr-rd-footer-copywrite a,sr-rd-footer-copywrite a:link,sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}sr-rd-footer-copywrite a:focus,sr-rd-footer-copywrite a:hover,sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.simpread-blocks{text-decoration:none!important}.simpread-blocks *{margin:0}.simpread-blocks a{padding:0;text-decoration:none!important}.simpread-blocks img{margin:0;padding:0;border:0;background:transparent;box-shadow:none}.simpread-focus-root{display:block;position:fixed;top:0;left:0;right:0;bottom:0;background-color:hsla(0,0%,92%,.9);z-index:2147483645;opacity:0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms}.simpread-focus-highlight{position:relative;box-shadow:0 0 0 20px #fff;background-color:#fff;overflow:visible;z-index:2147483646}.sr-controlbar-bg sr-rd-crlbar,.sr-controlbar-bg sr-rd-crlbar fab{z-index:2147483647}sr-rd-crlbar.controlbar{position:fixed;right:0;bottom:0;width:100px;height:100%;opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}sr-rd-crlbar.controlbar:hover{opacity:1}sr-rd-crlbar fap *{box-sizing:border-box}@media (max-height:620px){fab{zoom:.8}}@media (max-height:783px){dialog-gp dialog-content{max-height:580px}dialog-gp dialog-footer{border-top:1px solid #e0e0e0}}.simpread-highlight-selector{outline:3px dashed #1976d2!important;cursor:pointer!important}.simpread-highlight-controlbar,.simpread-highlight-selector{background-color:#fafafa!important;opacity:.8!important;-webkit-transition:opacity .5s ease!important;transition:opacity .5s ease!important}.simpread-highlight-controlbar{position:relative!important;border:3px dashed #1976d2!important}simpread-highlight,sr-snapshot-ctlbar{position:fixed;top:0;left:0;right:0;padding:15px;height:50px;background-color:rgba(50,50,50,.9);box-shadow:0 2px 5px rgba(0,0,0,.26);box-sizing:border-box;z-index:2147483640}simpread-highlight,sr-highlight-ctl,sr-snapshot-ctlbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-highlight-ctl{margin:0 5px;width:50px;height:20px;color:#fff;background-color:#1976d2;border-radius:4px;box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12);cursor:pointer}toc-bg{position:fixed;left:0;top:0;width:50px;height:200px;font-size:medium}toc-bg:hover{z-index:3}.toc-bg-hidden{opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}.toc-bg-hidden:hover{opacity:1;z-index:3}.toc-bg-hidden:hover toc{width:180px}toc *{all:unset}toc{position:fixed;left:0;top:100px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;padding:10px;width:0;max-width:200px;max-height:500px;overflow-x:hidden;overflow-y:hidden;cursor:pointer;border:1px solid hsla(0,0%,62%,.22);-webkit-transition:width .5s;transition:width .5s}toc:hover{overflow-y:auto}toc.mini:hover{width:200px!important}toc::-webkit-scrollbar{width:3px}toc::-webkit-scrollbar-thumb{border-radius:10px;background-color:hsla(36,2%,54%,.5)}toc outline{position:relative;display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;overflow:hidden;text-overflow:ellipsis;padding:2px 0;min-height:21px;line-height:21px;text-align:left}toc outline a,toc outline a:active,toc outline a:focus,toc outline a:visited{display:block;width:100%;color:inherit;font-size:11px;text-decoration:none!important;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}toc outline a:hover{font-weight:700!important}toc outline a.toc-outline-theme-dark,toc outline a.toc-outline-theme-night{color:#fff!important}.toc-level-h1{padding-left:5px}.toc-level-h2{padding-left:15px}.toc-level-h3{padding-left:25px}.toc-level-h4{padding-left:35px}.toc-outline-active{border-left:2px solid #f44336}toc outline active{position:absolute;left:0;top:0;bottom:0;padding:0 0 0 3px;border-left:2px solid #e8e8e8}sr-kbd{background:-webkit-gradient(linear,0 0,0 100%,from(#fff785),to(#ffc542));border:1px solid #e3be23;-o-border-image:none;border-image:none;-o-border-image:initial;border-image:initial;position:absolute;left:0;padding:1px 3px 0;font-size:11px!important;font-weight:700;box-shadow:0 3px 7px 0 rgba(0,0,0,.3);overflow:hidden;border-radius:3px}.sr-kbd-a{position:relative}kbd-mapping{position:fixed;left:5px;bottom:5px;-ms-flex-flow:row;flex-flow:row;width:250px;height:500px;background-color:#fff;border:1px solid hsla(0,0%,62%,.22);box-shadow:0 2px 5px rgba(0,0,0,.26);border-radius:3px}kbd-mapping,kbd-maps{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}kbd-maps{margin:40px 0 20px;width:100%;overflow-x:auto}kbd-maps::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}kbd-maps::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}kbd-mapping kbd-map-title{position:absolute;margin:5px 0;width:100%;font-size:14px;font-weight:700}kbd-maps-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}kbd-maps-title{margin:5px 0;padding-left:53px;font-size:12px;font-weight:700}kbd-map kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fafbfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}kbd-map kbd-name{display:inline-block;text-align:right;width:50px}kbd-map kbd-desc{padding-left:3px}sharecard-bg{position:fixed;top:0;left:0;width:100%;height:100%;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(0,0,0,.4);z-index:2147483647}sharecard{max-width:450px;background-color:#64b5f6}sharecard,sharecard-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-head{margin:25px;color:#fff;border-radius:10px;box-shadow:0 2px 6px 0 rgba(0,0,0,.2),0 25px 50px 0 rgba(0,0,0,.15)}sharecard-card{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-card,sharecard-top{display:-webkit-box;display:-ms-flexbox;display:flex}sharecard-top{-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:65px;background-color:#fff;color:#878787;font-size:25px;font-weight:500;border-top-left-radius:10px;border-top-right-radius:10px}sharecard-top span.logos{display:block;width:48px;height:48px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAMAAABg3Am1AAABU1BMVEUAAAAnNJMnNZI3Q5onNJInNJMnNJInNJMnNJI8SJ0tOZY/S55EUKAoNJI6RpwoNJNIU6InNJInNJImNJI7SJwmNJJ2fLUiMJFKVaNCTJ9faK1HUaJOWKVSXaUnNJNYY6pye7cmM5JXYKhwebMjMI8mL4719fW9vb0oNZP/UlLz8/QqN5TAwMAnNJPv7+/Pz8/q6+/p6enNzc3Kysry8vMsOJXc3env7/LU1uXo29vR0dHOzs7ExMTwjo73bW37XV3Aj1TCYELl5u3n5+fW2Obn6O7f4OrZ2+g0QJkxPpgvO5bh4uvS1OTP0ePCwsJQW6ZLVqTs7fHd3d3V1dXqv79VX6lET6A1TIxXUIBSgHxWQnpelHf+WVnopkXqbC7j5Ozi4uLDw8NGUaFATJ9SgH3r6+vGyd7BxNva2trX19ejqM2gpczHx8dze7Zha67Z2dlTgH1aXQeSAAAAJnRSTlMA6ff+497Y8NL+/fv49P379sqab/BeOiX06tzVy8m/tKqpalA7G6oKj0EAAAJlSURBVEjHndNXWxpBFIDhcS2ICRLAkt4Dx4WhLk0E6R0MYoIISrWX5P9f5cwSIRC2+T1czMV5n2FnZwn2eWONUqCAv3H2Uf5Ra1hx4+0WEXtDQW0fCPYJ1EffEfIV4CSROAE4jsePoTFsNmTJF/IeIHF2lgCIn57GodlqDWXBK7IwBYatVlMWFAildPKX7I3m74Z9fsCiQChoimoFQAz04Ad2gH1n9fv9n9hgMNDr9euLWD6fLxQKxaLfb7dTSlahbFVdEPwIQtrAihZQgyKCtCagbQe3xh0QFMgy5MR11+ewYY5/qlZ7vT2xu93ULKjbFLpiUxnIIwjgKmVTLDUFXMrAi2NJWCRLIthTBo4xyOLKpwyqU6CuDCI41hFBCVdOhyLw4FgJ1skCAiyl9BSHbCorgo6VJXTru5hrVCQS8Yr5xLzX59YJSFpVFwD9U0BGC3hGdFpATgRupTGe9R9I1b1ePBvXKDyvq/O/44LT4/E4BUbSCAwj8Evq6HlnOBprx6JhJz8Gktc7xeaP9ndY+0coQvCccFBD4JW60UIY50ciLOAODAQRVOeCHm4Q3Xks6uRDY+CQ+AR4T2wMYh6+jMCIQOp78CFoj0H7EQgIuhI3dGaHCrwgADwCPjJvA372GRigCJg49FUdk3D87pq3zp4SA5zc1Zh9DxfwkpjgUg5Mv+lbeE3McC8Lpu7SA3wk2xzcqL2tN5DfIsQC8HB7UamUy6FQOpTO5QKBQDZbKnWSyUzGjdWCwaDA8+7Le4BNgm3qQGWchYh9s5hNq6wVbBlbwhZYOp3OYOA4zmgEypnM2zj8ByIdedKrH8vDAAAAAElFTkSuQmCC");zoom:.8}sharecard-content{padding:15px;max-height:500px;font-size:20px;text-align:justify;background-color:#2196f3;overflow-x:hidden;overflow-y:auto}sharecard-via{padding:10px;font-size:10px;background-color:#2196f3}sharecard-footer{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:100px;background-color:#fff;color:#878787;font-size:15px;font-weight:500;border-bottom-left-radius:10px;border-bottom-right-radius:10px}sharecard-footer,sharecard-footer div{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sharecard-footer span.qrcode{display:block;width:100px;height:100px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,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")}sharecard-control{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:0 19px;height:80px;background-color:#fff}simpread-snapshot{width:100%;height:100%;cursor:move;z-index:2147483645}simpread-snapshot,sr-mask{position:fixed;left:0;top:0}sr-mask{background-color:rgba(0,0,0,.1)}.simpread-feedback,.simpread-urlscheme{position:fixed;right:20px;bottom:20px;z-index:2147483646}simpread-feedback,simpread-urlscheme{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:20px 20px 0;width:500px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:3px;box-shadow:0 0 2px rgba(0,0,0,.12),0 2px 2px rgba(0,0,0,.26);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease}simpread-feedback *,simpread-urlscheme *{font-size:12px!important;box-sizing:border-box}simpread-feedback.active,simpread-urlscheme.active{-webkit-animation-name:srFadeInUp;animation-name:srFadeInUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback.hide,simpread-urlscheme.hide{-webkit-animation-name:srFadeInDown;animation-name:srFadeInDown;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback sr-fb-label,simpread-urlscheme sr-urls-label{width:100%}simpread-feedback sr-fb-head,simpread-urlscheme sr-urls-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin-bottom:5px;width:100%}simpread-feedback sr-fb-content,simpread-urlscheme sr-urls-content{margin-bottom:5px;width:100%}simpread-feedback sr-urls-footer,simpread-urlscheme sr-urls-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-fb-a,simpread-urlscheme sr-urls-a{color:#2163f7;cursor:pointer}simpread-feedback text-field-state,simpread-urlscheme text-field-state{border-top:none rgba(34,101,247,.8)!important;border-left:none rgba(34,101,247,.8)!important;border-right:none rgba(34,101,247,.8)!important;border-bottom:2px solid rgba(34,101,247,.8)!important}simpread-feedback switch,simpread-urlscheme switch{margin-top:0!important}@-webkit-keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@-webkit-keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}@keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}simpread-feedback sr-fb-head{font-weight:700}simpread-feedback sr-fb-content{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}simpread-feedback sr-fb-content,simpread-feedback sr-fb-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}simpread-feedback sr-fb-footer{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-close{position:absolute;right:20px;cursor:pointer;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:200}simpread-feedback sr-close:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-top:10px}simpread-feedback sr-stars i{margin-right:10px;cursor:pointer}simpread-feedback sr-stars i svg{-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}simpread-feedback sr-stars i svg:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars i.active svg{-webkit-transform:rotate(0) scale(1);transform:rotate(0) scale(1)}simpread-feedback sr-emojis{display:block;height:100px;overflow:hidden}simpread-feedback sr-emoji{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:.3s;transition:.3s}simpread-feedback sr-emoji>svg{margin:15px 0;width:70px;height:70px;-ms-flex-negative:0;flex-shrink:0}simpread-feedback sr-stars-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin:10px 0 20px}</style>
                        <style type="text/css">.simpread-theme-root{font-size:62.5%!important}sr-rd-content,sr-rd-desc,sr-rd-title{width:100%}sr-rd-title{display:-webkit-box;margin:1em 0 .5em;overflow:hidden;text-overflow:ellipsis;text-rendering:optimizelegibility;-webkit-line-clamp:3;-webkit-box-orient:vertical}sr-rd-content{text-align:left;word-break:break-word}sr-rd-desc{text-align:justify;line-height:2.4;margin:0 0 1.2em;box-sizing:border-box}sr-rd-content{font-size:25.6px;font-size:1.6rem;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:25.6px;font-size:1.6rem}sr-rd-content div,sr-rd-content p,sr-rd-content pre,sr-rd-content sr-blockquote{margin:0 0 1.2em;word-break:break-word}sr-rd-content a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-rd-content a,sr-rd-content a:link{color:inherit;font-size:inherit;font-weight:inherit;border:none}sr-rd-content a:hover{background:transparent}sr-rd-content img{margin:10px;padding:5px;max-width:100%;background:#fff;border:1px solid #bbb;box-shadow:1px 1px 3px #d4d4d4}sr-rd-content figcaption{text-align:center;font-size:14px}sr-rd-content sr-blockquote{display:block;position:relative;padding:15px 25px;text-align:left;line-height:inherit}sr-rd-content sr-blockquote:before{position:absolute}sr-rd-content sr-blockquote *{margin:0;font-size:inherit}sr-rd-content table{width:100%;margin:0 0 1.2em;word-break:keep-all;word-break:normal;overflow:auto;border:none}sr-rd-content table td,sr-rd-content table th{border:none}sr-rd-content ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-rd-content ol{list-style:decimal;margin:0;padding:0}sr-rd-content ol li,sr-rd-content ul li{font-size:inherit;list-style:disc;margin:0 0 1.2em}sr-rd-content ol li{list-style:decimal;margin-left:1.3em}sr-rd-content ol li *,sr-rd-content ul li *{margin:0;text-align:left;text-align:initial}sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em}sr-rd-content li ul{list-style:circle}sr-rd-content pre{font-family:Consolas,Monaco,Andale Mono,Source Code Pro,Liberation Mono,Courier,monospace;display:block;padding:15px;line-height:1.5;word-break:break-all;word-wrap:break-word;white-space:pre;overflow:auto}sr-rd-content pre,sr-rd-content pre *,sr-rd-content pre div{font-size:17.6px;font-size:1.1rem}sr-rd-content li pre code,sr-rd-content p pre code,sr-rd-content pre{background-color:transparent;border:none}sr-rd-content pre code{margin:0;padding:0}sr-rd-content pre code,sr-rd-content pre code *{font-size:17.6px;font-size:1.1rem}sr-rd-content pre p{margin:0;padding:0;color:inherit;font-size:inherit;line-height:inherit}sr-rd-content li code,sr-rd-content p code{margin:0 4px;padding:2px 4px;font-size:17.6px;font-size:1.1rem}sr-rd-content mark{margin:0 5px;padding:2px;background:#fffdd1;border-bottom:1px solid #ffedce}.sr-rd-content-img{width:90%;height:auto}.sr-rd-content-img-load{width:48px;height:48px;margin:0;padding:0;border-style:none;border-width:0;background-repeat:no-repeat;background-image:url(data:image/gif;base64,R0lGODlhMAAwAPcAAAAAABMTExUVFRsbGx0dHSYmJikpKS8vLzAwMDc3Nz4+PkJCQkRERElJSVBQUFdXV1hYWFxcXGNjY2RkZGhoaGxsbHFxcXZ2dnl5eX9/f4GBgYaGhoiIiI6OjpKSkpaWlpubm56enqKioqWlpampqa6urrCwsLe3t7q6ur6+vsHBwcfHx8vLy8zMzNLS0tXV1dnZ2dzc3OHh4eXl5erq6u7u7vLy8vf39/n5+f///wEBAQQEBA4ODhkZGSEhIS0tLTk5OUNDQ0pKSk1NTV9fX2lpaXBwcHd3d35+foKCgoSEhIuLi4yMjJGRkZWVlZ2dnaSkpKysrLOzs7u7u7y8vMPDw8bGxsnJydvb293d3eLi4ubm5uvr6+zs7Pb29gYGBg8PDyAgICcnJzU1NTs7O0ZGRkxMTFRUVFpaWmFhYWVlZWtra21tbXNzc3V1dXh4eIeHh4qKipCQkJSUlJiYmJycnKampqqqqrW1tcTExMrKys7OztPT09fX19jY2Ojo6PPz8/r6+hwcHCUlJTQ0NDg4OEFBQU9PT11dXWBgYGZmZm9vb3Jycnp6en19fYCAgIWFhaurq8DAwMjIyM3NzdHR0dTU1ODg4OTk5Onp6fDw8PX19fv7+xgYGB8fHz8/P0VFRVZWVl5eXmpqanR0dImJiaCgoKenp6+vr9/f3+fn5+3t7fHx8QUFBQgICBYWFioqKlVVVWJiYo+Pj5eXl6ioqLa2trm5udbW1vT09C4uLkdHR1FRUVtbW3x8fJmZmcXFxc/Pz42Njb+/v+/v7/j4+EtLS5qamri4uL29vdDQ0N7e3jIyMpOTk6Ojo7GxscLCwisrK1NTU1lZWW5ubkhISAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH/C05FVFNDQVBFMi4wAwEAAAAh/i1NYWRlIGJ5IEtyYXNpbWlyYSBOZWpjaGV2YSAod3d3LmxvYWRpbmZvLm5ldCkAIfkEAAoA/wAsAAAAADAAMAAABv/AnHBILBqPyKRySXyNSC+mdFqEAAARqpaIux0dVwduq2VJLN7iI3ys0cZkosogIJSKODBAXLzJYjJpcTkuCAIBDTRceg5GNDGAcIM5GwKWHkWMkjk2kDI1k0MzCwEBCTBEeg9cM5AzoUQjAwECF5KaQzWQMYKwNhClBStDjEM4fzGKZCxRRioFpRA2OXlsQrqAvUM300gsCgofr0UWhwMjQhgHBxhjfpCgeDMtLtpCOBYG+g4lvS8JAQZoEHKjRg042GZsylHjBYuHMY7gyHBAn4EDE1ZI8tCAhL1tNLoJsQGDxYoVEJHcOPHAooEEGSLmKKjlWIuHKF/ES0IjxAL/lwxCfFRCwwVKlC4UTomxIYFFaVtKomzBi8yKCetMkKnxEIZIMjdKdBi6ZIYyWAthSZGUVu0RGRsyyJ07V0SoGC3yutCrN40KcIADK6hAlgmLE4hNIF58QlmKBYIDV2g75bBixouVydCAAUOGzp87h6AsBQa9vfTy0uuFA86Y1m5jyyaDQwUJ0kpexMC95AWHBw9YkJlBYoSKs1RmhJDgoIGDDIWN1BZBvUSLr0psmKDgoLuDCSZ4G4FhgrqIESZeFMbBAsOD7g0ifJBxT7wkGyxImB+Bgr7EEA8418ADGrhARAodtKCEDNYRQYNt+wl3RAfNOWBBCr3MkMEEFZxg3YwkLXjQQQg7URPDCSNQN8wRMEggwQjICUECBRNQoIIQKYAAQgpCvOABBx2ksNANLpRQQolFuCBTETBYQOMHaYxwwQV2UVMCkPO1MY4WN3wwwQQWNJPDCJ2hI4QMH3TQQXixsVDBlyNIIiUGZuKopgdihmLDBjVisOWYGFxQJ0MhADkCdnGcQCMFHsZyAQZVDhEikCtOIsMFNXKAHZmQ9kFCBxyAEGNUmFYgIREiTDmoEDCICMKfccQAgghpiRDoqtSkcAKsk7RlK51IiAcLCZ2RMJsWRbkw6rHMFhEEACH5BAAKAP8ALAAAAAAwADAAAAf/gDmCg4SFhoeIiYqLhFhRUViMkpOFEwICE5SahDg4hjgSAQJEh16em4ctRklehkQBAaSFXhMPVaiFVwoGPyeFOK+xp4MkOzoCVLiDL7sGEF2cwbKDW0A6Oj0tyoNOBt5PhUQCwoRL1zpI29QO3gxZhNLDLz7XP1rqg1E/3kmDwLDTcBS5tgMcPkG0vCW4MkjaICoBrgmxgcrFO0NWEnib0OofORtDrvGYcqhTIhcOHIjgYgiJtx9RcuBQEiSIEkFPjOnIZMiGFi3DCiVRQFTClFaDsDDg1UQQDhs2kB4x1uPFrC1ZsrL8tCQIUQVBMLgY9uSBFKSGvEABwoSQFy5Z/7NqgVZqygSvRIU0uSeTrqIuSHF00RI3yxa0iLqIePBVwYMoQSX5LKyF4qQsTIR8NYJYEla5XSIzwnHFSBAGtzZ5IcylsyYvJ564lmz5oO3buAttabKEie/fS5bE3LYFi/Hjx7MgtZKyefMhQzCIpvTiipUr2LNjp8vcuXck0ydVt649O90tTIIrUbKEfXsS4T0jn6+ck0x/8XPr34/Dyon8iRimDhZOFFGBC6hwMcUULfhFCRckGFHEBEUwAeAvLUhxwglUYDFbXRgUMeEEGExxYSFaULHhhlUApQgOLSwh4gQTGCECXyYtMowNL6i44hVcTIcDCRXQOEEFTVg1SPAVT0SSyBZVKClIFy1MIYWGUzhpyBM0FpGEFYhxscQRSKTmiTwkiCBFbTJt4d+GCB6CxRFHROGgTFLQiYQ2OVxBAgkM5ZAFFCKIECgnWVBBBZuFvMBXIVkkcQQGIpwiRXBSOFVFoSRsVYgNd0qCwxMYHJHERTlcykSmgkBYaBUnStICEhhgIMUwly7BqiBXFAoFqurY0ASdS3iaam+75mCDFIWe8KEmVJSKQWqD5JpsDi8QCoWUymwxJgZOMGrtL1QUaqc6WShBJreCjItimlEYi4sWUNxqiLu5WCHvNtPhu98iJ/hG0r+MdGFcqAQTHAgAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSDALHjxZGEqcWNCNAQNvKGokGCjQQTYX2Ry84XHjQT4a5JQk2CakwRtu1OQxWXCPAwVlqhQMBNJAm5UCoxAIcEAnTYF+bipYU4NjSwNsgP5pEIAon6MD6yjYeqdgzzYF5QgIIAAO1oF/0mxFI4NgT5ED/YypuqDtWYFSFmyVMzDQ06gCA7kZO8DO3YGA2mw1c1Xg24FVxIxFA8hkH7sF9TTY+uZGDr8XweYAhKaqGCoH96BG2CeNmihNOTLZugCFQCYOHDARaGcAWdEEZ2QYIMCoQTlmcrep4nlgljM4RQQGBKi5Bt9j+hAEVAcBgO9ngAb/pnMmt4MzcLQPtMOmiviBN6KU4RuYSoMv3wF8UdN8ZxU35jkQAR0zCHRDZQvVUFIfaoCRHwBk3PEeQTVEoUaAa+AxYUI3xEHAg2HE8cdEM8yBRm5mZNCfRDWQkR8Ya6inEUoOoKGHSXZ88UUDVGzI0A0oSGgSIG/UseJhG/k4kZJIolUHHXQ8CeWUGmIFyB9YZvlHDVuWpMcaa6ihRphgihkHkwr9kcWabLbZ3B5hihnnmGowgWZCM7SpZxYIzkDHHHP8CeigUpzFpZaIirfSnU026ihHexi30QyxHZVFHW9k4IdJNeyhhx8IalSDFHC8YWodjA7Uhx6s7iEDozdU/8HEG26YGoekE/3hKat68FGgQoHwMYeptGogxYiBaXRDFp7mwSqoCAUiRQbEZiBCRAPtIQW2CP2hB2aj+cErq+ASZAexcuwBVA11MJFuXytlgQIezBX0x6qscltQFnDEQUWoA1HBhLvq8YECCurNMC8Km+40wx57HNnQrwXJMMfAUngUSBUiiGBUIHs8REWl2wG8pBRMxDEHZhx7XFINVOCBgrpN9iHHwJK2LGkfD6FA8Vk32DFwHSTrTNANMeOhR6oJ6THwuwQZ3VDP+tL0Bx0D33Gk1H3p8VAVJm8kA9ZyVJ0DFR3jmoPCUox81x94rFYQx3WonYMffIR91IRcPxHKUB522DGT3xIBsqbehCceEAAh+QQACgD/ACwAAAAAMAAwAAAI/wBzCBxIsKDBgwgTKlxI8BIVSZcYSpxIkNMjBQo4UNxYkNNBRxgfHdzkkeNBLB3qlBzIqRFGRwY5OVpEyWRBS4kcPJjU0aUCmAXxIDCggKdNgVkQOXDgSFNFn0AHdkFjgKilowOhLHUgpaBPkQTrVDUwB+vATIuWrsHE8itBLAyqOmBrViCVpYfqEITK8lHVH13rCtz0aCmiqzlahhy4olBVRU45YqFbsBKapZA8KlYAdtOaqoRWHKwkaWVBLG7c4IlMcI6DQw8kCQSxaI0IgSV+VI06EBOHHz9EHwShqDikSaYvKYIdSSAnkiU76GaAheAmKIYECAigyLRzKGuKK/9aMwfLyhKOkCPcJOWBXueS0AgKEECAIEbenU+CFL44IyiZOLcJQ5oMmAMWjAxCn3YMSGEgQprg0Yh4azQyRX4KceIBIdvVR4gHAUqECRSMiNcBhgl1IUSHgzBSHUeWeLAGTSZFIoggaKyAIkObSCLFjgkRJgJrghVpJEeaJaakaV1EIgIUUD4JhQgiUIFVS4dspaUDaCBWSSNugNnImGG6AQKQCnWBgA5stulmczl8KWaYYjZy5lFquqmnDnA2KSWUU05p5VFY4rVllxkeyUlJSaJ5ZF2cWEKJowcVaBYmUngwRxYmbXLJJZk8SJEmVMzBQQcclEApQZlk4eolXVD/tMkkdXRgqwd11MSRJp++egmRCGURiQeocjCHJLEmtqpzXVziahagiloQFR5wcKoHUkQ0EBZUUFbpZBVh8iy0yRqEx6kdQIHYQJpIIUIk6yopECaUTFKJtJuI62q5BWECAgiTAJsDJYBymkMWK6xgcBf1UqJtRbxesiOoB2XipAilCUQJHnjoeuAk9krr3LIsSUJlJCHGybHHmtQ7yYtFXjKlCB6r3HFDIFPCL1ab4EGlFERujEcl1lUCcrxYWRIo0pWs3C/Ik3hrUxclUHlhZU5XhEW995qVSdWRPDyQ0EQX1AXIlQjMUSYrGFUQ2Qc5KzKho3Fc9qMTNY0H0ngrCrRJJqH2LXhCAQEAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSFBVlTyqGEqcSJBTBwdmPFDcWJDTwVIOHHQ4yMkjx4Op6pwySXBDyFIGvZTS8OJkQRikFFXY0xGkA5gFpxj6ZIaPzYGXcioqxaqiS5EFVyn6ZCgUjKMDTShSNGpKQZ9AB5r6RLYO1oGrNGx1FFEgJ58jB6ZyQFYRjbMDq4zaGokgSDMdTFokC8orXoFePGy1cDUHp6dxc7BoQPZNU46p2hZ8YWHrBy8C4SK2QLYBT4MvWLAsmGpDqRSXB3IytXcUC4GR3rzpm8OEoaEaC9L4QPb2wVO633jYs1rVG50m3HopKbAOqE+hUhFkhcqBge8VVrv/NeEouSNTqVie6MBHvOwqFXg7zqPowHcDCRy5d8znQ/I3GqByl2OgLTSdQKloUMh9BoRyQoEIsVJFB/+Vksd+CXFShyEMGlLHKhPRYIIGydWBIUKriHJfAhpoh5kpjtB0EioHHKCIakd5sceFJ7HSASoQHibkkBx5ZKRjSKJ1gglLMumkCcbZ5MUGolRppZWKNAZDBx2UUkqXXX4ZyYkLsQJKAGimKQCaAqAi0JZfesllmPKdtIoha66ZJptu5rDKFCYw2WSgJ+SB1WNXJpqlQmRuZOSjbhEpqUGcpFJTj2/UEdtJNFRxyimaUWTKF1+YkUKjBrGyRySmtJoCR6t8/wLArAGMcilDXrxgwimtnmLCrRPJ5Mmss3pSyoAIcXLJFLzyGgkLsaFK0AuK8EAsAIVEEiRBe/DaaxXI5pAKC+HGpEq0KTTwBbFfKLKtQFX0ekJ626VwwhQupnpJKpesxkodBxAbyn40oIIKH+++cMK9bV3ywgttsZLKxCAWdIkGnXRSRUI0VCycvSeclgMMeeSRryoTX/JuDnucehILC6fg8bgsNJaDF/umUu5ZqgB6gs0js1AzQaukvPJJXuSxcBWbwsCCyRXtC4Mq0i6UysInXHKT0PkKVPTEm9rEir1Qiud0HkALhDK/VaNYhQlT7Oz00AVJzO/RFK3CR9pvPhndNVo0tG0TyXRPKhHNfxue4Sqr4K244QEBACH5BAAKAP8ALAAAAAAwADAAAAj/AHMIHEiwoMGDCBMqXEhwBgsWNBhKnFjwiRo1pihqLMjpIK2LdA7m6rjxoJYRJkgS/KgmZMFctGZhKVkwy4Y3jnBxZOmS4IpYh2TppClwxs03dDQV/Eihp8BVRxw4UKOF6MAUb7KuIMiJliw1TwqikuqgltWBmjxknRVRYFeQBLXIknpk1dmBlBxlNbHyYtiBtKTGUnF3ICdTR45oyAL4a08XaKRuyFVyRtuaGrI+6fgWrMBcGqRGGFoQF6WEM2jRWUFZbFZHp3OYWLKEb44UQB04FUiDjlQXCG3RnjUCl8ocNJbgJJyDk/OBtWI5oFB1YC4TsgwpULABYQoPS2aF/0dVXaCKJzMRcmLhyJZhFm20bzfk4bhhLLXEi6eVwm5z+yKRlMUSQmyngCEUqAAgQblQ8oR44dFByYIJcTKCAwYqgEYtSkm0Sgq0hDcLKhQilMsi8h3iQXkUzWDCLB4wtpEKZRjyBnBEcWJaiRWacktrhQUpZEmcNefWcwJpsoIKS6rApJMqkEbkLItUaWUbbSxyhIwnmWLKCF6G6aNVmjgAy5kFoHkmLO7l0KWXYIp5C5lmrmnnmW0qCeWTT+JIEydUWiloG1sOuRCSziFp6KKGzSDjRppoMAKQJa1CyS23XEYRKoIIgoaCkGKRgi2ksgCpEAGkWsARUirESRYqkP9KqgosSgQTAq+kGkACHmhqECcOyXpLClgAyeNTrWHRRgG6viKECZQShMUtwlLiH2+4XGtQLiMksIRhKqAhiK6CtLGgC6TessIMxzXIAiUzIPRGKwD44GcOmoxgSK4ByLLgKk5mAaAWD7Hg3yozzODfE/QCoIZ9Rh1wwFYIrdJhQZaysEJ6yGWRRVuaHAIAAGCkcJALzG2ExUOUXEyDx5elAMbIQlx81yoas8Diyx8bpsbIrfx1FycurMCCC5TyrCkuPoyMQK00zWA0RAU52jNBS4wMgCN35eKCxsYVpHTVQIzcQ2xEaULJQ9ryBrNBtbgCwCsmn5VLFlB3fDWDFAwUxihBY297bGGB/31oLiMZrnhBAQEAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSDCTCxeZGEqcWPDOmzd3KGosyOmgnQtv7Bzk1HHjQVW2qJQk+PGCyII3RPxKZbKgql9MmtAsaOeiCIMs2Ci64KfmwEw4mdy5UVDExZcDWUFSNFSV0YEsmGhlQZDTxzc/CdqiusbW1ah2tIqowfIpQVVvqEJidXbgiyZaqbAEKaIkJxFU2QCrO5CTCa1OLg38CvWFBapOVlLMxNbgJSdaTXT06jYHpyZULbw4mMpFwkwlSrhgWpCK1iajc1D59UtvDhVrqEIdWEOEBAlFDwITIcKOrVSSe+cMVnilCaG+rA68QYUNrwa8miBkYYd4cRURBwb/K7FzZDAmtgW60PCA1/UHvyQTvISiO/E7LOh6ln+QdY7LETSA3QNvsMBfVy+Y4J0dJvhxYEKclCCBe+4pYoJ+DLESzB3epTfRDb5gx0sEv0inUSYq2HGHYhux0B4TsdXESSoxahShCv4RpuOOJpHk2Y+S3eBCMEMGY2SR5dUUAkhv+HKRk29owGImKJhggi1YYnklMA8ydAMbCoQp5gJhLmAbSlnacqWatgxm1JdixlmmbUIaeeSdSW70ly++aNCnn3wywSKPhBZaVyYmanQDEyVgaBIrfgTDQmUamaCLLooYuNENqUjKAjDBUVRDLwaUmoAGeUKoigufAsMCRJuG/7BLqaXuEkJ4CdXwAgutBnNJlwfVwJofGiRAqwEPoJAjQanw6ioLqTjKiirLEnTDHbtoJxAnwCiiC60I+HJgs66+UINknFySSrQC3cDKuQJpMEAACdR4gwkN0GrBgaw8pAp/mazLLidvXHqBQHbMK4AFBqniRJhcIcRKtTncoG4q4XHCCwAA8CIQK70EEIAYKhy0K7AIBZzKrwNt3HFJKoghci+OnsXKupdQqjHHHg9kgQABDLDbWar4sfJKO3dMkB8JiLxAokbVILCjSfc8UBNAB8BEXemm4gfUVUuWSQMi68LcVRavvGzYBZVAgAC6lHwWJ5Qd5LLV01kggZuGehZ2d38oE9YLxxH0LdELdthRo+GM5xAQACH5BAAKAP8ALAAAAAAwADAAAAj/AHMIHEiwoMGDCBMqXEiQGAwYxBhKnFgQhTBhKChqLFjsoIklwkwc7LgRYSZgVw7iuSiSowk7l0oWzFRCBEyDJlga5JMBg5IsMgcSMyFCBAqSA3OGLGjjiRufM4IO5GPHJq6CSvEUlISh6zCpA3OhKGrCBsGcS1oKzLSkqxyzYAVeqiqCEkE8ILUmdeMmg924AotJKloi08CVS/TmyKKk6xOkFInBnRmpqCSSaFsWE9E1CVCDl2AkJCZpWBbIAq8UtfP5SqRIKXNQyvBUrVATfD/vxMMb2AzINohGuhoYqaSeSwwPFJxEkfPHB2Gg4I0HBaWIA2FIioqwGIwnkgji/5JTxLmiIpESZroynfcwXLmWM0Q6t4L5IksooeZ4SRJ1FJLEtBEKbtyHwTCTLZQLDMO0d8V+ChUjjHmM2KGcRsRQggIKF1JESQUVOKGbTJmMSFExeAADIWAstjgRSTBCVkwWD2VBIww3cidTMZEoscQSPgL5oxzcEXPFkUgmSdyOGTgwhANQRvkkMAIZmeSVS5ZUDAZRSjnEEKFQmcOMONqIY406yhQJSBe1CRKRLkq0Ypx0DmRDgic+YUJ8QeWSySWX8KmRJAww4IZ+GxVDzCU2ZpGmRLm4ocCkQixhYkLF2DBDo47iOV8koUw6aSgiYJdQLps2egkxJOXiqUE28P95iRxDiBqEIigIWtCiqmYCmTCFiKArQcWYEMoTBFGCQRC2LgFhiTbOMCwuPejQihsCuWoDScL8YAADI4olgahJdDfDJZ4Wo4gO1iKbgxJBBKGEQCV4a0ASqBEjApRZcgQhCjywOwRcRAQQABHZKmKAAQmIWVAWf2lkgxDsBvBVDrkUfDBJVySwsCLDSvVEK+wWAaPGRCCVxMI/lMDiJT+w60OWKBOUBQMLO/CoTBmwq8MSxBb8CsIEPbGwAU7ERckr7BbSYQ4oQ0YMEQsr0O9GwzDdSnpBG0z0WQgYoEBsUkkSiiKeRl1QLhkwQjZYxYRcDBGvHDzSnC0qUrcieNcLmV0JJYjm9+AGBQQAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSBCQlmWAGEqcWHAFFBErKGqUKEmECEkHA21MCEhZn4OSLoI0mOzElpEFa7RE9rJgx48Gl8lZcqwmzByAJJ04sUIkwZsrB3qpxYTnn58Dlw09scymx4wEW8hhwuQK1IGBVpyQIsnLUY9Jc9R4whWK2a8C/yAbenIgUoLJuMqpCzdHoBZDkdUYuALtQC20mpYwqhHQ24KAWp5oYfQm1kBSuNLScnBLVYQllW1hPLDP1JrKkCFTJrDPTibJDEbesIHzwWVXcisbTNCLUGSfDV5J/IS3wL9yMCiHglBL7ucQCTp/mlBLiRYEl4lAohwDEimkCdb/gPH8SotljyUy/iMliRs3ymkpC2/wj7Lyyv7QXyhpSXcMS5Q1USBatLBCbjBsFMgTGMCXhBTUNYZbC8ZR1AcSSIgQHEw1RLiRJFfs19eIJKoH1nGkBfLHiiy2WOFIJdAioxwy1vhETV4so+OOPPo0UiBLKCLkkERil4MXD/HYI1RAEulkEUaq2OKUL2oUyAm0HHNMllweI4KHJYYp5k+AMBiRgrUkk56VyRjzxRcijHTFA7wkwdpGfRQBBgB8klGlQl4kwcugEBxjG0N/LOEDn3x6ssSaC12pCC9mUCpBCX8qVQsZjAIAhiJ1eZFpb0ZtcQwElFbqhiT7eaHIF4x+/2EMMozJYUwJkB4nCRvMlbYEnYM+cAx9gTzAKAJPnNnaGAF0ksRxgABilAigKPDAhr4ZQSkvTOwnSSedIOGjX0YIEIAnzAXCxKBMCITMAgoosER4NZQggQQJIpSMkTYVEEAAEJxphAEGsCGQFxjEawxWBS3DF0WAQPBvAQwPbIARRiljRrxG5AoTFJ0IIIAbRgVisREEyRHvAieMuMUCIo+Rr0AnSwdBvBGACdMS/wogR0E1E1RLvAo8AZcyB/xrjIcmE4yxeGzEy8vMMElygACelFBQ0xeHJ0m1vPD70woSdGxQ0AQFIoedIwaSKxsEG2xQICKWiEEBBmAw5kRSSQex4d6ADxQQACH5BAAKAP8ALAAAAAAwADAAAAj/AHMIHEiwoMGDCBMqXEhwE5ctmxhKnFgQFx48lShqlEjpYkaDxTYm3JQly8FKFymBpGSFi8iCmihdoVTDYEc8KgtqseMMlcuXAjdVunIFV0iCNz8OLIbCWc+aQAVyIXrl58CkBf04taM0ajFcRCtFHIgSJ8Eaz5ziGRtVYA2ZV7Qg9Yh0q8m2BLMQpaSJLF2pkZwOO6qxGGGCMYn6ufq32DCnkawS5CIXYTEtWvoa1LL3p94ri3Nk4eksZ0MrIEBsQcilZJYtmpcOpbRa4GFcgZ/FzvHVTocOHPAgrKHFdRYubHNwwQUV4ZZhuAhuQdWMA/Bmw0ZuMa6lxmGGhGtA/5vDwXqHSFm+G9S03XV3kZSe/Lb+hFJyhcWIu65NsRgq83MM0xxFDmF2n0RZNNPMM/y9tMluGhWlHl4UWmYbb7xN+NKEhOGCBi8ghhhiIwdS9BhPKDpjhx2RCRSJDjDGKCMzAxYGQiMX4Ihjjjl+ZIeMQOpAI1DFgMCjjhfk2MhHHooo4iGNaCgRNE5tpSJkkhmGYYYVdumlSJrYkUSJCxWDBzRkTomGIIJEAt8iozQT3UZ+XDBIAHgKUWOZzUzgZxt2NKgQF80QIgCeAhAyR5oHOdbIKH5O0AgeezaECigCHCrAIG2E9iBDmxzFhR1tRDqKEldweIEgmQYgyAPQEP/2xAPPkFnMFY6gQpAfcywyAaSjONPoBIgaYsdufoACywEd2BbqUZE8wMsEldl2hRKQTgDChFYccAAHguaQBCyDHKBrDs4sssgTAkHzwCGHzPFdDXjkeNdB0HQ1kBWEwALLBGM5ooACUfLGAS+HoKGvQFuEppEmE/hbyBUDCUzwQLhEAOKYXaLCjL9JEJbEwI0Q9ESI2VG4BS/+gnJvDhYXzPAEh/CyiGRAzeEvLOwSNPLFBOGBMC924IWLAv4+gLPFjhymSSMgRvCySFYgfYBwBcX83RXSprHwRlcswnHWJIMEQgcOt6WlQTE3+iVCHAwc8tsTaTHMMNXSrbdBAQEAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSPDGqlWcGEqcWDDLlStZKGqUaPEKlo0bOWXKdBDLFSsfDWJRZgNkwRtasmi5ofJkSoKZUOBRscrlQE4xs5AsaNJjQU5X8OBJ0dKnQBtZovYkWPSmQC1KUWR0KpDTlqhaIg6s2lCFUis0uT6NmmWqQLJjleLZohYn2LQ54OawkUIKnmBiNaYIdhBoVLpvL95UpjSFW4Krhh5U0amTBi0GV7FNu8WSJcRbdOKxZPCGshIlHv8MBaC1rhBNu37VonpgFp0q8ObglAUPFCjOrBy8oehLawBfGqQIbGOLboOZrmAemEkFcGfOoBAeXqvQcQA8FJH/psj8Si3s2FGEVZiplI/vPko9Z2hJCvYQUKRYCrzQkqIAxyVQm0KcqIBeLVfERlEKDXzxhTMgbVELFCpIBpINIbyhIEWWbKUWf3UlxMmIu0VEYogLYaGIKKKsyOKLkICo0RVS1FgjHjbiMZUUAfTo44+gDDhRLaUU2UGRpRzZQUol/OhkAKBsSF4tRxqJZAdLvuUiixO8KAok802ElI1k3uiWiSWSKCOKbLaJ0A0ldBDmQgUC5pQViugSjRQgWaJBBiF4SBEWGiRgQDTRTCMlgRm+8YYGUljIXghBGHBoNEGEMGdCVpTiqKMdqLDoQDfgMQ2iiCaQwU2bkipWJlJo//DpG07YaRAnGegZjQG6KGJFYLVQo8KauwXTAR4EZRFCBqQ4moEUMnLCCKoNlKAbFtOAkmlXuw2EBzWKvDFdV8E0IesbUCCkDBmFOCFpDk2wGwSfOUDxBinp5mAFuIo4AyJfkEAyrkFWKHNQMA2QAQopaXUgjTQx5nCDE4oowojBBn0F0g1vFFJIA1cMVIoZ0pQyFiMVN9GqRiiA4nETgZUijRkmDwRFxWsIV1cmiigciqAdkByxQJlkULEGQmrkjMug5Cvyw0MLlMIaFdPrVBbSeKyIpA6bAUlBNpRSMSmCgqRMKIWAgoJBI5dsUDBrUMOIVS4po0EpMsoMMYicQB7hRNk+nVhQ11/f6uZBTZDcweETbWGFFQMzLvlAAQEAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSLDYjRvFGEqcWPBPqlR/KGpseOOgRYwbN6oINaFjxYsZDWpJZTLkwGQEALiqZfBjSoJd9kyqBMjlwD2CAAAAclPgR0wGYUyatKelTyRCAXA4CZIgJp2TkPocqAWBUB8wCNpsWGmppYhbBz5pJZQC2hxjuS7d0yUtQUDVhAZINjBujhtYw4bMU+lgMh5Ch/SEi3JgqqWTFhe8URfhpB8/OGgdWIyC0FZPBHbBhKnyH8ipDBZLlUyF5IYTAgR4tcDO60oxWzVCiKlsJadw89gaXlh1GwKyAxCAoOItByC2EwKCUbRLpVvDbd2yhPCGiWqvkg//ciOYssYbMJJlv5V1IaZmhMLPJvTh7UQtKtarSGVfIQw3g4T3SjWVTVTMHtklYwlwDBWjAgQECELTRn/ccgtdWwFihwYMSpQKJv25FKJdCkX01ogkGpSKG9RQ04aLL7Y4S4cTWaLCjTjimMdithjg44+D/CjNaxvdIsKRSCJphxYC9fjjkz6GQiRFxSST5JVLCpRKIy3G2KKMNEpkY4457thQDvahmOKabCp0g5FhJnTgWVtV0sgCDKgQkhbNNGPCZhTxWc0nhLYRp2qozMLBLB8kU+BCgNQCAaGESmOHmgjtccwsis7yRFMlqkDBApRWw0FqaGIq0FtdJPNBp7PU/8LfQcU0wwClC7QxCUEmILFrQjA8oedAmJjQzKIcNMOXahpQGoEtr2lBgTShTGjiQCog0QgHRRVjiQiccnALQpVIM8QTRQl0zBDSSDNuDrZwwIEJAu2hbSP0TpbHMccAWtAe3BlkSQTscqguBRN8sKoIjbihAaoVMbnRDRu0C0FxORwzQcJopaKBG26IcChFI7GrsFoTUHCyQCY00ggSe6TYhRvsyiKxuhsfI9YsbjTSzJQh1WKuNKgUdAzCKwukgsuNLLuVFhOY68ajGW+c9F8f9KxZWpbIMkQowxKkMccFWYKEGxvc7BMMsxwT4thXo2lCliQWM6LGKtPaJkIipA8c2t4T/bHHHv4CbjhBAQEAOw==)}.sr-rd-content-center{text-align:center;display:-webkit-box;-webkit-box-align:center;-webkit-box-pack:center;-webkit-box-orient:vertical}.sr-rd-content-center-small{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.sr-rd-content-center-small img{margin:0;padding:0;border:0;box-shadow:none}img.simpread-img-broken{cursor:pointer}.sr-rd-content-nobeautify{margin:0;padding:0;border:0;box-shadow:0 0 0}sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-radius:4px;box-shadow:0 1px 2px 0 rgba(60,64,67,.3),0 2px 6px 2px rgba(60,64,67,.15)}sr-rd-mult:hover{-webkit-transition:all .45s 0ms;transition:all .45s 0ms;box-shadow:1px 1px 8px rgba(0,0,0,.16)}sr-rd-mult sr-rd-mult-content{padding:0 16px;overflow:auto}sr-rd-mult sr-rd-mult-avatar,sr-rd-mult sr-rd-mult-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sr-rd-mult sr-rd-mult-avatar{-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 15px}sr-rd-mult sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}sr-rd-mult sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}sr-rd-mult sr-rd-mult-content img{max-width:80%}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-page{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%}</style>
                        <style type="text/css">sr-rd-theme-github{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:57.6px;font-size:3.6rem;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;line-height:1.225}sr-rd-content h3{font-size:38.4px;font-size:2.4rem;line-height:1.43}sr-rd-content h4{font-size:32px;font-size:2rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}sr-rd-title{font-family:PT Sans,SF UI Display,\.PingFang SC,PingFang SC,Neue Haas Grotesk Text Pro,Arial Nova,Segoe UI,Microsoft YaHei,Microsoft JhengHei,Helvetica Neue,Source Han Sans SC,Noto Sans CJK SC,Source Han Sans CN,Noto Sans SC,Source Han Sans TC,Noto Sans CJK TC,Hiragino Sans GB,sans-serif;font-size:54.4px;font-size:3.4rem;font-weight:700;line-height:1.3}sr-rd-desc{position:relative;margin:0;margin-bottom:30px;padding:25px;padding-left:56px;font-size:28.8px;font-size:1.8rem;color:#777;background-color:rgba(0,0,0,.05);box-sizing:border-box}sr-rd-desc:before{content:"\201C";position:absolute;top:-28px;left:16px;font-size:80px;font-family:Arial;color:rgba(0,0,0,.15)}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content b *,sr-rd-content strong,sr-rd-content strong * sr-rd-content b{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}</style>
                        <style type="text/css"></style>
                        <style type="text/css">@media (pointer:coarse){sr-read{margin:20px 5%!important;min-width:0!important;max-width:90%!important}sr-rd-title{margin-top:0;font-size:2.7rem}sr-rd-content sr-blockquote,sr-rd-desc{margin:10 0!important;padding:0 0 0 10px!important;width:90%;font-size:1.8rem;font-style:normal;line-height:1.7;text-align:justify}sr-rd-content{font-size:1.75rem;font-weight:300}sr-rd-content figure{margin:0;padding:0;text-align:center}sr-rd-content a,sr-rd-content a:link,sr-rd-content li code,sr-rd-content p code{font-size:inherit}sr-rd-footer{margin-top:20px}sr-blockquote,sr-blockquote *{margin:5px!important;padding:5px!important}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6,sr-rd-title{font-family:PingFang SC,Verdana,Helvetica Neue,Microsoft Yahei,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#000;font-weight:100;line-height:1.35}sr-rd-content-h1,sr-rd-content-h2,sr-rd-content-h3,sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;line-height:1.35}sr-rd-content-h1,sr-rd-content h1{font-size:1.8em}sr-rd-content-h2,sr-rd-content h2{font-size:1.6em}sr-rd-content-h3,sr-rd-content h3{font-size:1.4em}sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:1.2em}sr-rd-content-ul,sr-rd-content ul{margin-left:1.3em!important;list-style:disc}sr-rd-content-ol,sr-rd-content ol{list-style:decimal;margin-left:1.9em!important}sr-rd-content-ol ol,sr-rd-content-ol ul,sr-rd-content-ul ol,sr-rd-content-ul ul,sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em!important}sr-rd-content img{margin:0;padding:0;border:0;max-width:100%!important;height:auto;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-rd-mult{min-width:0;background-color:#fff;box-shadow:0 1px 6px rgba(32,33,36,.28);border-radius:8px}sr-rd-mult sr-rd-mult-avatar div{margin:0}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center-small{margin:7px 0!important}sr-rd-mult sr-rd-mult-avatar span{display:block}sr-rd-mult sr-rd-mult-content{padding-left:0}@media only screen and (max-device-width:1024px){.simpread-theme-root,html.simpread-theme-root{font-size:80%!important}sr-rd-mult sr-rd-mult-avatar img{width:50px;height:50px;min-width:50px;min-height:50px}toc-bg toc{width:10px!important}toc-bg:hover toc{width:auto!important}}@media only screen and (max-device-width:414px){.simpread-theme-root,html.simpread-theme-root{font-size:70%!important}sr-rd-mult sr-rd-mult-avatar img{width:30px;height:30px;min-width:30px;min-height:30px}}@media only screen and (max-device-width:320px){.simpread-theme-root,html.simpread-theme-root{font-size:90%!important}sr-rd-content p{margin-bottom:.5em}}}</style>
                        <style type="text/css">undefinedsr-rd-content *, sr-rd-content p, sr-rd-content div {}sr-rd-content pre code, sr-rd-content pre code * {}sr-rd-desc {}sr-rd-content pre {}sr-rd-title {}</style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css">sr-rd-content *, sr-rd-content p, sr-rd-content div {
        font-size: 15px;
    }

    .annote-perview, .annote-perview * {
        color: rgb(85, 85, 85);
        font-weight: 400;
        line-height: 1.8;
    }</style>
                        
                        
                        <script>setTimeout(()=>{const e=location.hash.replace("#id=","");let t,a=!1;const n=t=>{for(let n of t){let t;if((t=e.length>6?n.getAttribute("data-id"):n.getAttribute("data-idx"))==e){n.scrollIntoView({behavior:"smooth",block:"start",inline:"nearest"}),a=!0;break}}};e&&(0==(t=document.getElementsByClassName("sr-unread-card")).length&&(t=document.getElementsByTagName("sr-annote")),n(t),a||n(t=document.getElementsByClassName("sr-annote")))},500);</script>
                        <script>document.addEventListener("DOMContentLoaded",function(){if("localhost"==location.hostname){const t=document.getElementsByTagName("img");for(let o of t){const t=o.src;t.startsWith("http")&&(o.src=location.origin+"/proxy?url="+t)}}},!1);</script>
                        <style>toc a {font-size: inherit!important;font-weight: 300!important;}</style><script>setTimeout(()=>{const max=document.getElementsByTagName('sr-annote-note-tip').length;for(let i=0;i<max;i++){const target=document.getElementsByTagName('sr-annote-note-tip')[i],value=target.dataset.value;value&&(target.innerText=value)}},1000);</script><title>简悦 | 08 | Torchvision（下）：其他有趣的功能</title>
                    </head>
                    <body>
                        <sr-read style='font-family: "LXGW WenKai Screen";'>
                            <sr-rd-title>08 | Torchvision（下）：其他有趣的功能</sr-rd-title>
                    <sr-rd-desc style="margin: 0;padding-top: 0;padding-bottom: 0;font-style: normal;font-size: 18px;">今天这次加餐，我们就一起来看看什么是机器学习，它是怎么分类的，都有哪些常见名词。在补充了这些基础知识之后，我还会和你聊聊模型训练的本质是什么</sr-rd-desc>
                    <sr-rd-content><h1 id="sr-toc-0"> 08 | Torchvision（下）：其他有趣的功能</h1><div><span>方远</span> <span> 2021-10-27</span></div><div><div><div class="sr-rd-content-center"><img class="sr-rd-content-img" src="https://static001.geekbang.org/resource/image/b3/e5/b3884bd8cb001a6de6331f796bb2dde5.jpg"></div><div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADEAAAABCAYAAAB35kaxAAAAAXNSR0IArs4c6QAAAChJREFUGFdjfPfu3X8GKBAUFASz3r9/DxNiQBbDxcamn1yzSLEDZi8A2agny86FR+IAAAAASUVORK5CYII="></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAABCAYAAAA8TpVcAAAAAXNSR0IArs4c6QAAAB1JREFUGFdjfPfu3X9BQUEGEHj//j2YBgFCYtjkAcT9D8ti7hUOAAAAAElFTkSuQmCC"></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD0AAAABCAYAAABt2qY/AAAAAXNSR0IArs4c6QAAACdJREFUGFdjfPfu3X9BQUEGEHj//j2YBgFkMULy2PQQK0YLewiZCQDhUS3LBhDf1QAAAABJRU5ErkJggg=="></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAABCAYAAAA4u0VhAAAAAXNSR0IArs4c6QAAAB1JREFUGFdjfPfu3X9BQUEGEHj//j2YBgFkMULyAF6lD8tbqYWOAAAAAElFTkSuQmCC"></div></div></div><div><div><div></div></div><div><div><div><div><div><div>00:00</div></div><div></div></div></div><a href="javascript:;"> 1.0x <i><span></span></i></a></div><div><span>讲述：方远</span><span>大小：12.75M</span><span> 时长：13:57</span></div></div><audio title="08 | Torchvision（下）：其他有趣的功能" src="https://res001.geekbang.org/media/audio/57/24/570a6e8b9110ab03ff0d4493a3b9bd24/ld/ld.m3u8" __idm_id__="81928"></audio></div><div><div><div><div data-slate-editor="true" data-key="5352" autocorrect="off" spellcheck="false" data-gramm="false"><div data-slate-type="paragraph" data-slate-object="block" data-key="5353"><span data-slate-object="text" data-key="5354"><span data-slate-leaf="true" data-offset-key="5354:0" data-first-offset="true"><span data-slate-string="true">你好，我是方远。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5355"><span data-slate-object="text" data-key="5356"><span data-slate-leaf="true" data-offset-key="5356:0" data-first-offset="true"><span data-slate-string="true">在前面的课程中，我们已经学习了 Torchvision 的数据读取与常用的图像变换方法。其实，Torchvision 除了帮我们封装好了常用的数据集，还为我们提供了深度学习中各种经典的网络结构以及训练好的模型，只要直接将这些经典模型的类实例化出来，就可以进行训练或使用了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5357"><span data-slate-object="text" data-key="5358"><span data-slate-leaf="true" data-offset-key="5358:0" data-first-offset="true"><span data-slate-string="true">我们可以利用这些训练好的模型来实现图片分类、物体检测、视频分类等一系列应用。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5359"><span data-slate-object="text" data-key="5360"><span data-slate-leaf="true" data-offset-key="5360:0" data-first-offset="true"><span data-slate-string="true">今天，我们就来学习一下经典网络模型的实例化与 Torchvision 中其他有趣的功能。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="5361" id="sr-toc-1"><span data-slate-object="text" data-key="5362"><span data-slate-leaf="true" data-offset-key="5362:0" data-first-offset="true"><span data-slate-string="true">常见网络模型</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="5363"><span data-slate-object="text" data-key="5364"><span data-slate-leaf="true" data-offset-key="5364:0" data-first-offset="true"><span data-slate-string="true">Torchvision 中的各种经典网络结构以及训练好的模型，都放在了</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5365"><span data-slate-object="text" data-key="5366"><span data-slate-leaf="true" data-offset-key="5366:0" data-first-offset="true"><span data-slate-string="true"> torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5367"><span data-slate-leaf="true" data-offset-key="5367:0" data-first-offset="true"><span data-slate-string="true"> 模块中，下面我们来看一看</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5368"><span data-slate-object="text" data-key="5369"><span data-slate-leaf="true" data-offset-key="5369:0" data-first-offset="true"><span data-slate-string="true"> torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5370"><span data-slate-leaf="true" data-offset-key="5370:0" data-first-offset="true"><span data-slate-string="true"> 具体为我们提供了什么支持，以及这些功能如何使用。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="5371" id="sr-toc-2"><span data-slate-object="text" data-key="5372"><span data-slate-leaf="true" data-offset-key="5372:0" data-first-offset="true"><span data-slate-string="true">torchvision.models 模块</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="5373"><span data-slate-type="code" data-slate-object="inline" data-key="5374"><span data-slate-object="text" data-key="5375"><span data-slate-leaf="true" data-offset-key="5375:0" data-first-offset="true"><span data-slate-string="true">torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5376"><span data-slate-leaf="true" data-offset-key="5376:0" data-first-offset="true"><span data-slate-string="true"> 模块中包含了常见网络模型结构的定义，这些网络模型可以解决以下四大类问题：图像分类、图像分割、物体检测和视频分类。图像分类、物体检测与图像分割的示意图如下图所示。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="5377"><div class="sr-rd-content-center"><img class="sr-rd-content-img" src="https://static001.geekbang.org/resource/image/42/b3/4211c2d8cd27db3e903e6125122f47b3.jpg?wh=1920x1204"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5378"><span data-slate-object="text" data-key="5379"><span data-slate-leaf="true" data-offset-key="5379:0" data-first-offset="true"><span data-slate-string="true">图像分类，指的是单纯把一张图片判断为某一类，例如将上图左侧第一张判断为 cat。目标检测则是说，首先检测出物体的位置，还要识别出对应物体的类别。如上图中间的那张图，不仅仅要找到猫、鸭子、狗的位置，还有给出给定物体的类别信息。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5380"><span data-slate-object="text" data-key="5381"><span data-slate-leaf="true" data-offset-key="5381:0" data-first-offset="true"><span data-slate-string="true">我们看一下图里最右侧的例子，它表示的是分割。分割即是对图像中每一个像素点进行分类，确定每个点的类别，从而进行区域划分。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5382"><span data-slate-object="text" data-key="5383"><span data-slate-leaf="true" data-offset-key="5383:0" data-first-offset="true"><span data-slate-string="true">在早期的 Torchvision 版本中，</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5384"><span data-slate-object="text" data-key="5385"><span data-slate-leaf="true" data-offset-key="5385:0" data-first-offset="true"><span data-slate-string="true">torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5386"><span data-slate-leaf="true" data-offset-key="5386:0" data-first-offset="true"><span data-slate-string="true"> 模块中只包含了图片分类中的一部分网络，例如 AlexNet、VGG 系列、ResNet 系列、Inception 系列等。这里你先有个印象就行，具体网络特点，我后面会在图像分类中详细讲解。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5387"><span data-slate-object="text" data-key="5388"><span data-slate-leaf="true" data-offset-key="5388:0" data-first-offset="true"><span data-slate-string="true">到了现在，随着深度学习技术的不断发展，人工智能应用更为广泛，</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5389"><span data-slate-object="text" data-key="5390"><span data-slate-leaf="true" data-offset-key="5390:0" data-first-offset="true"><span data-slate-string="true">torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5391"><span data-slate-leaf="true" data-offset-key="5391:0" data-first-offset="true"><span data-slate-string="true"> 模块中所封装的网络模型也在不断丰富。比如在当前版本（v0.10.0）的 Torchvision 中，新增了图像语义分割、物体检测和视频分类的相关网络，并且在图像分类中也新增了 GoogLeNet、ShuffleNet 以及可以使用于移动端的 MobileNet 系列。这些新模型，都能让我们站在巨人的肩膀上看世界。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="5392" id="sr-toc-3"><span data-slate-object="text" data-key="5393"><span data-slate-leaf="true" data-offset-key="5393:0" data-first-offset="true"><span data-slate-string="true">实例化一个 GoogLeNet 网络</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="5394"><span data-slate-object="text" data-key="5395"><span data-slate-leaf="true" data-offset-key="5395:0" data-first-offset="true"><span data-slate-string="true">如果我们直接把一个网络模型的类实例化，就会得到一个网络模型。而这个网络模型的类可以是我们自己定义的结构，也可以是按照经典模型的论文设计出来的结构。其实你自己按照经典模型的论文写一个类，然后实例化一下，这和从 Torchvision 中直接实例化一个网络效果是相同的。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5396"><span data-slate-object="text" data-key="5397"><span data-slate-leaf="true" data-offset-key="5397:0" data-first-offset="true"><span data-slate-string="true">下面我们就以 GoogLeNet 网络为例，来说说如何使用</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5398"><span data-slate-object="text" data-key="5399"><span data-slate-leaf="true" data-offset-key="5399:0" data-first-offset="true"><span data-slate-string="true"> torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5400"><span data-slate-leaf="true" data-offset-key="5400:0" data-first-offset="true"><span data-slate-string="true"> 模块实例化一个网络。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5401"><span data-slate-object="text" data-key="5402"><span data-slate-leaf="true" data-offset-key="5402:0" data-first-offset="true"><span data-slate-string="true">GoogLeNet 是 Google 推出的基于 Inception 模块的深度神经网络模型。你可别小看这个模型，GoogLeNet 获得了 2014 年的 ImageNet 竞赛的冠军，并且相比之前的 AlexNet、VGG 等结构能更高效地利用计算资源。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5403"><span data-slate-object="text" data-key="5404"><span data-slate-leaf="true" data-offset-key="5404:0" data-first-offset="true"><span data-slate-string="true">GoogLeNet 也被称为 Inception V1，在随后的两年中它一直在改进，形成了 Inception V2、Inception V3 等多个版本。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5405"><span data-slate-object="text" data-key="5406"><span data-slate-leaf="true" data-offset-key="5406:0" data-first-offset="true"><span data-slate-string="true">我们可以使用</span></span></span><span data-slate-object="text" data-key="5407"><span data-slate-leaf="true" data-offset-key="5407:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">随机初始化的权重，创建一个 GoogLeNet 模型</span></span></span></span><span data-slate-object="text" data-key="5408"><span data-slate-leaf="true" data-offset-key="5408:0" data-first-offset="true"><span data-slate-string="true">，具体代码如下：</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="5409"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="5410"><span data-slate-object="text" data-key="5411"><span data-slate-leaf="true" data-offset-key="5411:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">import</span></span></span></span><span data-slate-object="text" data-key="5412"><span data-slate-leaf="true" data-offset-key="5412:0" data-first-offset="true"><span data-slate-string="true"> torchvision.models </span></span></span><span data-slate-object="text" data-key="5413"><span data-slate-leaf="true" data-offset-key="5413:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">as</span></span></span></span><span data-slate-object="text" data-key="5414"><span data-slate-leaf="true" data-offset-key="5414:0" data-first-offset="true"><span data-slate-string="true"> models</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5415"><span data-slate-object="text" data-key="5416"><span data-slate-leaf="true" data-offset-key="5416:0" data-first-offset="true"><span data-slate-string="true">googlenet = models.googlenet()</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5417"><span data-slate-object="text" data-key="5418"><span data-slate-leaf="true" data-offset-key="5418:0" data-first-offset="true"><span data-slate-string="true">这时候的 GoogLeNet 模型，相当于只有一个实例化好的网络结构，里面的参数都是随机初始化的，需要经过训练之后才能使用，并不能直接用于预测。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5419"><span data-slate-type="code" data-slate-object="inline" data-key="5420"><span data-slate-object="text" data-key="5421"><span data-slate-leaf="true" data-offset-key="5421:0" data-first-offset="true"><span data-slate-string="true">torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5422"><span data-slate-leaf="true" data-offset-key="5422:0" data-first-offset="true"><span data-slate-string="true"> 模块除了包含了定义好的网络结构，还为我们提供了预训练好的模型，我们可以</span></span></span><span data-slate-object="text" data-key="5423"><span data-slate-leaf="true" data-offset-key="5423:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">直接导入训练好的模型来使用</span></span></span></span><span data-slate-object="text" data-key="5424"><span data-slate-leaf="true" data-offset-key="5424:0" data-first-offset="true"><span data-slate-string="true">。导入预训练好的模型的代码如下：</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="5425"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="5426"><span data-slate-object="text" data-key="5427"><span data-slate-leaf="true" data-offset-key="5427:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">import</span></span></span></span><span data-slate-object="text" data-key="5428"><span data-slate-leaf="true" data-offset-key="5428:0" data-first-offset="true"><span data-slate-string="true"> torchvision.models </span></span></span><span data-slate-object="text" data-key="5429"><span data-slate-leaf="true" data-offset-key="5429:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">as</span></span></span></span><span data-slate-object="text" data-key="5430"><span data-slate-leaf="true" data-offset-key="5430:0" data-first-offset="true"><span data-slate-string="true"> models</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5431"><span data-slate-object="text" data-key="5432"><span data-slate-leaf="true" data-offset-key="5432:0" data-first-offset="true"><span data-slate-string="true">googlenet = models.googlenet(pretrained=</span></span></span><span data-slate-object="text" data-key="5433"><span data-slate-leaf="true" data-offset-key="5433:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">True</span></span></span></span><span data-slate-object="text" data-key="5434"><span data-slate-leaf="true" data-offset-key="5434:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5435"><span data-slate-object="text" data-key="5436"><span data-slate-leaf="true" data-offset-key="5436:0" data-first-offset="true"><span data-slate-string="true">可以看出，我们只是在实例化的时候，引入了一个参数 “pretrained=True”，即可获得预训练好的模型，因为所有的工作</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5437"><span data-slate-object="text" data-key="5438"><span data-slate-leaf="true" data-offset-key="5438:0" data-first-offset="true"><span data-slate-string="true"> torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5439"><span data-slate-leaf="true" data-offset-key="5439:0" data-first-offset="true"><span data-slate-string="true"> 模块都已经帮我们封装好了，用起来很方便。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5440"><span data-slate-type="code" data-slate-object="inline" data-key="5441"><span data-slate-object="text" data-key="5442"><span data-slate-leaf="true" data-offset-key="5442:0" data-first-offset="true"><span data-slate-string="true">torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5443"><span data-slate-leaf="true" data-offset-key="5443:0" data-first-offset="true"><span data-slate-string="true"> 模块中所有预训练好的模型，都是在 ImageNet 数据集上训练的，它们都是由 PyTorch 的</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5444"><span data-slate-object="text" data-key="5445"><span data-slate-leaf="true" data-offset-key="5445:0" data-first-offset="true"><span data-slate-string="true"> torch.utils.model_zoo</span></span></span></span><span data-slate-object="text" data-key="5446"><span data-slate-leaf="true" data-offset-key="5446:0" data-first-offset="true"><span data-slate-string="true"> 模块所提供的，并且我们可以通过参数 &nbsp;pretrained=True&nbsp; 来构造这些预训练模型。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5447"><span data-slate-object="text" data-key="5448"><span data-slate-leaf="true" data-offset-key="5448:0" data-first-offset="true"><span data-slate-string="true">如果之前没有加载过带预训练参数的网络，在实例化一个预训练好的模型时，模型的参数会被下载至缓存目录中，下载一次后不需要重复下载。这个缓存目录可以通过环境变量 TORCH_MODEL_ZOO 来指定。当然，你也可以把自己下载好的模型，然后复制到指定路径中。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5449"><span data-slate-object="text" data-key="5450"><span data-slate-leaf="true" data-offset-key="5450:0" data-first-offset="true"><span data-slate-string="true">下图是运行了上述实例化代码的结果，可以看到，GoogLeNet 的模型参数被下载到了缓存目录 /root/.cache/torch 下面。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="5451"><div class="sr-rd-content-center"><img class="sr-rd-content-img" src="https://static001.geekbang.org/resource/image/16/49/16975c6f4071ee1dacc9a41a28f93c49.png?wh=1920x270"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5452"><span data-slate-type="code" data-slate-object="inline" data-key="5453"><span data-slate-object="text" data-key="5454"><span data-slate-leaf="true" data-offset-key="5454:0" data-first-offset="true"><span data-slate-string="true">torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5455"><span data-slate-leaf="true" data-offset-key="5455:0" data-first-offset="true"><span data-slate-string="true"> 模块也包含了 Inception V3 和其他常见的网络结构，在实例化时，只需要修改网络的类名，即可做到举一反三。</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5456"><span data-slate-object="text" data-key="5457"><span data-slate-leaf="true" data-offset-key="5457:0" data-first-offset="true"><span data-slate-string="true">torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5458"><span data-slate-leaf="true" data-offset-key="5458:0" data-first-offset="true"><span data-slate-string="true"> 模块中可实例化的全部模型详见这个</span></span></span><a data-slate-type="link" data-slate-object="inline" data-key="5459"><span data-slate-object="text" data-key="5460"><span data-slate-leaf="true" data-offset-key="5460:0" data-first-offset="true"><span data-slate-string="true">网页</span></span></span></a><span data-slate-object="text" data-key="5461"><span data-slate-leaf="true" data-offset-key="5461:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="5462" id="sr-toc-4"><span data-slate-object="text" data-key="5463"><span data-slate-leaf="true" data-offset-key="5463:0" data-first-offset="true"><span data-slate-string="true">模型微调</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="5464"><span data-slate-object="text" data-key="5465"><span data-slate-leaf="true" data-offset-key="5465:0" data-first-offset="true"><span data-slate-string="true">完成了刚才的工作，你可能会疑惑，实例化了带预训练参数的网络有什么用呢？其实它除了可以直接用来做预测使用，还可以基于它做网络模型的微调，也就是 “fine-tuning”。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5466"><span data-slate-object="text" data-key="5467"><span data-slate-leaf="true" data-offset-key="5467:0" data-first-offset="true"><span data-slate-string="true">那什么是 “fine-tuning” 呢？</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5468"><span data-slate-object="text" data-key="5469"><span data-slate-leaf="true" data-offset-key="5469:0" data-first-offset="true"><span data-slate-string="true">举个例子，假设你的老板给布置了一个有关于图片分类的任务，数据集是关于狗狗的图片，让你区分图片中狗的种类，例如金毛、柯基、边牧等等。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5470"><span data-slate-object="text" data-key="5471"><span data-slate-leaf="true" data-offset-key="5471:0" data-first-offset="true"><span data-slate-string="true">问题是数据集中狗的类别很多，但数据却不多。你发现从零开始训练一个图片分类模型，但这样模型效果很差，并且很容易过拟合。这种问题该如何解决呢？于是你想到了使用迁移学习，可以用已经在 ImageNet 数据集上训练好的模型来达成你的目的。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5472"><span data-slate-object="text" data-key="5473"><span data-slate-leaf="true" data-offset-key="5473:0" data-first-offset="true"><span data-slate-string="true">例如上面我们已经实例化的 GoogLeNet 模型，只需要使用我们自己的数据集，重新训练网络最后的分类层，即可得到区分狗种类的图片分类模型。这就是所谓的 “fine-tuning” 方法。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5474"><span data-slate-object="text" data-key="5475"><span data-slate-leaf="true" data-offset-key="5475:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">模型微调</span></span></span></span><span data-slate-object="text" data-key="5476"><span data-slate-leaf="true" data-offset-key="5476:0" data-first-offset="true"><span data-slate-string="true">，简单来说就是先在一个比较通用、宽泛的数据集上进行大量训练得出了一套参数，然后再使用这套预训练好的网络和参数，在自己的任务和数据集上进行训练。使用经过预训练的模型，要比使用随机初始化的模型训练</span></span></span><span data-slate-object="text" data-key="5477"><span data-slate-leaf="true" data-offset-key="5477:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">效果更好</span></span></span></span><span data-slate-object="text" data-key="5478"><span data-slate-leaf="true" data-offset-key="5478:0" data-first-offset="true"><span data-slate-string="true">，</span></span></span><span data-slate-object="text" data-key="5479"><span data-slate-leaf="true" data-offset-key="5479:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">更容易收敛</span></span></span></span><span data-slate-object="text" data-key="5480"><span data-slate-leaf="true" data-offset-key="5480:0" data-first-offset="true"><span data-slate-string="true">，并且</span></span></span><span data-slate-object="text" data-key="5481"><span data-slate-leaf="true" data-offset-key="5481:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">训练速度更快</span></span></span></span><span data-slate-object="text" data-key="5482"><span data-slate-leaf="true" data-offset-key="5482:0" data-first-offset="true"><span data-slate-string="true">，在小数据集上也能取得比较理想的效果。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5483"><span data-slate-object="text" data-key="5484"><span data-slate-leaf="true" data-offset-key="5484:0" data-first-offset="true"><span data-slate-string="true">那新的问题又来了，为什么模型微调如此有效呢？因为我们相信同样是处理图片分类任务的两个模型，网络的参数也具有某种相似性。因此，把一个已经训练得很好的模型参数迁移到另一个模型上，同样有效。即使两个模型的工作不完全相同，我们也可以在这套预训练参数的基础上，经过微调性质的训练，同样能取得不错的效果。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5485"><span data-slate-object="text" data-key="5486"><span data-slate-leaf="true" data-offset-key="5486:0" data-first-offset="true"><span data-slate-string="true">ImageNet 数据集共有 1000 个类别，而狗的种类远远达不到 1000 类。因此，加载了预训练好的模型之后，还需要根据你的具体问题对模型或数据进行一些调整，通常来说是调整输出类别的数量。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5487"><span data-slate-object="text" data-key="5488"><span data-slate-leaf="true" data-offset-key="5488:0" data-first-offset="true"><span data-slate-string="true">假设狗的种类一共为 10 类，那么我们自然需要将 GoogLeNet 模型的输出分类数也调整为 10。对预训练模型进行调整对代码如下：</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="5489"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div><div data-code-line-number="6"></div><div data-code-line-number="7"></div><div data-code-line-number="8"></div><div data-code-line-number="9"></div><div data-code-line-number="10"></div><div data-code-line-number="11"></div><div data-code-line-number="12"></div><div data-code-line-number="13"></div><div data-code-line-number="14"></div><div data-code-line-number="15"></div><div data-code-line-number="16"></div><div data-code-line-number="17"></div><div data-code-line-number="18"></div><div data-code-line-number="19"></div><div data-code-line-number="20"></div><div data-code-line-number="21"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="5490"><span data-slate-object="text" data-key="5491"><span data-slate-leaf="true" data-offset-key="5491:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">import</span></span></span></span><span data-slate-object="text" data-key="5492"><span data-slate-leaf="true" data-offset-key="5492:0" data-first-offset="true"><span data-slate-string="true"> torch</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5493"><span data-slate-object="text" data-key="5494"><span data-slate-leaf="true" data-offset-key="5494:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">import</span></span></span></span><span data-slate-object="text" data-key="5495"><span data-slate-leaf="true" data-offset-key="5495:0" data-first-offset="true"><span data-slate-string="true"> torchvision.models </span></span></span><span data-slate-object="text" data-key="5496"><span data-slate-leaf="true" data-offset-key="5496:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">as</span></span></span></span><span data-slate-object="text" data-key="5497"><span data-slate-leaf="true" data-offset-key="5497:0" data-first-offset="true"><span data-slate-string="true"> models</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5498"></div><div data-slate-type="code-line" data-slate-object="block" data-key="5499"><span data-slate-object="text" data-key="5500"><span data-slate-leaf="true" data-offset-key="5500:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 加载预训练模型</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5501"><span data-slate-object="text" data-key="5502"><span data-slate-leaf="true" data-offset-key="5502:0" data-first-offset="true"><span data-slate-string="true">googlenet = models.googlenet(pretrained=</span></span></span><span data-slate-object="text" data-key="5503"><span data-slate-leaf="true" data-offset-key="5503:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">True</span></span></span></span><span data-slate-object="text" data-key="5504"><span data-slate-leaf="true" data-offset-key="5504:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5505"></div><div data-slate-type="code-line" data-slate-object="block" data-key="5506"><span data-slate-object="text" data-key="5507"><span data-slate-leaf="true" data-offset-key="5507:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 提取分类层的输入参数</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5508"><span data-slate-object="text" data-key="5509"><span data-slate-leaf="true" data-offset-key="5509:0" data-first-offset="true"><span data-slate-string="true">fc_in_features = googlenet.fc.in_features</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5510"><span data-slate-object="text" data-key="5511"><span data-slate-leaf="true" data-offset-key="5511:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">print</span></span></span></span><span data-slate-object="text" data-key="5512"><span data-slate-leaf="true" data-offset-key="5512:0" data-first-offset="true"><span data-slate-string="true">(</span></span></span><span data-slate-object="text" data-key="5513"><span data-slate-leaf="true" data-offset-key="5513:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"fc_in_features:"</span></span></span></span><span data-slate-object="text" data-key="5514"><span data-slate-leaf="true" data-offset-key="5514:0" data-first-offset="true"><span data-slate-string="true">, fc_in_features)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5515"></div><div data-slate-type="code-line" data-slate-object="block" data-key="5516"><span data-slate-object="text" data-key="5517"><span data-slate-leaf="true" data-offset-key="5517:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 查看分类层的输出参数</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5518"><span data-slate-object="text" data-key="5519"><span data-slate-leaf="true" data-offset-key="5519:0" data-first-offset="true"><span data-slate-string="true">fc_out_features = googlenet.fc.out_features</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5520"><span data-slate-object="text" data-key="5521"><span data-slate-leaf="true" data-offset-key="5521:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">print</span></span></span></span><span data-slate-object="text" data-key="5522"><span data-slate-leaf="true" data-offset-key="5522:0" data-first-offset="true"><span data-slate-string="true">(</span></span></span><span data-slate-object="text" data-key="5523"><span data-slate-leaf="true" data-offset-key="5523:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">"fc_out_features:"</span></span></span></span><span data-slate-object="text" data-key="5524"><span data-slate-leaf="true" data-offset-key="5524:0" data-first-offset="true"><span data-slate-string="true">, fc_out_features)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5525"></div><div data-slate-type="code-line" data-slate-object="block" data-key="5526"><span data-slate-object="text" data-key="5527"><span data-slate-leaf="true" data-offset-key="5527:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 修改预训练模型的输出分类数 (在图像分类原理中会具体介绍 torch.nn.Linear)</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5528"><span data-slate-object="text" data-key="5529"><span data-slate-leaf="true" data-offset-key="5529:0" data-first-offset="true"><span data-slate-string="true">googlenet.fc = torch.nn.Linear(fc_in_features, </span></span></span><span data-slate-object="text" data-key="5530"><span data-slate-leaf="true" data-offset-key="5530:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">10</span></span></span></span><span data-slate-object="text" data-key="5531"><span data-slate-leaf="true" data-offset-key="5531:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5532"><span data-slate-object="text" data-key="5533"><span data-slate-leaf="true" data-offset-key="5533:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'''</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5534"><span data-slate-object="text" data-key="5535"><span data-slate-leaf="true" data-offset-key="5535:0" data-first-offset="true"><span data-slate-string="true">输出：</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5536"><span data-slate-object="text" data-key="5537"><span data-slate-leaf="true" data-offset-key="5537:0" data-first-offset="true"><span data-slate-string="true">fc_in_features: 1024</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5538"><span data-slate-object="text" data-key="5539"><span data-slate-leaf="true" data-offset-key="5539:0" data-first-offset="true"><span data-slate-string="true">fc_out_features: 1000</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5540"><span data-slate-object="text" data-key="5541"><span data-slate-leaf="true" data-offset-key="5541:0" data-first-offset="true"><span data-slate-string="true">'''</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5542"><span data-slate-object="text" data-key="5543"><span data-slate-leaf="true" data-offset-key="5543:0" data-first-offset="true"><span data-slate-string="true">首先，你需要加载预训练模型，然后提取预训练模型的分类层固定参数，最后修改预训练模型的输出分类数为 10。根据输出结果，我们可以看到预训练模型的原始输出分类数是 1000。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="5544" id="sr-toc-5"><span data-slate-object="text" data-key="5545"><span data-slate-leaf="true" data-offset-key="5545:0" data-first-offset="true"><span data-slate-string="true">其他常用函数</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="5546"><span data-slate-object="text" data-key="5547"><span data-slate-leaf="true" data-offset-key="5547:0" data-first-offset="true"><span data-slate-string="true">之前在</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5548"><span data-slate-object="text" data-key="5549"><span data-slate-leaf="true" data-offset-key="5549:0" data-first-offset="true"><span data-slate-string="true"> torchvision.transforms</span></span></span></span><span data-slate-object="text" data-key="5550"><span data-slate-leaf="true" data-offset-key="5550:0" data-first-offset="true"><span data-slate-string="true"> 中，我们学习了很多有关于图像处理的函数，Torchvision 还提供了几个常用的函数，make_grid 和 save_img，让我们依次来看一看它们又能实现哪些有趣的功能。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="5551" id="sr-toc-6"><span data-slate-object="text" data-key="5552"><span data-slate-leaf="true" data-offset-key="5552:0" data-first-offset="true"><span data-slate-string="true">make_grid</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="5553"><span data-slate-object="text" data-key="5554"><span data-slate-leaf="true" data-offset-key="5554:0" data-first-offset="true"><span data-slate-string="true">make_grid 的作用是将若干幅图像拼成在一个网格中，它的定义如下。</span></span></span></div><div data-slate-type="pre" data-slate-object="block" data-key="5555"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="5556"><span data-slate-object="text" data-key="5557"><span data-slate-leaf="true" data-offset-key="5557:0" data-first-offset="true"><span data-slate-string="true">torchvision.utils.make_grid(tensor, nrow=8, padding=2) </span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5558"><span data-slate-object="text" data-key="5559"><span data-slate-leaf="true" data-offset-key="5559:0" data-first-offset="true"><span data-slate-string="true">定义中对应的几个参数含义如下：</span></span></span></div><div data-slate-type="list" data-slate-object="block" data-key="5560"><div data-slate-type="list-line" data-slate-object="block" data-key="5561"><span data-slate-object="text" data-key="5562"><span data-slate-leaf="true" data-offset-key="5562:0" data-first-offset="true"><span data-slate-string="true">tensor：类型是 Tensor 或列表，如果输入类型是 Tensor，其形状应是 (B x C x H x W)；如果输入类型是列表，列表中元素应为相同大小的图片。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="5563"><span data-slate-object="text" data-key="5564"><span data-slate-leaf="true" data-offset-key="5564:0" data-first-offset="true"><span data-slate-string="true">nrow：表示一行放入的图片数量，默认为 8。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="5565"><span data-slate-object="text" data-key="5566"><span data-slate-leaf="true" data-offset-key="5566:0" data-first-offset="true"><span data-slate-string="true">padding：子图像与子图像之间的边框宽度，默认为 2 像素。</span></span></span></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5567"><span data-slate-object="text" data-key="5568"><span data-slate-leaf="true" data-offset-key="5568:0" data-first-offset="true"><span data-slate-string="true">make_grid 函数主要用于展示数据集或模型输出的图像结果。我们以 MNIST 数据集为例，整合之前学习过的读取数据集以及图像变换的内容，来看一看 make_grid 函数的效果。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5569"><span data-slate-object="text" data-key="5570"><span data-slate-leaf="true" data-offset-key="5570:0" data-first-offset="true"><span data-slate-string="true">下面的程序利用 make_grid 函数，展示了 MNIST 的测试集中的 32 张图片。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="5571"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div><div data-code-line-number="6"></div><div data-code-line-number="7"></div><div data-code-line-number="8"></div><div data-code-line-number="9"></div><div data-code-line-number="10"></div><div data-code-line-number="11"></div><div data-code-line-number="12"></div><div data-code-line-number="13"></div><div data-code-line-number="14"></div><div data-code-line-number="15"></div><div data-code-line-number="16"></div><div data-code-line-number="17"></div><div data-code-line-number="18"></div><div data-code-line-number="19"></div><div data-code-line-number="20"></div><div data-code-line-number="21"></div><div data-code-line-number="22"></div><div data-code-line-number="23"></div><div data-code-line-number="24"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="5572"><span data-slate-object="text" data-key="5573"><span data-slate-leaf="true" data-offset-key="5573:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">import</span></span></span></span><span data-slate-object="text" data-key="5574"><span data-slate-leaf="true" data-offset-key="5574:0" data-first-offset="true"><span data-slate-string="true"> torchvision</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5575"><span data-slate-object="text" data-key="5576"><span data-slate-leaf="true" data-offset-key="5576:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">from</span></span></span></span><span data-slate-object="text" data-key="5577"><span data-slate-leaf="true" data-offset-key="5577:0" data-first-offset="true"><span data-slate-string="true"> torchvision </span></span></span><span data-slate-object="text" data-key="5578"><span data-slate-leaf="true" data-offset-key="5578:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">import</span></span></span></span><span data-slate-object="text" data-key="5579"><span data-slate-leaf="true" data-offset-key="5579:0" data-first-offset="true"><span data-slate-string="true"> datasets</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5580"><span data-slate-object="text" data-key="5581"><span data-slate-leaf="true" data-offset-key="5581:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">from</span></span></span></span><span data-slate-object="text" data-key="5582"><span data-slate-leaf="true" data-offset-key="5582:0" data-first-offset="true"><span data-slate-string="true"> torchvision </span></span></span><span data-slate-object="text" data-key="5583"><span data-slate-leaf="true" data-offset-key="5583:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">import</span></span></span></span><span data-slate-object="text" data-key="5584"><span data-slate-leaf="true" data-offset-key="5584:0" data-first-offset="true"><span data-slate-string="true"> transforms</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5585"><span data-slate-object="text" data-key="5586"><span data-slate-leaf="true" data-offset-key="5586:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">from</span></span></span></span><span data-slate-object="text" data-key="5587"><span data-slate-leaf="true" data-offset-key="5587:0" data-first-offset="true"><span data-slate-string="true"> torch.utils.data </span></span></span><span data-slate-object="text" data-key="5588"><span data-slate-leaf="true" data-offset-key="5588:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">import</span></span></span></span><span data-slate-object="text" data-key="5589"><span data-slate-leaf="true" data-offset-key="5589:0" data-first-offset="true"><span data-slate-string="true"> DataLoader</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5590"></div><div data-slate-type="code-line" data-slate-object="block" data-key="5591"><span data-slate-object="text" data-key="5592"><span data-slate-leaf="true" data-offset-key="5592:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 加载 MNIST 数据集</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5593"><span data-slate-object="text" data-key="5594"><span data-slate-leaf="true" data-offset-key="5594:0" data-first-offset="true"><span data-slate-string="true">mnist_dataset = datasets.MNIST(root=</span></span></span><span data-slate-object="text" data-key="5595"><span data-slate-leaf="true" data-offset-key="5595:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'./data'</span></span></span></span><span data-slate-object="text" data-key="5596"><span data-slate-leaf="true" data-offset-key="5596:0" data-first-offset="true"><span data-slate-string="true">,</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5597"><span data-slate-object="text" data-key="5598"><span data-slate-leaf="true" data-offset-key="5598:0" data-first-offset="true"><span data-slate-string="true">                               train=</span></span></span><span data-slate-object="text" data-key="5599"><span data-slate-leaf="true" data-offset-key="5599:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">False</span></span></span></span><span data-slate-object="text" data-key="5600"><span data-slate-leaf="true" data-offset-key="5600:0" data-first-offset="true"><span data-slate-string="true">,</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5601"><span data-slate-object="text" data-key="5602"><span data-slate-leaf="true" data-offset-key="5602:0" data-first-offset="true"><span data-slate-string="true">                               transform=transforms.ToTensor(),</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5603"><span data-slate-object="text" data-key="5604"><span data-slate-leaf="true" data-offset-key="5604:0" data-first-offset="true"><span data-slate-string="true">                               target_transform=</span></span></span><span data-slate-object="text" data-key="5605"><span data-slate-leaf="true" data-offset-key="5605:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">None</span></span></span></span><span data-slate-object="text" data-key="5606"><span data-slate-leaf="true" data-offset-key="5606:0" data-first-offset="true"><span data-slate-string="true">,</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5607"><span data-slate-object="text" data-key="5608"><span data-slate-leaf="true" data-offset-key="5608:0" data-first-offset="true"><span data-slate-string="true">                               download=</span></span></span><span data-slate-object="text" data-key="5609"><span data-slate-leaf="true" data-offset-key="5609:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">True</span></span></span></span><span data-slate-object="text" data-key="5610"><span data-slate-leaf="true" data-offset-key="5610:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5611"><span data-slate-object="text" data-key="5612"><span data-slate-leaf="true" data-offset-key="5612:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 取 32 张图片的 tensor</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5613"><span data-slate-object="text" data-key="5614"><span data-slate-leaf="true" data-offset-key="5614:0" data-first-offset="true"><span data-slate-string="true">tensor_dataloader = DataLoader(dataset=mnist_dataset,</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5615"><span data-slate-object="text" data-key="5616"><span data-slate-leaf="true" data-offset-key="5616:0" data-first-offset="true"><span data-slate-string="true">                               batch_size=</span></span></span><span data-slate-object="text" data-key="5617"><span data-slate-leaf="true" data-offset-key="5617:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">32</span></span></span></span><span data-slate-object="text" data-key="5618"><span data-slate-leaf="true" data-offset-key="5618:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5619"><span data-slate-object="text" data-key="5620"><span data-slate-leaf="true" data-offset-key="5620:0" data-first-offset="true"><span data-slate-string="true">data_iter = </span></span></span><span data-slate-object="text" data-key="5621"><span data-slate-leaf="true" data-offset-key="5621:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">iter</span></span></span></span><span data-slate-object="text" data-key="5622"><span data-slate-leaf="true" data-offset-key="5622:0" data-first-offset="true"><span data-slate-string="true">(tensor_dataloader)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5623"><span data-slate-object="text" data-key="5624"><span data-slate-leaf="true" data-offset-key="5624:0" data-first-offset="true"><span data-slate-string="true">img_tensor, label_tensor = data_iter.</span></span></span><span data-slate-object="text" data-key="5625"><span data-slate-leaf="true" data-offset-key="5625:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">next</span></span></span></span><span data-slate-object="text" data-key="5626"><span data-slate-leaf="true" data-offset-key="5626:0" data-first-offset="true"><span data-slate-string="true">()</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5627"><span data-slate-object="text" data-key="5628"><span data-slate-leaf="true" data-offset-key="5628:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">print</span></span></span></span><span data-slate-object="text" data-key="5629"><span data-slate-leaf="true" data-offset-key="5629:0" data-first-offset="true"><span data-slate-string="true">(img_tensor.shape)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5630"><span data-slate-object="text" data-key="5631"><span data-slate-leaf="true" data-offset-key="5631:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'''</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5632"><span data-slate-object="text" data-key="5633"><span data-slate-leaf="true" data-offset-key="5633:0" data-first-offset="true"><span data-slate-string="true">输出：torch.Size ([32, 1, 28, 28])</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5634"><span data-slate-object="text" data-key="5635"><span data-slate-leaf="true" data-offset-key="5635:0" data-first-offset="true"><span data-slate-string="true">'''</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5636"><span data-slate-object="text" data-key="5637"><span data-slate-leaf="true" data-offset-key="5637:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 将 32 张图片拼接在一个网格中</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5638"><span data-slate-object="text" data-key="5639"><span data-slate-leaf="true" data-offset-key="5639:0" data-first-offset="true"><span data-slate-string="true">grid_tensor = torchvision.utils.make_grid(img_tensor, nrow=</span></span></span><span data-slate-object="text" data-key="5640"><span data-slate-leaf="true" data-offset-key="5640:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">8</span></span></span></span><span data-slate-object="text" data-key="5641"><span data-slate-leaf="true" data-offset-key="5641:0" data-first-offset="true"><span data-slate-string="true">, padding=</span></span></span><span data-slate-object="text" data-key="5642"><span data-slate-leaf="true" data-offset-key="5642:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">2</span></span></span></span><span data-slate-object="text" data-key="5643"><span data-slate-leaf="true" data-offset-key="5643:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5644"><span data-slate-object="text" data-key="5645"><span data-slate-leaf="true" data-offset-key="5645:0" data-first-offset="true"><span data-slate-string="true">grid_img = transforms.ToPILImage()(grid_tensor)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5646"><span data-slate-object="text" data-key="5647"><span data-slate-leaf="true" data-offset-key="5647:0" data-first-offset="true"><span data-slate-string="true">display(grid_img)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5648"><span data-slate-object="text" data-key="5649"><span data-slate-leaf="true" data-offset-key="5649:0" data-first-offset="true"><span data-slate-string="true">结合代码我们可以看到，程序首先利用</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5650"><span data-slate-object="text" data-key="5651"><span data-slate-leaf="true" data-offset-key="5651:0" data-first-offset="true"><span data-slate-string="true"> torchvision.datasets</span></span></span></span><span data-slate-object="text" data-key="5652"><span data-slate-leaf="true" data-offset-key="5652:0" data-first-offset="true"><span data-slate-string="true"> 加载 MNIST 的测试集，然后利用 DataLoader 类的迭代器一次获取到 32 张图片的 Tensor，最后利用 make_grid 函数将 32 张图片拼接在了一幅图片中。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5653"><span data-slate-object="text" data-key="5654"><span data-slate-leaf="true" data-offset-key="5654:0" data-first-offset="true"><span data-slate-string="true">MNIST 的测试集中的 32 张图片，如下图所示，这里我要特别说明一下，因为 MNIST 的尺寸为 28x28，所以测试集里的手写数字图片像素都比较低，但这并不影响咱们动手实践。你可以参照我给到的示范，自己动手试试看。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="5655"><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/bb/yy/bb73d6bdf49fc876d983cfa48569dcyy.png?wh=242x122"></div></div><h3 data-slate-type="heading" data-slate-object="block" data-key="5656" id="sr-toc-7"><span data-slate-object="text" data-key="5657"><span data-slate-leaf="true" data-offset-key="5657:0" data-first-offset="true"><span data-slate-string="true">save_img</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="5658"><span data-slate-object="text" data-key="5659"><span data-slate-leaf="true" data-offset-key="5659:0" data-first-offset="true"><span data-slate-string="true">一般来说，在保存模型输出的图片时，需要将 Tensor 类型的数据转化为图片类型才能进行保存，过程比较繁琐。Torchvision 提供了 save_image 函数，能够直接将 Tensor 保存为图片，即使 Tensor 数据在 CUDA 上，也会自动移到 CPU 中进行保存。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5660"><span data-slate-object="text" data-key="5661"><span data-slate-leaf="true" data-offset-key="5661:0" data-first-offset="true"><span data-slate-string="true">save_image 函数的定义如下。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="5662"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="5663"><span data-slate-object="text" data-key="5664"><span data-slate-leaf="true" data-offset-key="5664:0" data-first-offset="true"><span data-slate-string="true">torchvision.utils.save_image(tensor, fp, **kwargs)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5665"><span data-slate-object="text" data-key="5666"><span data-slate-leaf="true" data-offset-key="5666:0" data-first-offset="true"><span data-slate-string="true">这些参数也很好理解：</span></span></span></div><div data-slate-type="list" data-slate-object="block" data-key="5667"><div data-slate-type="list-line" data-slate-object="block" data-key="5668"><span data-slate-object="text" data-key="5669"><span data-slate-leaf="true" data-offset-key="5669:0" data-first-offset="true"><span data-slate-string="true">tensor：类型是 Tensor 或列表，如果输入类型是 Tensor，直接将 Tensor 保存；如果输入类型是列表，则先调用 make_grid 函数生成一张图片的 Tensor，然后再保存。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="5670"><span data-slate-object="text" data-key="5671"><span data-slate-leaf="true" data-offset-key="5671:0" data-first-offset="true"><span data-slate-string="true">fp：保存图片的文件名；</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="5672"><span data-slate-object="text" data-key="5673"><span data-slate-leaf="true" data-offset-key="5673:0" data-first-offset="true"><span data-slate-string="true">**kwargs：make_grid 函数中的参数，前面已经讲过了。</span></span></span></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5674"><span data-slate-object="text" data-key="5675"><span data-slate-leaf="true" data-offset-key="5675:0" data-first-offset="true"><span data-slate-string="true">我们接着上面的小例子，将 32 张图片的拼接图直接保存，代码如下。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="5676"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="5677"><span data-slate-object="text" data-key="5678"><span data-slate-leaf="true" data-offset-key="5678:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 输入为一张图片的 tensor 直接保存</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5679"><span data-slate-object="text" data-key="5680"><span data-slate-leaf="true" data-offset-key="5680:0" data-first-offset="true"><span data-slate-string="true">torchvision.utils.save_image(grid_tensor, </span></span></span><span data-slate-object="text" data-key="5681"><span data-slate-leaf="true" data-offset-key="5681:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'grid.jpg'</span></span></span></span><span data-slate-object="text" data-key="5682"><span data-slate-leaf="true" data-offset-key="5682:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5683"></div><div data-slate-type="code-line" data-slate-object="block" data-key="5684"><span data-slate-object="text" data-key="5685"><span data-slate-leaf="true" data-offset-key="5685:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 输入为 List 调用 grid_img 函数后保存</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="5686"><span data-slate-object="text" data-key="5687"><span data-slate-leaf="true" data-offset-key="5687:0" data-first-offset="true"><span data-slate-string="true">torchvision.utils.save_image(img_tensor, </span></span></span><span data-slate-object="text" data-key="5688"><span data-slate-leaf="true" data-offset-key="5688:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">'grid2.jpg'</span></span></span></span><span data-slate-object="text" data-key="5689"><span data-slate-leaf="true" data-offset-key="5689:0" data-first-offset="true"><span data-slate-string="true">, nrow=</span></span></span><span data-slate-object="text" data-key="5690"><span data-slate-leaf="true" data-offset-key="5690:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">5</span></span></span></span><span data-slate-object="text" data-key="5691"><span data-slate-leaf="true" data-offset-key="5691:0" data-first-offset="true"><span data-slate-string="true">, padding=</span></span></span><span data-slate-object="text" data-key="5692"><span data-slate-leaf="true" data-offset-key="5692:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">2</span></span></span></span><span data-slate-object="text" data-key="5693"><span data-slate-leaf="true" data-offset-key="5693:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5694"><span data-slate-object="text" data-key="5695"><span data-slate-leaf="true" data-offset-key="5695:0" data-first-offset="true"><span data-slate-string="true">当输入为一张图片的 Tensor 时，直接保存，保存的图片如下所示。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="5696"><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/bb/yy/bb73d6bdf49fc876d983cfa48569dcyy.png?wh=242x122"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5697"><span data-slate-object="text" data-key="5698"><span data-slate-leaf="true" data-offset-key="5698:0" data-first-offset="true"><span data-slate-string="true">当输入为 List 时，则会先调用 make_grid 函数，make_grid 函数的参数直接加在后面即可，代码中令 nrow=5，保存的图片如下所示。这时我们可以看到图片中，每行中有 5 个数字，最后一行不足的数字，已经自动填充了空图像。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="5699"><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/21/a6/21435a2115ca4704bc51496f8a1c8da6.png?wh=152x212"></div></div><h2 data-slate-type="heading" data-slate-object="block" data-key="5700" id="sr-toc-8"><span data-slate-object="text" data-key="5701"><span data-slate-leaf="true" data-offset-key="5701:0" data-first-offset="true"><span data-slate-string="true">小结</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="5702"><span data-slate-object="text" data-key="5703"><span data-slate-leaf="true" data-offset-key="5703:0" data-first-offset="true"><span data-slate-string="true">恭喜你完成了这节课的学习。至此，Torchvision 的全部内容我们就学完了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5704"><span data-slate-object="text" data-key="5705"><span data-slate-leaf="true" data-offset-key="5705:0" data-first-offset="true"><span data-slate-string="true">今天的重点内容是</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5706"><span data-slate-object="text" data-key="5707"><span data-slate-leaf="true" data-offset-key="5707:0" data-first-offset="true"><span data-slate-string="true"> torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5708"><span data-slate-leaf="true" data-offset-key="5708:0" data-first-offset="true"><span data-slate-string="true"> 模块的使用，包括如何实例化一个网络与如何进行模型的微调。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5709"><span data-slate-type="code" data-slate-object="inline" data-key="5710"><span data-slate-object="text" data-key="5711"><span data-slate-leaf="true" data-offset-key="5711:0" data-first-offset="true"><span data-slate-string="true">torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5712"><span data-slate-leaf="true" data-offset-key="5712:0" data-first-offset="true"><span data-slate-string="true"> 模块为我们提供了深度学习中各种经典的网络结构以及训练好的模型，我们不仅可以实例化一个随机初始化的网络模型，还可以实例化一个预训练好的网络模型。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5713"><span data-slate-object="text" data-key="5714"><span data-slate-leaf="true" data-offset-key="5714:0" data-first-offset="true"><span data-slate-string="true">模型微调可以让我们在自己的小数据集上快速训练模型，并取得比较理想的效果。但是我们需要根据具体问题对预训练模型或数据进行一些修改，你可以灵活调整输出类别的数量，或者调整输入图像的大小。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5715"><span data-slate-object="text" data-key="5716"><span data-slate-leaf="true" data-offset-key="5716:0" data-first-offset="true"><span data-slate-string="true">除了模型微调，我还讲了两个 Torchvision 中有趣的函数，make_grid 和 save_img，我还结合之前我们学习过的读取数据集以及图像变换的内容，为你做了演示。相信 Torchvision 工具配合 PyTorch 使用，一定能够使你事半功倍。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="5717" id="sr-toc-9"><span data-slate-object="text" data-key="5718"><span data-slate-leaf="true" data-offset-key="5718:0" data-first-offset="true"><span data-slate-string="true">每课一练</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="5719"><span data-slate-object="text" data-key="5720"><span data-slate-leaf="true" data-offset-key="5720:0" data-first-offset="true"><span data-slate-string="true">请你使用</span></span></span><span data-slate-type="code" data-slate-object="inline" data-key="5721"><span data-slate-object="text" data-key="5722"><span data-slate-leaf="true" data-offset-key="5722:0" data-first-offset="true"><span data-slate-string="true"> torchvision.models</span></span></span></span><span data-slate-object="text" data-key="5723"><span data-slate-leaf="true" data-offset-key="5723:0" data-first-offset="true"><span data-slate-string="true"> 模块实例化一个 VGG 16 网络。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5724"><span data-slate-object="text" data-key="5725"><span data-slate-leaf="true" data-offset-key="5725:0" data-first-offset="true"><span data-slate-string="true">欢迎你在留言区跟我交流讨论，也推荐你把这节课分享给更多的同事、朋友。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="5726"><span data-slate-object="text" data-key="5727"><span data-slate-leaf="true" data-offset-key="5727:0" data-first-offset="true"><span data-slate-string="true">我是方远，我们下节课见！</span></span></span></div></div></div></div><div><div></div><div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><textarea placeholder="将学到的知识总结成笔记，方便日后快速查找及复习"></textarea></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div><div><div>确认放弃笔记？</div><div>放弃后所记笔记将不保留。</div></div><div><div>新功能上线，你的历史笔记已初始化为私密笔记，是否一键批量公开？</div><div>批量公开的笔记不会为你同步至部落</div></div><div></div></div><div><div><div>公开</div><div>同步至部落</div></div><div>取消</div><div>完成</div></div><div><span>0/2000</span></div></div><div><div><span>划线</span></div><div></div><div></div><div></div><div><span>笔记</span></div><div></div><div><span>复制</span></div></div></div><div><div><div><span></span></div><p><a href="javascript:void(0);">给文章提建议</a></p></div></div><div><span>©</span> 版权归极客邦科技所有，未经许可不得传播售卖。 页面已增加防盗追踪，如有侵权极客邦将依法追究其法律责任。 </div></div><div><div><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div><div>Geek__653581f21177</div></div><div><textarea placeholder="由作者筛选后的优质留言将会公开显示，欢迎踊跃留言。" rows="16"></textarea></div><div><div>Ctrl + Enter 发表</div><div>0/2000 字符</div><div>提交留言</div></div></div><div><h2 id="sr-toc-10">精选留言 (15)</h2><ul><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/2a/fc/de/6e2cb960.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>autiplex</span></div></div><div>
import torch
import torchvision.models as models

# 加载预训练模型
googlenet = models.googlenet (pretrained=True)

# 提取分类层的输入参数
fc_in_features = googlenet.fc.in_features
print ("fc_in_features:", fc_in_features)

# 查看分类层的输出参数
fc_out_features = googlenet.fc.out_features
print ("fc_out_features:", fc_out_features)

# 修改预训练模型的输出分类数 (在图像分类原理中会具体介绍 torch.nn.Linear)
googlenet.fc = torch.nn.Linear (fc_in_features, 10)
'''
输出：
fc_in_features: 1024
fc_out_features: 1000
'''

老师这段代码里 torch.nn.Linear 里是不是应该是 fc_out_features，因为不是要转换输出分类数为 10 么</div><div><p>作者回复：你好，autiplex。谢谢你的留言。
fc_out_features 是原来 googlenet 最后一层（fc 层）的输出，10 是我们期望的输出，现在是要将 fc_out_features（1000）替换成 10。
因此，googlenet.fc = torch.nn.Linear (fc_in_features, 10)，这里表示最后一层的输出改为 10。
torch.nn.Linear 的参数是：（网络输入特征数，网络输出特征数），所以第一个参数应该是 fc_in_features。</p></div><div><div>2021-11-11</div><div><div><i></i></div><div><i></i><span>10</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div></div><div><div><div><span>AstrHan</span></div></div><div>老师，微调的原理是什么啊？image net 训练出来的倒数第二层应该是包含 1000 个类型的特征，那微调训练之后这层输出的会全部变成狗相关的特征吗？如果这样，那训练过程感觉反而会更慢啊。还是说微调训练最多的是输出层，降低倒数第二层里非狗的特征权重？</div><div><p>作者回复：你好，AstrHan。感谢你的留言。

首先，不使用预训练模型的话，我们的模型是从随机初始化的参数开始训练。这个训练过程会非常慢。
使用预训练模型话，相当于在预训练模型的基础上进行训练。
举个例子，把模型训练比作从山顶走向山脚的过程，山脚就是我们模型收敛的状态。
不使用预训练模型的话，就相当于从山顶开始一步步向下走。
使用预训练模型的话，相当于从一个接近山脚的位置向下走。

有两种微调方式。第一种就是这节课中提到的微调，也就是固定卷积层的参数，只训练全连接层。
这也可以理解为 Embedding。ImageNet 上有很多丰富的数据，现在有一个在 ImageNet 上训练很好的模型，然后将我们例子中的图片（狗）用 ImageNet 上训练的模型进行特征提取（ImageNet 模型倒数第二层的输出）。
提取后的数据相当于是原始图片（狗）的另一种表示。
这个过程相当于，原始的数据就是狗的图片。
经过 ImageNet 提取后的特征变为有有耳朵、四条腿、爱吃骨头、跑得快、会汪汪叫之类的特征。
然后用提取后的特征作为新模型的输入，这里的新模型就是修改后的全连接层。

还有一种微调，会在 14 节中讲到，就是使用在 ImageNet 上训练好的模型的参数，作为我们模型的初始化参数进行训练。这种方式是训练所有的参数，而不是固定全连接层前面的参数。</p></div><div><div>2021-10-28</div><div><div><i></i><span>2</span></div><div><i></i><span>10</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="http://thirdwx.qlogo.cn/mmopen/vi_32/j24oyxHcpB5AMR9pMO6fITqnOFVOncnk2T1vdu1rYLfq1cN6Sj7xVrBVbCvHXUad2MpfyBcE4neBguxmjIxyiaQ/132"></div></div><div><div><div><span>vcjmhg</span></div></div><div>vgg16 = models.vgg16(pretrained=True)</div><div><p>作者回复：你好，谢谢留言。每次都非常正确👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻。^^</p></div><div><div>2021-10-27</div><div><div><i></i></div><div><i></i><span>5</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/14/0f/53/92a50f01.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span> 徐洲更</span></div></div><div>老师您好，请教一个可能跟计算机视觉相关的深度学习的问题。假如一款 3D 的赛车类游戏，我在里面做了一系列操作，从起点开到了终点，我能否让神经网络根据我的操作和对应的录屏学会我的操作，并大致重复出来呢？这需要什么样的深度学习模型呢？</div><div><p>作者回复：你好，徐洲更。
可以学习一下强化学习（Reinforcement Learning）相关的内容。^^</p></div><div><div>2021-12-02</div><div><div><i></i><span>2</span></div><div><i></i><span>2</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/29/ca/58/8b4723dc.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span> 奔跑的火龙果</span></div></div><div>iter (tensor_dataloader) 和 next () 分别是什么意思呢？
</div><div><p>作者回复：你好，感谢留言。
iter () 函数用来生成迭代器。
next () 函数用来获取迭代器的元素。
具体可以参见 Python 迭代器的创建和使用。
https://www.w3school.com.cn/python/ref_func_iter.asp</p></div><div><div>2022-08-23</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span>John (易筋)</span></div></div><div>解决 AttributeError: module 'torchvision.models' has no attribute 'googlenet'
运行以下命令报错
googlenet = models.googlenet ()
解决方法
google_net = torchvision.models.inception_v3 (pretrained=True)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-3-0e4152b595be&gt; in &lt;module&gt;
      1 import torchvision.models as models
----&gt; 2 googlenet = models.googlenet ()

AttributeError: module 'torchvision.models' has no attribute 'googlenet'

ref: https://stackoverflow.com/questions/55762706/how-to-load-pretrained-googlenet-model-in-pytorch</div><div><p>作者回复: 👍🏻👍🏻👍🏻^^</p></div><div><div>2022-07-30</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/2b/12/2d/aba06a9a.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>李泽</span></div></div><div>googlenet.fc = torch.nn.Linear (fc_in_features, 10) 之后要如何使用它做一下测试呢？</div><div><p>作者回复: hello， 你好，李泽。测试的话具体是测试什么呢？
如果是推断的话，可以继续看看后面的章节。</p></div><div><div>2022-07-20</div><div><div><i></i><span>2</span></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>亚林</span></div></div><div>vgg16 = models.vgg16()
vgg16 = models.vgg16(pretrained=True)</div><div><p>作者回复: ^^👍🏻👍🏻</p></div><div><div>2022-05-13</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/2d/7b/1e/8bb7c7fe.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>..................</span></div></div><div>老师请问这句话 “tensor：类型是 Tensor 或列表，如果输入类型是 Tensor，其形状应是 (B x C x H x W)；” 中的 B,C,H,W 分别是什么意思？实在想不明白</div><div><p>作者回复：你好，感谢你的留言。
B 是指 batch_size，就是一次处理多少张图片。
C 是指 channel，图片的通道数，通常来说是 3。
H 和 W 就是只图片的高与宽了。</p></div><div><div>2022-04-23</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/2b/17/6b/4e089b79.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>度</span></div></div><div>googlenet.fc = torch.nn.Linear (fc_in_features, 10)
print ("fc_out_features:", fc_out_features)
老师，我更改后得到的输出特征数还是 fc_out_features: 1000 这个结果。请赐教！</div><div><p>作者回复: hello。print (googlenet.fc.out_features) 试试呢？

fc_out_features 变量并没有赋新值，所以不会改变呀</p></div><div><div>2021-11-16</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,/9j/4QAYRXhpZgAASUkqAAgAAAAAAAAAAAAAAP/sABFEdWNreQABAAQAAABkAAD/4QN5aHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLwA8P3hwYWNrZXQgYmVnaW49Iu+7vyIgaWQ9Ilc1TTBNcENlaGlIenJlU3pOVGN6a2M5ZCI/PiA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJBZG9iZSBYTVAgQ29yZSA1LjYtYzE0MCA3OS4xNjA0NTEsIDIwMTcvMDUvMDYtMDE6MDg6MjEgICAgICAgICI+IDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+IDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiIHhtbG5zOnhtcE1NPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvbW0vIiB4bWxuczpzdFJlZj0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL3NUeXBlL1Jlc291cmNlUmVmIyIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bXBNTTpPcmlnaW5hbERvY3VtZW50SUQ9InhtcC5kaWQ6YWE3YmZhMDItMzBhMC00MDg3LTg3MmYtOGMwMjMxNjNhZWRjIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOjI2MTlEODM3NTgzMTExRTk5NDY4Qjk3QUFCNDFBN0QzIiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOjI2MTlEODM2NTgzMTExRTk5NDY4Qjk3QUFCNDFBN0QzIiB4bXA6Q3JlYXRvclRvb2w9IkFkb2JlIFBob3Rvc2hvcCBDQyAyMDE1IChNYWNpbnRvc2gpIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6OTYyRTNCMDNBREI4MTFFOEFFNTJDODlGREQ1OTUzMDMiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6OTYyRTNCMDRBREI4MTFFOEFFNTJDODlGREQ1OTUzMDMiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz7/7gAOQWRvYmUAZMAAAAAB/9sAhAABAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAgICAgICAgICAgIDAwMDAwMDAwMDAQEBAQEBAQIBAQICAgECAgMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwP/wAARCADuAO4DAREAAhEBAxEB/8QAfAABAAICAwEBAAAAAAAAAAAAAAYHBAgBAwUCCgEBAAAAAAAAAAAAAAAAAAAAABAAAgIBAgIECwQJBQAAAAAAAAECAwQRBSEGMWESF0FRgVITk+MUVJTUIkJiB5EyhBVFhbXFNnFygqJTEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwD9vAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGHmbhg7fD0mbl4+LF69l32wrctPBCMmpTfUk2BG7ue+Wqm4rNsua6XTi5DWvVKyutPyaoDmnnrlq19l506W9NPTYuSk2/xQqnGPlaQElxM7Dzq/S4WVj5VfhlRbC1Rfil2G3GXU9GBlAAAAAAAAAAAAAAAAAAAAAAAAAAAA4bUU5SajGKblJtJJJattvgkkBVHMnP8AJSswtilHSLcLdxcVLV9DWHCWsdF/6ST1+6uiQFW35F+VbK/Jutvum9Z23WSssk/xTm3JgdIADvx8nIxLY34t9uPdD9W2myVc1412otPR6cV0MC1uWufvTTrwd8cITlpCrcYpQhKT4KOXBaQrbf346R8aXFgWmnrxXFPimvCAAAAAAAAAAAAAAAAAAAAAAAAAAFUfmBzHKLexYVjjrGMtxsg+LU12oYia6E4tSn400vOQFTAAAAAAAuDkDmSWRFbHm2OVtUHLb7ZvWU6oLWeK2+LdMV2ofgTX3UBaAAAAAAAAAAAAAAAAAAAAAAAABi52XDAwsvNs4wxce6+S10cvRQlNQX4ptaLrYGr+RfblX3ZN8nO7Itsutk/vWWSc5Pq4sDpAAAAAABlYWXbgZeNmUPS3Guruhx0TcJJ9mWnTGa4NeFMDaDGvrysejJqeteRTVfW/HC2EbI/9ZAdwAAAAAAAAAAAAAAAAAAAAAACJc8WurlncOzwdrxateqeVT2v0wTXlA18AAAAAAAAAbFcnXSu5a2mcnq402U/8cfJuoivJGtASYAAAAAAAAAAAAAAAAAAAAAABFOdqXdyzuSjxlWse7yVZVMp/or1YGvQAAAAAAAADY3lGiWPy3tNclo5Yzv8AF9nJusyYvyxtQEjAAAAAAAAAAAAAAAAAAAAAAAdGVj15eNkYty1qyaLaLF4exbCVctOvSXADWDNxLsDLycLIj2bsa6dM/E3B6KUfHCa0afhTAxQAAAAAAZ224Nu55+LgUp+kyboV6pa9iDetljXm1VpyfUgNnqaoUU1UVLs101wqrj4oVxUILyRQHYAAAAAAAAAAAAAAAAAAAAAAAAVrz5yzPNh++cGtzyaK1HNpgtZX0QX2bopcZW0R4NdLhp5ujCmQAAAAAAXbyLyzPbaXumdW4ZuVX2aKprSWNjS0bck+Mbr9FqumMeHS2gLDAAAAAAAAAAAAAAAAAAAAAAAAAACuOZOQ6c+dmbtDrxcubc7cWX2cbIm+LlW0n7vbLw8OxJ+bxbCpM7bM/bLXVn4l+NPVpekg1CenhrsWtdseuLaAwQAHo7ftO47raqsDEuyZapSlCOlVevhtul2aql/uaAt3lrkWjbJ15u5yry86Gk6qYrXFxpripfaSd90X0NpRi+hNpSAsIAAAAAAAAAAAAAAAAAAAAAAAAAAAAD4sqrug67a4W1y/WhZCM4P/AFjJNMDw7eVuXrm5T2jCTfT6Kr0C49VLrQHNPK/L1ElKvaMJtcU7alfo/Gle7FqB7cK4VQVdcIVwitIwhFQhFeJRikkgPsAAAAAAAAAAAAAAAAAAAAAAAAAY2XmYuBRPKzL68aiv9ay2XZjq+iKXTKcvBFJt+BARGf5g8uRk4q3LsSeinDFkoy60pyhPR9aQHz3h8u+dm/K+0Ad4fLvnZvyvtAHeHy752b8r7QB3h8u+dm/K+0Ad4fLvnZvyvtAHeHy752b8r7QB3h8u+dm/K+0Ad4fLvnZvyvtAMjG575cybY1PKtxnJpRnk0Trq1fglZHtxrXXLRLxgTCMozjGUZKUZJSjKLTjKLWqlFrVNNPgwOQAAAAAAAAAAAAAAAAAAAAUZ+YW43ZG9ywHOSx9vqpUa9fsu7IphkTta8MnCyMepLrYECAAAAAAAAAAAF0/lxuN2Tt+Zg2zlOO320uhyerhTlK1qpPzYWUSa8Xa06NALHAAAAAAAAAAAAAAAAAAAABr3zx/lO6fsX9OxAImAAAAAAAAAAALY/K/+Ofyz+4AWwAAAAAAAAAAAAAAAAAAAADXvnj/ACndP2L+nYgETAAAAAAAAAAAFsflf/HP5Z/cALYAAAAAAAAAAAAAAAAAAAABVvMfJG7bxvOZuONkbdCjI937Eb7cmNq9DiUUS7Ua8S2C1nU2tJPgB4fdrvvxe0+vzPoAHdrvvxe0+vzPoAHdrvvxe0+vzPoAHdrvvxe0+vzPoAHdrvvxe0+vzPoAHdrvvxe0+vzPoAHdrvvxe0+vzPoAHdrvvxe0+vzPoAHdrvvxe0+vzPoAHdrvvxe0+vzPoAJvyby1ncvfvH323Et989z9F7rZdPs+7+9dvt+loo019OtNNfD0ATcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA//Z"></div></div><div><div><div><span>clee</span></div></div><div>vgg16 = models.vgg16(pretrained=True)</div><div><p>作者回复：你好，clee。谢谢你的留言。
👍🏻👍🏻👍🏻👍🏻，完全正确 ^^。</p></div><div><div>2021-11-07</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/2a/cc/41/af863836.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>夏阳</span></div></div><div>老师，这段段代码会报错
torchvision.utils.save_image (grid_tensor, 'grid.jpg')

ValueError: unknown file extension: 

搞不清楚什么状况，请帮忙指教</div><div><p>作者回复：你好，夏阳，感谢你的留言。能新开一个留言提供一下你的代码吗？或者在发到微信群里。
</p></div><div><div>2021-10-30</div><div><div><i></i><span>3</span></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div></div><div><div><div><span>勿更改任何信息</span></div></div><div>老师，代码太分散了，后期是否可以在 github 上统一存储呢？</div><div><p>作者回复：你好，勿更改任何信息，谢谢你的留言。
专栏中每一篇中的代码，基本都是可运行代码 ^^，参照讲解来看更好一点。
暂时不会放到 GitHub 上。</p></div><div><div>2021-10-27</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://thirdwx.qlogo.cn/mmopen/vi_32/SKmvhbNe9LMPQ0ib8ZqbJEfHicUxzxCSKVXiaibn7OrmXGUFQjkesgvODymZz4kibzqOGxuRq42t3sB2ibcBIIGWRgSg/132"></div></div><div><div><div><span>超人不会飞</span></div></div><div>vgg16net=torchvision.models.vgg16(pretrained=True)</div><div><p>作者回复：你好，超人不会飞。感谢你的留言。👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻。完全正确 ^^。</p></div><div><div>2021-10-27</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/14/e4/5a/b61bd7f1.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>黄焯</span></div></div><div>文中的当前版本应该是 v1.10.0，而不是 v0.10.0 吧</div><div><p>作者回复：你好，黄焯，感谢你的留言。
是 v0.10.0 版本 ^^。你可以看下这个页面 https://github.com/pytorch/vision。</p></div><div><div>2021-10-27</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li></ul><div> 收起评论<span></span></div></div></sr-rd-content>
                            <toc-bg><toc style="width: initial;overflow: auto!important;"class="simpread-font simpread-theme-root" data-reactid=".p"><outline class="toc-level-h1" data-reactid=".p.0" style="width: 175px;"><active data-reactid=".p.0.0" ></active><a class="toc-outline-theme-github" href="#sr-toc-0" data-reactid=".p.0.1"><span data-reactid=".p.0.1.0"> 08 | Torchvision（下）：其他有趣的功能</span></a></outline><outline class="toc-level-h2" data-reactid=".p.1" style="width: 165px;"><active data-reactid=".p.1.0"></active><a class="toc-outline-theme-github" href="#sr-toc-1" data-reactid=".p.1.1"><span data-reactid=".p.1.1.0">常见网络模型</span></a></outline><outline class="toc-level-h3" data-reactid=".p.2" style="width: 155px;"><active data-reactid=".p.2.0"></active><pangu> </pangu><a class="toc-outline-theme-github" href="#sr-toc-2" data-reactid=".p.2.1"><span data-reactid=".p.2.1.0">torchvision.models 模块</span></a></outline><outline class="toc-level-h3" data-reactid=".p.3" style="width: 155px;"><active data-reactid=".p.3.0"></active><a class="toc-outline-theme-github" href="#sr-toc-3" data-reactid=".p.3.1"><span data-reactid=".p.3.1.0">实例化一个 GoogLeNet 网络</span></a></outline><outline class="toc-level-h3" data-reactid=".p.4" style="width: 155px;"><active data-reactid=".p.4.0"></active><a class="toc-outline-theme-github" href="#sr-toc-4" data-reactid=".p.4.1"><span data-reactid=".p.4.1.0">模型微调</span></a></outline><outline class="toc-level-h2" data-reactid=".p.5" style="width: 165px;"><active data-reactid=".p.5.0"></active><a class="toc-outline-theme-github" href="#sr-toc-5" data-reactid=".p.5.1"><span data-reactid=".p.5.1.0">其他常用函数</span></a></outline><outline class="toc-level-h3" data-reactid=".p.6" style="width: 155px;"><active data-reactid=".p.6.0"></active><pangu> </pangu><a class="toc-outline-theme-github" href="#sr-toc-6" data-reactid=".p.6.1"><span data-reactid=".p.6.1.0">make_grid</span></a></outline><outline class="toc-level-h3" data-reactid=".p.7" style="width: 155px;"><active data-reactid=".p.7.0"></active><a class="toc-outline-theme-github" href="#sr-toc-7" data-reactid=".p.7.1"><span data-reactid=".p.7.1.0">save_img</span></a></outline><outline class="toc-level-h2" data-reactid=".p.8" style="width: 165px;"><active data-reactid=".p.8.0"></active><pangu> </pangu><a class="toc-outline-theme-github" href="#sr-toc-8" data-reactid=".p.8.1"><span data-reactid=".p.8.1.0">小结</span></a></outline><outline class="toc-level-h2" data-reactid=".p.9" style="width: 165px;"><active data-reactid=".p.9.0"></active><a class="toc-outline-theme-github" href="#sr-toc-9" data-reactid=".p.9.1"><span data-reactid=".p.9.1.0">每课一练</span></a></outline><outline class="toc-level-h2" data-reactid=".p.a" style="width: 165px;"><active data-reactid=".p.a.0"></active><a class="toc-outline-theme-github" href="#sr-toc-10" data-reactid=".p.a.1"><span data-reactid=".p.a.1.0">精选留言 (15)</span></a></outline></toc></toc-bg>
                            <sr-rd-footer>
                                <sr-rd-footer-group>
                                    <sr-rd-footer-line></sr-rd-footer-line>
                                    <sr-rd-footer-text>全文完</sr-rd-footer-text>
                                    <sr-rd-footer-line></sr-rd-footer-line>
                                </sr-rd-footer-group>
                                <sr-rd-footer-copywrite>
                                    <div>本文由 <a href="http://ksria.com/simpread" target="_blank">简悦 SimpRead</a> 转码，用以提升阅读体验，<a href="https://time.geekbang.org/column/article/431420" target="_blank">原文地址 </a></div>
                                </sr-rd-footer-copywrite>
                            </sr-rd-footer>
                        </sr-read>
                    </body>
                </html>