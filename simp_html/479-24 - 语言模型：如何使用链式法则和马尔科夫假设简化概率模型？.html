
                <html lang="en" class="simpread-font simpread-theme-root" style=''>
                    <head>
                        <meta charset="utf-8">
                        <meta http-equiv="content-type" content="text/html; charset=UTF-8;charset=utf-8">
                        <meta http-equiv="X-UA-Compatible" content="IE=Edge">
                        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1">
                        <meta name="author" content="Kenshin"/>
                        <meta name="description" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展" />
                        <meta name="keywords" content="Chrome extension, Chrome 扩展, 阅读模式, 沉浸式阅读, 简悦, 简阅, read mode, reading mode, reader view, firefox, firefox addon, userscript, safari, opera, tampermonkey"/>
                        <meta name="thumbnail" content="https://simpread-1254315611.cos.ap-shanghai.myqcloud.com/static/introduce-2.png"/>
                        <meta property="og:title" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展"/>
                        <meta property="og:type" content="website">
                        <meta property="og:local" content="zh_CN"/>
                        <meta property="og:url" content="http://ksria.com/simpread"/>
                        <meta property="og:image" content="https://simpread-1254315611.cos.ap-shanghai.myqcloud.com/static/introduce-2.png"/>
                        <meta property="og:image:type" content="image/png"/>
                        <meta property="og:image:width" content="960"/>
                        <meta property="og:image:height" content="355"/>
                        <meta property="og:site_name" content="http://ksria.com/simpread"/>
                        <meta property="og:description" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展"/>
                        <style type="text/css">.simpread-font{font:300 16px/1.8 -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#333;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased}.simpread-hidden{display:none}.simpread-read-root{display:-webkit-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;margin:0;top:-1000px;left:0;width:100%;z-index:2147483646;overflow-x:hidden;opacity:0;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}.simpread-read-root-show{top:0}.simpread-read-root-hide{top:1000px}sr-read{display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;margin:20px 20%;min-width:400px;min-height:400px;text-align:center}read-process{position:fixed;top:0;left:0;height:3px;width:100%;background-color:#64b5f6;-webkit-transition:width 2s;transition:width 2s;z-index:20000}sr-rd-content-error{display:block;position:relative;margin:0;margin-bottom:30px;padding:25px;background-color:rgba(0,0,0,.05)}sr-rd-footer{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;font-size:14px}sr-rd-footer,sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}sr-rd-footer-group{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}sr-rd-footer-text{min-width:150px}sr-rd-footer-copywrite{margin:10px 0 0;color:inherit}sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}sr-rd-footer-copywrite .second{margin:10px 0}sr-rd-footer-copywrite .third a:hover{border:none!important}sr-rd-footer-copywrite .third a:first-child{margin-right:50px}sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}sr-rd-footer-copywrite .sr-icon:hover{opacity:1}sr-rd-footer-copywrite a,sr-rd-footer-copywrite a:link,sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}sr-rd-footer-copywrite a:focus,sr-rd-footer-copywrite a:hover,sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.simpread-blocks{text-decoration:none!important}.simpread-blocks *{margin:0}.simpread-blocks a{padding:0;text-decoration:none!important}.simpread-blocks img{margin:0;padding:0;border:0;background:transparent;box-shadow:none}.simpread-focus-root{display:block;position:fixed;top:0;left:0;right:0;bottom:0;background-color:hsla(0,0%,92%,.9);z-index:2147483645;opacity:0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms}.simpread-focus-highlight{position:relative;box-shadow:0 0 0 20px #fff;background-color:#fff;overflow:visible;z-index:2147483646}.sr-controlbar-bg sr-rd-crlbar,.sr-controlbar-bg sr-rd-crlbar fab{z-index:2147483647}sr-rd-crlbar.controlbar{position:fixed;right:0;bottom:0;width:100px;height:100%;opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}sr-rd-crlbar.controlbar:hover{opacity:1}sr-rd-crlbar fap *{box-sizing:border-box}@media (max-height:620px){fab{zoom:.8}}@media (max-height:783px){dialog-gp dialog-content{max-height:580px}dialog-gp dialog-footer{border-top:1px solid #e0e0e0}}.simpread-highlight-selector{outline:3px dashed #1976d2!important;cursor:pointer!important}.simpread-highlight-controlbar,.simpread-highlight-selector{background-color:#fafafa!important;opacity:.8!important;-webkit-transition:opacity .5s ease!important;transition:opacity .5s ease!important}.simpread-highlight-controlbar{position:relative!important;border:3px dashed #1976d2!important}simpread-highlight,sr-snapshot-ctlbar{position:fixed;top:0;left:0;right:0;padding:15px;height:50px;background-color:rgba(50,50,50,.9);box-shadow:0 2px 5px rgba(0,0,0,.26);box-sizing:border-box;z-index:2147483640}simpread-highlight,sr-highlight-ctl,sr-snapshot-ctlbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-highlight-ctl{margin:0 5px;width:50px;height:20px;color:#fff;background-color:#1976d2;border-radius:4px;box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12);cursor:pointer}toc-bg{position:fixed;left:0;top:0;width:50px;height:200px;font-size:medium}toc-bg:hover{z-index:3}.toc-bg-hidden{opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}.toc-bg-hidden:hover{opacity:1;z-index:3}.toc-bg-hidden:hover toc{width:180px}toc *{all:unset}toc{position:fixed;left:0;top:100px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;padding:10px;width:0;max-width:200px;max-height:500px;overflow-x:hidden;overflow-y:hidden;cursor:pointer;border:1px solid hsla(0,0%,62%,.22);-webkit-transition:width .5s;transition:width .5s}toc:hover{overflow-y:auto}toc.mini:hover{width:200px!important}toc::-webkit-scrollbar{width:3px}toc::-webkit-scrollbar-thumb{border-radius:10px;background-color:hsla(36,2%,54%,.5)}toc outline{position:relative;display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;overflow:hidden;text-overflow:ellipsis;padding:2px 0;min-height:21px;line-height:21px;text-align:left}toc outline a,toc outline a:active,toc outline a:focus,toc outline a:visited{display:block;width:100%;color:inherit;font-size:11px;text-decoration:none!important;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}toc outline a:hover{font-weight:700!important}toc outline a.toc-outline-theme-dark,toc outline a.toc-outline-theme-night{color:#fff!important}.toc-level-h1{padding-left:5px}.toc-level-h2{padding-left:15px}.toc-level-h3{padding-left:25px}.toc-level-h4{padding-left:35px}.toc-outline-active{border-left:2px solid #f44336}toc outline active{position:absolute;left:0;top:0;bottom:0;padding:0 0 0 3px;border-left:2px solid #e8e8e8}sr-kbd{background:-webkit-gradient(linear,0 0,0 100%,from(#fff785),to(#ffc542));border:1px solid #e3be23;-o-border-image:none;border-image:none;-o-border-image:initial;border-image:initial;position:absolute;left:0;padding:1px 3px 0;font-size:11px!important;font-weight:700;box-shadow:0 3px 7px 0 rgba(0,0,0,.3);overflow:hidden;border-radius:3px}.sr-kbd-a{position:relative}kbd-mapping{position:fixed;left:5px;bottom:5px;-ms-flex-flow:row;flex-flow:row;width:250px;height:500px;background-color:#fff;border:1px solid hsla(0,0%,62%,.22);box-shadow:0 2px 5px rgba(0,0,0,.26);border-radius:3px}kbd-mapping,kbd-maps{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}kbd-maps{margin:40px 0 20px;width:100%;overflow-x:auto}kbd-maps::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}kbd-maps::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}kbd-mapping kbd-map-title{position:absolute;margin:5px 0;width:100%;font-size:14px;font-weight:700}kbd-maps-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}kbd-maps-title{margin:5px 0;padding-left:53px;font-size:12px;font-weight:700}kbd-map kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fafbfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}kbd-map kbd-name{display:inline-block;text-align:right;width:50px}kbd-map kbd-desc{padding-left:3px}sharecard-bg{position:fixed;top:0;left:0;width:100%;height:100%;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(0,0,0,.4);z-index:2147483647}sharecard{max-width:450px;background-color:#64b5f6}sharecard,sharecard-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-head{margin:25px;color:#fff;border-radius:10px;box-shadow:0 2px 6px 0 rgba(0,0,0,.2),0 25px 50px 0 rgba(0,0,0,.15)}sharecard-card{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-card,sharecard-top{display:-webkit-box;display:-ms-flexbox;display:flex}sharecard-top{-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:65px;background-color:#fff;color:#878787;font-size:25px;font-weight:500;border-top-left-radius:10px;border-top-right-radius:10px}sharecard-top span.logos{display:block;width:48px;height:48px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAMAAABg3Am1AAABU1BMVEUAAAAnNJMnNZI3Q5onNJInNJMnNJInNJMnNJI8SJ0tOZY/S55EUKAoNJI6RpwoNJNIU6InNJInNJImNJI7SJwmNJJ2fLUiMJFKVaNCTJ9faK1HUaJOWKVSXaUnNJNYY6pye7cmM5JXYKhwebMjMI8mL4719fW9vb0oNZP/UlLz8/QqN5TAwMAnNJPv7+/Pz8/q6+/p6enNzc3Kysry8vMsOJXc3env7/LU1uXo29vR0dHOzs7ExMTwjo73bW37XV3Aj1TCYELl5u3n5+fW2Obn6O7f4OrZ2+g0QJkxPpgvO5bh4uvS1OTP0ePCwsJQW6ZLVqTs7fHd3d3V1dXqv79VX6lET6A1TIxXUIBSgHxWQnpelHf+WVnopkXqbC7j5Ozi4uLDw8NGUaFATJ9SgH3r6+vGyd7BxNva2trX19ejqM2gpczHx8dze7Zha67Z2dlTgH1aXQeSAAAAJnRSTlMA6ff+497Y8NL+/fv49P379sqab/BeOiX06tzVy8m/tKqpalA7G6oKj0EAAAJlSURBVEjHndNXWxpBFIDhcS2ICRLAkt4Dx4WhLk0E6R0MYoIISrWX5P9f5cwSIRC2+T1czMV5n2FnZwn2eWONUqCAv3H2Uf5Ra1hx4+0WEXtDQW0fCPYJ1EffEfIV4CSROAE4jsePoTFsNmTJF/IeIHF2lgCIn57GodlqDWXBK7IwBYatVlMWFAildPKX7I3m74Z9fsCiQChoimoFQAz04Ad2gH1n9fv9n9hgMNDr9euLWD6fLxQKxaLfb7dTSlahbFVdEPwIQtrAihZQgyKCtCagbQe3xh0QFMgy5MR11+ewYY5/qlZ7vT2xu93ULKjbFLpiUxnIIwjgKmVTLDUFXMrAi2NJWCRLIthTBo4xyOLKpwyqU6CuDCI41hFBCVdOhyLw4FgJ1skCAiyl9BSHbCorgo6VJXTru5hrVCQS8Yr5xLzX59YJSFpVFwD9U0BGC3hGdFpATgRupTGe9R9I1b1ePBvXKDyvq/O/44LT4/E4BUbSCAwj8Evq6HlnOBprx6JhJz8Gktc7xeaP9ndY+0coQvCccFBD4JW60UIY50ciLOAODAQRVOeCHm4Q3Xks6uRDY+CQ+AR4T2wMYh6+jMCIQOp78CFoj0H7EQgIuhI3dGaHCrwgADwCPjJvA372GRigCJg49FUdk3D87pq3zp4SA5zc1Zh9DxfwkpjgUg5Mv+lbeE3McC8Lpu7SA3wk2xzcqL2tN5DfIsQC8HB7UamUy6FQOpTO5QKBQDZbKnWSyUzGjdWCwaDA8+7Le4BNgm3qQGWchYh9s5hNq6wVbBlbwhZYOp3OYOA4zmgEypnM2zj8ByIdedKrH8vDAAAAAElFTkSuQmCC");zoom:.8}sharecard-content{padding:15px;max-height:500px;font-size:20px;text-align:justify;background-color:#2196f3;overflow-x:hidden;overflow-y:auto}sharecard-via{padding:10px;font-size:10px;background-color:#2196f3}sharecard-footer{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:100px;background-color:#fff;color:#878787;font-size:15px;font-weight:500;border-bottom-left-radius:10px;border-bottom-right-radius:10px}sharecard-footer,sharecard-footer div{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sharecard-footer span.qrcode{display:block;width:100px;height:100px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,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")}sharecard-control{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:0 19px;height:80px;background-color:#fff}simpread-snapshot{width:100%;height:100%;cursor:move;z-index:2147483645}simpread-snapshot,sr-mask{position:fixed;left:0;top:0}sr-mask{background-color:rgba(0,0,0,.1)}.simpread-feedback,.simpread-urlscheme{position:fixed;right:20px;bottom:20px;z-index:2147483646}simpread-feedback,simpread-urlscheme{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:20px 20px 0;width:500px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:3px;box-shadow:0 0 2px rgba(0,0,0,.12),0 2px 2px rgba(0,0,0,.26);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease}simpread-feedback *,simpread-urlscheme *{font-size:12px!important;box-sizing:border-box}simpread-feedback.active,simpread-urlscheme.active{-webkit-animation-name:srFadeInUp;animation-name:srFadeInUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback.hide,simpread-urlscheme.hide{-webkit-animation-name:srFadeInDown;animation-name:srFadeInDown;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback sr-fb-label,simpread-urlscheme sr-urls-label{width:100%}simpread-feedback sr-fb-head,simpread-urlscheme sr-urls-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin-bottom:5px;width:100%}simpread-feedback sr-fb-content,simpread-urlscheme sr-urls-content{margin-bottom:5px;width:100%}simpread-feedback sr-urls-footer,simpread-urlscheme sr-urls-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-fb-a,simpread-urlscheme sr-urls-a{color:#2163f7;cursor:pointer}simpread-feedback text-field-state,simpread-urlscheme text-field-state{border-top:none rgba(34,101,247,.8)!important;border-left:none rgba(34,101,247,.8)!important;border-right:none rgba(34,101,247,.8)!important;border-bottom:2px solid rgba(34,101,247,.8)!important}simpread-feedback switch,simpread-urlscheme switch{margin-top:0!important}@-webkit-keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@-webkit-keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}@keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}simpread-feedback sr-fb-head{font-weight:700}simpread-feedback sr-fb-content{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}simpread-feedback sr-fb-content,simpread-feedback sr-fb-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}simpread-feedback sr-fb-footer{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-close{position:absolute;right:20px;cursor:pointer;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:200}simpread-feedback sr-close:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-top:10px}simpread-feedback sr-stars i{margin-right:10px;cursor:pointer}simpread-feedback sr-stars i svg{-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}simpread-feedback sr-stars i svg:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars i.active svg{-webkit-transform:rotate(0) scale(1);transform:rotate(0) scale(1)}simpread-feedback sr-emojis{display:block;height:100px;overflow:hidden}simpread-feedback sr-emoji{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:.3s;transition:.3s}simpread-feedback sr-emoji>svg{margin:15px 0;width:70px;height:70px;-ms-flex-negative:0;flex-shrink:0}simpread-feedback sr-stars-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin:10px 0 20px}</style>
                        <style type="text/css">.simpread-theme-root{font-size:62.5%!important}sr-rd-content,sr-rd-desc,sr-rd-title{width:100%}sr-rd-title{display:-webkit-box;margin:1em 0 .5em;overflow:hidden;text-overflow:ellipsis;text-rendering:optimizelegibility;-webkit-line-clamp:3;-webkit-box-orient:vertical}sr-rd-content{text-align:left;word-break:break-word}sr-rd-desc{text-align:justify;line-height:2.4;margin:0 0 1.2em;box-sizing:border-box}sr-rd-content{font-size:25.6px;font-size:1.6rem;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:25.6px;font-size:1.6rem}sr-rd-content div,sr-rd-content p,sr-rd-content pre,sr-rd-content sr-blockquote{margin:0 0 1.2em;word-break:break-word}sr-rd-content a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-rd-content a,sr-rd-content a:link{color:inherit;font-size:inherit;font-weight:inherit;border:none}sr-rd-content a:hover{background:transparent}sr-rd-content img{margin:10px;padding:5px;max-width:100%;background:#fff;border:1px solid #bbb;box-shadow:1px 1px 3px #d4d4d4}sr-rd-content figcaption{text-align:center;font-size:14px}sr-rd-content sr-blockquote{display:block;position:relative;padding:15px 25px;text-align:left;line-height:inherit}sr-rd-content sr-blockquote:before{position:absolute}sr-rd-content sr-blockquote *{margin:0;font-size:inherit}sr-rd-content table{width:100%;margin:0 0 1.2em;word-break:keep-all;word-break:normal;overflow:auto;border:none}sr-rd-content table td,sr-rd-content table th{border:none}sr-rd-content ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-rd-content ol{list-style:decimal;margin:0;padding:0}sr-rd-content ol li,sr-rd-content ul li{font-size:inherit;list-style:disc;margin:0 0 1.2em}sr-rd-content ol li{list-style:decimal;margin-left:1.3em}sr-rd-content ol li *,sr-rd-content ul li *{margin:0;text-align:left;text-align:initial}sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em}sr-rd-content li ul{list-style:circle}sr-rd-content pre{font-family:Consolas,Monaco,Andale Mono,Source Code Pro,Liberation Mono,Courier,monospace;display:block;padding:15px;line-height:1.5;word-break:break-all;word-wrap:break-word;white-space:pre;overflow:auto}sr-rd-content pre,sr-rd-content pre *,sr-rd-content pre div{font-size:17.6px;font-size:1.1rem}sr-rd-content li pre code,sr-rd-content p pre code,sr-rd-content pre{background-color:transparent;border:none}sr-rd-content pre code{margin:0;padding:0}sr-rd-content pre code,sr-rd-content pre code *{font-size:17.6px;font-size:1.1rem}sr-rd-content pre p{margin:0;padding:0;color:inherit;font-size:inherit;line-height:inherit}sr-rd-content li code,sr-rd-content p code{margin:0 4px;padding:2px 4px;font-size:17.6px;font-size:1.1rem}sr-rd-content mark{margin:0 5px;padding:2px;background:#fffdd1;border-bottom:1px solid #ffedce}.sr-rd-content-img{width:90%;height:auto}.sr-rd-content-img-load{width:48px;height:48px;margin:0;padding:0;border-style:none;border-width:0;background-repeat:no-repeat;background-image:url(data:image/gif;base64,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)}.sr-rd-content-center{text-align:center;display:-webkit-box;-webkit-box-align:center;-webkit-box-pack:center;-webkit-box-orient:vertical}.sr-rd-content-center-small{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.sr-rd-content-center-small img{margin:0;padding:0;border:0;box-shadow:none}img.simpread-img-broken{cursor:pointer}.sr-rd-content-nobeautify{margin:0;padding:0;border:0;box-shadow:0 0 0}sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-radius:4px;box-shadow:0 1px 2px 0 rgba(60,64,67,.3),0 2px 6px 2px rgba(60,64,67,.15)}sr-rd-mult:hover{-webkit-transition:all .45s 0ms;transition:all .45s 0ms;box-shadow:1px 1px 8px rgba(0,0,0,.16)}sr-rd-mult sr-rd-mult-content{padding:0 16px;overflow:auto}sr-rd-mult sr-rd-mult-avatar,sr-rd-mult sr-rd-mult-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sr-rd-mult sr-rd-mult-avatar{-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 15px}sr-rd-mult sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}sr-rd-mult sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}sr-rd-mult sr-rd-mult-content img{max-width:80%}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-page{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%}</style>
                        <style type="text/css">sr-rd-theme-github{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:57.6px;font-size:3.6rem;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;line-height:1.225}sr-rd-content h3{font-size:38.4px;font-size:2.4rem;line-height:1.43}sr-rd-content h4{font-size:32px;font-size:2rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}sr-rd-title{font-family:PT Sans,SF UI Display,\.PingFang SC,PingFang SC,Neue Haas Grotesk Text Pro,Arial Nova,Segoe UI,Microsoft YaHei,Microsoft JhengHei,Helvetica Neue,Source Han Sans SC,Noto Sans CJK SC,Source Han Sans CN,Noto Sans SC,Source Han Sans TC,Noto Sans CJK TC,Hiragino Sans GB,sans-serif;font-size:54.4px;font-size:3.4rem;font-weight:700;line-height:1.3}sr-rd-desc{position:relative;margin:0;margin-bottom:30px;padding:25px;padding-left:56px;font-size:28.8px;font-size:1.8rem;color:#777;background-color:rgba(0,0,0,.05);box-sizing:border-box}sr-rd-desc:before{content:"\201C";position:absolute;top:-28px;left:16px;font-size:80px;font-family:Arial;color:rgba(0,0,0,.15)}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content b *,sr-rd-content strong,sr-rd-content strong * sr-rd-content b{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}</style>
                        <style type="text/css"></style>
                        <style type="text/css">@media (pointer:coarse){sr-read{margin:20px 5%!important;min-width:0!important;max-width:90%!important}sr-rd-title{margin-top:0;font-size:2.7rem}sr-rd-content sr-blockquote,sr-rd-desc{margin:10 0!important;padding:0 0 0 10px!important;width:90%;font-size:1.8rem;font-style:normal;line-height:1.7;text-align:justify}sr-rd-content{font-size:1.75rem;font-weight:300}sr-rd-content figure{margin:0;padding:0;text-align:center}sr-rd-content a,sr-rd-content a:link,sr-rd-content li code,sr-rd-content p code{font-size:inherit}sr-rd-footer{margin-top:20px}sr-blockquote,sr-blockquote *{margin:5px!important;padding:5px!important}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6,sr-rd-title{font-family:PingFang SC,Verdana,Helvetica Neue,Microsoft Yahei,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#000;font-weight:100;line-height:1.35}sr-rd-content-h1,sr-rd-content-h2,sr-rd-content-h3,sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;line-height:1.35}sr-rd-content-h1,sr-rd-content h1{font-size:1.8em}sr-rd-content-h2,sr-rd-content h2{font-size:1.6em}sr-rd-content-h3,sr-rd-content h3{font-size:1.4em}sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:1.2em}sr-rd-content-ul,sr-rd-content ul{margin-left:1.3em!important;list-style:disc}sr-rd-content-ol,sr-rd-content ol{list-style:decimal;margin-left:1.9em!important}sr-rd-content-ol ol,sr-rd-content-ol ul,sr-rd-content-ul ol,sr-rd-content-ul ul,sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em!important}sr-rd-content img{margin:0;padding:0;border:0;max-width:100%!important;height:auto;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-rd-mult{min-width:0;background-color:#fff;box-shadow:0 1px 6px rgba(32,33,36,.28);border-radius:8px}sr-rd-mult sr-rd-mult-avatar div{margin:0}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center-small{margin:7px 0!important}sr-rd-mult sr-rd-mult-avatar span{display:block}sr-rd-mult sr-rd-mult-content{padding-left:0}@media only screen and (max-device-width:1024px){.simpread-theme-root,html.simpread-theme-root{font-size:80%!important}sr-rd-mult sr-rd-mult-avatar img{width:50px;height:50px;min-width:50px;min-height:50px}toc-bg toc{width:10px!important}toc-bg:hover toc{width:auto!important}}@media only screen and (max-device-width:414px){.simpread-theme-root,html.simpread-theme-root{font-size:70%!important}sr-rd-mult sr-rd-mult-avatar img{width:30px;height:30px;min-width:30px;min-height:30px}}@media only screen and (max-device-width:320px){.simpread-theme-root,html.simpread-theme-root{font-size:90%!important}sr-rd-content p{margin-bottom:.5em}}}</style>
                        <style type="text/css">undefinedsr-rd-content *, sr-rd-content p, sr-rd-content div {}sr-rd-content pre code, sr-rd-content pre code * {}sr-rd-desc {}sr-rd-content pre {}sr-rd-title {}</style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css">sr-rd-content *, sr-rd-content p, sr-rd-content div {
        font-size: 15px;
    }

    .annote-perview, .annote-perview * {
        color: rgb(85, 85, 85);
        font-weight: 400;
        line-height: 1.8;
    }</style>
                        
                        
                        <script>setTimeout(()=>{const e=location.hash.replace("#id=","");let t,a=!1;const n=t=>{for(let n of t){let t;if((t=e.length>6?n.getAttribute("data-id"):n.getAttribute("data-idx"))==e){n.scrollIntoView({behavior:"smooth",block:"start",inline:"nearest"}),a=!0;break}}};e&&(0==(t=document.getElementsByClassName("sr-unread-card")).length&&(t=document.getElementsByTagName("sr-annote")),n(t),a||n(t=document.getElementsByClassName("sr-annote")))},500);</script>
                        <script>document.addEventListener("DOMContentLoaded",function(){if("localhost"==location.hostname){const t=document.getElementsByTagName("img");for(let o of t){const t=o.src;t.startsWith("http")&&(o.src=location.origin+"/proxy?url="+t)}}},!1);</script>
                        <style>toc a {font-size: inherit!important;font-weight: 300!important;}</style><script>setTimeout(()=>{const max=document.getElementsByTagName('sr-annote-note-tip').length;for(let i=0;i<max;i++){const target=document.getElementsByTagName('sr-annote-note-tip')[i],value=target.dataset.value;value&&(target.innerText=value)}},1000);</script><title>简悦 | 24 | 语言模型：如何使用链式法则和马尔科夫假设简化概率模型？</title>
                    </head>
                    <body>
                        <sr-read style='font-family: "LXGW WenKai Screen";'>
                            <sr-rd-title>24 | 语言模型：如何使用链式法则和马尔科夫假设简化概率模型？</sr-rd-title>
                    <sr-rd-desc style="margin: 0;padding-top: 0;padding-bottom: 0;font-style: normal;font-size: 18px;">如果你只想当一个普通的程序员，那么数学对你来说，并不重要。但是如果你想做一个顶级程序员，梦想着改变世界，那么数学对你来说就很重要了。</sr-rd-desc>
                    <sr-rd-content><h1 id="sr-toc-0">24 | 语言模型：如何使用链式法则和马尔科夫假设简化概率模型？</h1><div><span>黄申</span> <span> 2019-02-08</span></div><div><div><div class="sr-rd-content-center"><img class="sr-rd-content-img" src="https://static001.geekbang.org/resource/image/49/8d/497a423b47b58b38c5a66ee31c4cd08d.jpg"></div><div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADEAAAABCAYAAAB35kaxAAAAAXNSR0IArs4c6QAAAChJREFUGFdjfPfu3X8GKBAUFASz3r9/DxNiQBbDxcamn1yzSLEDZi8A2agny86FR+IAAAAASUVORK5CYII="></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAABCAYAAAA8TpVcAAAAAXNSR0IArs4c6QAAAB1JREFUGFdjfPfu3X9BQUEGEHj//j2YBgFCYtjkAcT9D8ti7hUOAAAAAElFTkSuQmCC"></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD0AAAABCAYAAABt2qY/AAAAAXNSR0IArs4c6QAAACdJREFUGFdjfPfu3X9BQUEGEHj//j2YBgFkMULy2PQQK0YLewiZCQDhUS3LBhDf1QAAAABJRU5ErkJggg=="></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAABCAYAAAA4u0VhAAAAAXNSR0IArs4c6QAAAB1JREFUGFdjfPfu3X9BQUEGEHj//j2YBgFkMULyAF6lD8tbqYWOAAAAAElFTkSuQmCC"></div></div></div><div><div><div></div></div><div><div><div><div><div><div>00:00</div></div><div></div></div></div><a href="javascript:;"> 1.0x <i><span></span></i></a></div><div><span>讲述：黄申</span><span>大小：14.59M</span><span> 时长：15:58</span></div></div><audio title="24 | 语言模型：如何使用链式法则和马尔科夫假设简化概率模型？" src="https://res001.geekbang.org//media/audio/20/fc/207b007240e15cf081d84c73e79f67fc/ld/ld.m3u8"></audio></div><div><div><div><div data-slate-editor="true" data-key="8779" autocorrect="off" spellcheck="false" data-gramm="false"><div data-slate-type="paragraph" data-slate-object="block" data-key="8780"><span data-slate-object="text" data-key="8781"><span data-slate-leaf="true" data-offset-key="8781:0" data-first-offset="true"><span data-slate-string="true">你好，我是黄申。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8782"><span data-slate-object="text" data-key="8783"><span data-slate-leaf="true" data-offset-key="8783:0" data-first-offset="true"><span data-slate-string="true">之前我给你介绍了用于分类的朴素贝叶斯算法。我们讲了，朴素贝叶斯算法可以利用贝叶斯定理和变量之间的独立性，预测一篇文章属于某个分类的概率。除了朴素贝叶斯分类，概率的知识还广泛地运用在其他机器学习算法中，例如语言模型、马尔科夫模型、决策树等等。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8784"><span data-slate-object="text" data-key="8785"><span data-slate-leaf="true" data-offset-key="8785:0" data-first-offset="true"><span data-slate-string="true">今天我就来说说，基于概率和统计的语言模型。语言模型在不同的领域、不同的学派都有不同的定义和实现，因此为了避免歧义，我这里先说明一下，我们谈到的语言模型，都是指基于概率和统计的模型。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="8786" id="sr-toc-1"><span data-slate-object="text" data-key="8787"><span data-slate-leaf="true" data-offset-key="8787:0" data-first-offset="true"><span data-slate-string="true">语言模型是什么？</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="8788"><span data-slate-object="text" data-key="8789"><span data-slate-leaf="true" data-offset-key="8789:0" data-first-offset="true"><span data-slate-string="true">在解释语言模型之前，我们先来看两个重要的概念。第一个是链式法则，第二个是马尔科夫假设及其对应的多元文法模型。为什么要先说这两个概念呢？这是因为链式法则可以把联合概率转化为条件概率，而马尔科夫假设通过变量间的独立性来减少条件概率中的随机变量，两者结合就可以大幅简化计算的复杂度。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="8790" id="sr-toc-2"><span data-slate-object="text" data-key="8791"><span data-slate-leaf="true" data-offset-key="8791:0" data-first-offset="true"><span data-slate-string="true">1. 链式法则</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="8792"><span data-slate-object="text" data-key="8793"><span data-slate-leaf="true" data-offset-key="8793:0" data-first-offset="true"><span data-slate-string="true">链式法则是概率论中一个常用法则。它使用一系列条件概率和边缘概率，来推导联合概率，我用一个公式来给你看看它的具体表现形式。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="8794"><div class="sr-rd-content-center"><img class="" src="https://static001.geekbang.org/resource/image/de/0c/de5d37d9392a18225b4f9d522b4f180c.png?wh=554*42"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8795"><span data-slate-object="text" data-key="8796"><span data-slate-leaf="true" data-offset-key="8796:0" data-first-offset="true"><span data-slate-string="true">其中，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8797"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="8799"><span data-slate-leaf="true" data-offset-key="8799:0" data-first-offset="true"><span data-slate-string="true"> 到 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8800"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="8802"><span data-slate-leaf="true" data-offset-key="8802:0" data-first-offset="true"><span data-slate-string="true"> 表示了 n 个随机变量。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8803"><span data-slate-object="text" data-key="8804"><span data-slate-leaf="true" data-offset-key="8804:0" data-first-offset="true"><span data-slate-string="true">这个公式是怎么来的呢？你还记得联合概率、条件概率和边缘概率之间的 “三角” 关系吗？我们用这三者的关系来推导一下，最终我们可以得到链式法则。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="8805"><div class="sr-rd-content-center"><img class="" src="https://static001.geekbang.org/resource/image/35/a2/35bbc4bd80d8fba7bd6a63c1cdbd86a2.png?wh=475*149"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8806"><span data-slate-object="text" data-key="8807"><span data-slate-leaf="true" data-offset-key="8807:0" data-first-offset="true"><span data-slate-string="true">推导的每一步，都是使用了三种概率之间的关系，这个应该不难理解。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="8808" id="sr-toc-3"><span data-slate-object="text" data-key="8809"><span data-slate-leaf="true" data-offset-key="8809:0" data-first-offset="true"><span data-slate-string="true">2. 马尔科夫假设</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="8810"><span data-slate-object="text" data-key="8811"><span data-slate-leaf="true" data-offset-key="8811:0" data-first-offset="true"><span data-slate-string="true">理解了链式法则，我们再来看看马尔可夫假设。这个假设的内容是：任何一个词 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8812"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="8814"><span data-slate-leaf="true" data-offset-key="8814:0" data-first-offset="true"><span data-slate-string="true"> 出现的概率只和它前面的 1 个或若干个词有关。基于这个假设，我们可以提出</span></span></span><span data-slate-object="text" data-key="8815"><span data-slate-leaf="true" data-offset-key="8815:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">多元文法（Ngram）模型</span></span></span></span><span data-slate-object="text" data-key="8816"><span data-slate-leaf="true" data-offset-key="8816:0" data-first-offset="true"><span data-slate-string="true">。Ngram 中的 “N” 很重要，它表示任何一个词出现的概率，只和它前面的 N-1 个词有关。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8817"><span data-slate-object="text" data-key="8818"><span data-slate-leaf="true" data-offset-key="8818:0" data-first-offset="true"><span data-slate-string="true">我以二元文法模型为例，来给你解释。按照刚才的说法，二元文法表示，某个单词出现的概率只和它前面的 1 个单词有关。也就是说，即使某个单词出现在一个很长的句子中，我们也只需要看前面那 1 个单词。用公式来表示出来就是这样：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="8819"><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/b0/e9/b0bc4bb97ba4b233e0ae9db0291059e9.png?wh=263*39"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8820"><span data-slate-object="text" data-key="8821"><span data-slate-leaf="true" data-offset-key="8821:0" data-first-offset="true"><span data-slate-string="true">如果是三元文法，就说明某个单词出现的概率只和它前面的 2 个单词有关。即使某个单词出现在很长的一个句子中，它也只看相邻的前 2 个单词。用公式来表达就是这样：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="8822"><div class="sr-rd-content-center"><img class="" src="https://static001.geekbang.org/resource/image/a6/94/a619504d5c698f0c519904cd6eddf094.png?wh=302*37"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8823"><span data-slate-object="text" data-key="8824"><span data-slate-leaf="true" data-offset-key="8824:0" data-first-offset="true"><span data-slate-string="true">你也许会好奇，那么一元文法呢？按照字面的意思，就是每个单词出现的概率和前面 0 个单词有关。这其实说明，每个词的出现都是相互独立的。用公式来表达就是这样的：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="8825"><div class="sr-rd-content-center"><img class="" src="https://static001.geekbang.org/resource/image/52/25/523ff06450418d4d91d8e6d2756e1025.png?wh=400*40"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8826"><span data-slate-object="text" data-key="8827"><span data-slate-leaf="true" data-offset-key="8827:0" data-first-offset="true"><span data-slate-string="true">弄明白链式法则和马尔科夫假设之后，我们现在来看语言模型。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8828"><span data-slate-object="text" data-key="8829"><span data-slate-leaf="true" data-offset-key="8829:0" data-first-offset="true"><span data-slate-string="true">假设我们有一个统计样本文本 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8830"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>d</span></span></span></span></span></span><span data-slate-object="text" data-key="8832"><span data-slate-leaf="true" data-offset-key="8832:0" data-first-offset="true"><span data-slate-string="true">，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8833"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>s</span></span></span></span></span></span><span data-slate-object="text" data-key="8835"><span data-slate-leaf="true" data-offset-key="8835:0" data-first-offset="true"><span data-slate-string="true"> 表示某个有意义的句子，由一连串按照特定顺序排列的词 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8836"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>，</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="8838"><span data-slate-leaf="true" data-offset-key="8838:0" data-first-offset="true"><span data-slate-string="true"> 组成，这里 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8839"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>n</span></span></span></span></span></span><span data-slate-object="text" data-key="8841"><span data-slate-leaf="true" data-offset-key="8841:0" data-first-offset="true"><span data-slate-string="true"> 是句子里单词的数量。现在，我们想知道根据文档 d 的统计数据，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8842"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>s</span></span></span></span></span></span><span data-slate-object="text" data-key="8844"><span data-slate-leaf="true" data-offset-key="8844:0" data-first-offset="true"><span data-slate-string="true"> 在文本中出现的可能性，即 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8845"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>s</span><span>∣</span><span>d</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8847"><span data-slate-leaf="true" data-offset-key="8847:0" data-first-offset="true"><span data-slate-string="true">，那么我们可以把它表示为 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8848"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>s</span><span>∣</span><span>d</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∣</span><span>d</span></span></span></span></span></span><span data-slate-object="text" data-key="8850"><span data-slate-leaf="true" data-offset-key="8850:0" data-first-offset="true"><span data-slate-string="true">)。假设我们这里考虑的都是在集合 d 的情况下发生的概率，所以可以忽略 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8851"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>d</span></span></span></span></span></span><span data-slate-object="text" data-key="8853"><span data-slate-leaf="true" data-offset-key="8853:0" data-first-offset="true"><span data-slate-string="true">，写为 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8854"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>s</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8856"><span data-slate-leaf="true" data-offset-key="8856:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8857"><span data-slate-object="text" data-key="8858"><span data-slate-leaf="true" data-offset-key="8858:0" data-first-offset="true"><span data-slate-string="true">到这里，我们碰到了第一个难题，就是如何计算 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8859"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8861"><span data-slate-leaf="true" data-offset-key="8861:0" data-first-offset="true"><span data-slate-string="true"> 要在集合中找到一模一样的句子，基本是不可能的。这个时候，我们就需要使用链式法则。我们可以把这个式子改写为：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="8862"><div class="sr-rd-content-center"><img class="" src="https://static001.geekbang.org/resource/image/2f/23/2fe51ac1be8efbad2082627860aada23.png?wh=733*40"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8863"><span data-slate-object="text" data-key="8864"><span data-slate-leaf="true" data-offset-key="8864:0" data-first-offset="true"><span data-slate-string="true">乍一看，问题似乎是解决了。因为通过文档集合 C，你可以知道 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8865"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8867"><span data-slate-leaf="true" data-offset-key="8867:0" data-first-offset="true"><span data-slate-string="true">，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8868"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∣</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8870"><span data-slate-leaf="true" data-offset-key="8870:0" data-first-offset="true"><span data-slate-string="true"> 这种概率。不过，再往后看，好像 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8871"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>3</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∣</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8873"><span data-slate-leaf="true" data-offset-key="8873:0" data-first-offset="true"><span data-slate-string="true"> 出现概率很低，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8874"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>4</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∣</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>3</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8876"><span data-slate-leaf="true" data-offset-key="8876:0" data-first-offset="true"><span data-slate-string="true"> 出现的概率就更低了。一直到 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8877"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∣</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8879"><span data-slate-leaf="true" data-offset-key="8879:0" data-first-offset="true"><span data-slate-string="true">，基本上又为 0 了。我们可以使用上一节提到的平滑技巧，减少 0 概率的出现。不过，如果太多的概率都是通过平滑的方式而得到的，那么模型和真实的数据分布之间的差距就会加大，最终预测的效果也会很差，所以平滑也不是解决 0 概率的最终办法。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8880"><span data-slate-object="text" data-key="8881"><span data-slate-leaf="true" data-offset-key="8881:0" data-first-offset="true"><span data-slate-string="true">除此之外，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8882"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span> 和</span><span> P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∣</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8884"><span data-slate-leaf="true" data-offset-key="8884:0" data-first-offset="true"><span data-slate-string="true"> 还不只会导致 0 概率，它还会使得模型存储空间的急速增加。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8885"><span data-slate-object="text" data-key="8886"><span data-slate-leaf="true" data-offset-key="8886:0" data-first-offset="true"><span data-slate-string="true">为了统计现有文档集合中 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8887"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8889"><span data-slate-leaf="true" data-offset-key="8889:0" data-first-offset="true"><span data-slate-string="true"> 这类值，我们就需要生成很多的计数器。我们假设文档集合中有 m 个不同的单词，那么从中挑出 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8890"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>n</span></span></span></span></span></span><span data-slate-object="text" data-key="8892"><span data-slate-leaf="true" data-offset-key="8892:0" data-first-offset="true"><span data-slate-string="true"> 个单词的可重复排列，数量就是 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8893"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="8895"><span data-slate-leaf="true" data-offset-key="8895:0" data-first-offset="true"><span data-slate-string="true">。此外，还有 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8896"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span><span>n</span><span>−</span><span>1</span></span></span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="8898"><span data-slate-leaf="true" data-offset-key="8898:0" data-first-offset="true"><span data-slate-string="true">, </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8899"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span><span>n</span><span>−</span><span>2</span></span></span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="8901"><span data-slate-leaf="true" data-offset-key="8901:0" data-first-offset="true"><span data-slate-string="true"> 等等。这也意味着，如果要统计并存储的所有 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8902"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8904"><span data-slate-leaf="true" data-offset-key="8904:0" data-first-offset="true"><span data-slate-string="true"> 或 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8905"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∣</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8907"><span data-slate-leaf="true" data-offset-key="8907:0" data-first-offset="true"><span data-slate-string="true"> 这类概率，就需要大量的内存和磁盘空间。当然，你可以做一些简化，不考虑单词出现的顺序，那么问题就变成了可重复组合，但是数量仍然非常巨大。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8908"><span data-slate-object="text" data-key="8909"><span data-slate-leaf="true" data-offset-key="8909:0" data-first-offset="true"><span data-slate-string="true">如何解决 0 概率和高复杂度的问题呢？马尔科夫假设和多元文法模型能帮上大忙了。如果我们使用三元文法模型，上述公式可以改写为：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="8910"><div class="sr-rd-content-center"><img class="" src="https://static001.geekbang.org/resource/image/6d/b1/6dfe1f3c267787677090a611fee964b1.png?wh=651*38"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8911"><span data-slate-object="text" data-key="8912"><span data-slate-leaf="true" data-offset-key="8912:0" data-first-offset="true"><span data-slate-string="true">这样，系统的复杂度大致在 (C (m, 1) + C (m, 2) + C (m, 3)) 这个数量级，而且 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8913"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∣</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span><span>−</span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8915"><span data-slate-leaf="true" data-offset-key="8915:0" data-first-offset="true"><span data-slate-string="true"> 为 0 的概率也会大大低于 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8916"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∣</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8918"><span data-slate-leaf="true" data-offset-key="8918:0" data-first-offset="true"><span data-slate-string="true"> （其中 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8919"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>&gt;</span></span><span><span></span><span>&gt;</span><span></span></span><span><span></span><span>3</span></span></span></span></span></span><span data-slate-object="text" data-key="8921"><span data-slate-leaf="true" data-offset-key="8921:0" data-first-offset="true"><span data-slate-string="true">）为 0 的概率。当然，多元文法模型中的 N 还是不能太大。随着 N 的增大，系统复杂度仍然会快速升高，就无法体现出多元文法的优势了。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="8922" id="sr-toc-4"><span data-slate-object="text" data-key="8923"><span data-slate-leaf="true" data-offset-key="8923:0" data-first-offset="true"><span data-slate-string="true">语言模型的应用</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="8924"><span data-slate-object="text" data-key="8925"><span data-slate-leaf="true" data-offset-key="8925:0" data-first-offset="true"><span data-slate-string="true">基于概率的语言模型，本身不是新兴的技术。它已经在机器翻译、语音识别和中文分词中得到了成功应用。近几年来，人们也开始在信息检索领域中尝试语言模型。下面我就来讲讲语言模型在信息检索和中文分词这两个方面是如何发挥作用的。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="8926" id="sr-toc-5"><span data-slate-object="text" data-key="8927"><span data-slate-leaf="true" data-offset-key="8927:0" data-first-offset="true"><span data-slate-string="true">1. 信息检索</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="8928"><span data-slate-object="text" data-key="8929"><span data-slate-leaf="true" data-offset-key="8929:0" data-first-offset="true"><span data-slate-string="true">信息检索很关心的一个问题就是相关性，也就是说，给定一个查询，哪篇文档是更相关的呢？为了解决相关性问题，布尔模型和向量空间检索模型都是从查询的角度出发，观察查询和文档之间的相似程度，并以此来决定如何找出相关的文档。这里的 “相似程度”，你可以理解为两者长得有多像。那么，语言模型如何来刻画查询和文档之间的相关度呢？</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8930"><span data-slate-object="text" data-key="8931"><span data-slate-leaf="true" data-offset-key="8931:0" data-first-offset="true"><span data-slate-string="true">它不再使用相似度定义，而是采用了概率。一种常见的做法是计算 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8932"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>d</span><span>∣</span><span>q</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8934"><span data-slate-leaf="true" data-offset-key="8934:0" data-first-offset="true"><span data-slate-string="true">，其中 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8935"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>q</span></span></span></span></span></span><span data-slate-object="text" data-key="8937"><span data-slate-leaf="true" data-offset-key="8937:0" data-first-offset="true"><span data-slate-string="true"> 表示一个查询，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8938"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>d</span></span></span></span></span></span><span data-slate-object="text" data-key="8940"><span data-slate-leaf="true" data-offset-key="8940:0" data-first-offset="true"><span data-slate-string="true"> 表示一篇文档。</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8941"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>d</span><span>∣</span><span>q</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8943"><span data-slate-leaf="true" data-offset-key="8943:0" data-first-offset="true"><span data-slate-string="true"> 表示用户输入查询 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8944"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>q</span></span></span></span></span></span><span data-slate-object="text" data-key="8946"><span data-slate-leaf="true" data-offset-key="8946:0" data-first-offset="true"><span data-slate-string="true"> 的情况下，文档 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8947"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>d</span></span></span></span></span></span><span data-slate-object="text" data-key="8949"><span data-slate-leaf="true" data-offset-key="8949:0" data-first-offset="true"><span data-slate-string="true"> 出现的概率是多少？如果这个概率越高，我们就认为 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8950"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>q</span></span></span></span></span></span><span data-slate-object="text" data-key="8952"><span data-slate-leaf="true" data-offset-key="8952:0" data-first-offset="true"><span data-slate-string="true"> 和 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8953"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>d</span></span></span></span></span></span><span data-slate-object="text" data-key="8955"><span data-slate-leaf="true" data-offset-key="8955:0" data-first-offset="true"><span data-slate-string="true"> 之间的相关性越高。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8956"><span data-slate-object="text" data-key="8957"><span data-slate-leaf="true" data-offset-key="8957:0" data-first-offset="true"><span data-slate-string="true">通过我们手头的文档集合，并不能直接获得 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8958"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>d</span><span>∣</span><span>q</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8960"><span data-slate-leaf="true" data-offset-key="8960:0" data-first-offset="true"><span data-slate-string="true">。好在我们已经学习过了贝叶斯定理，通过这个定理，我们可以将 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8961"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>d</span><span>∣</span><span>q</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8963"><span data-slate-leaf="true" data-offset-key="8963:0" data-first-offset="true"><span data-slate-string="true"> 重写如下：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="8964"><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/de/67/de8d794df4e2758063f5ffb26546c467.png?wh=197*66"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8965"><span data-slate-object="text" data-key="8966"><span data-slate-leaf="true" data-offset-key="8966:0" data-first-offset="true"><span data-slate-string="true">对于同一个查询，其出现概率 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8967"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>q</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8969"><span data-slate-leaf="true" data-offset-key="8969:0" data-first-offset="true"><span data-slate-string="true"> 都是相同的，同一个文档 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8970"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>d</span></span></span></span></span></span><span data-slate-object="text" data-key="8972"><span data-slate-leaf="true" data-offset-key="8972:0" data-first-offset="true"><span data-slate-string="true"> 的出现概率 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8973"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>d</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8975"><span data-slate-leaf="true" data-offset-key="8975:0" data-first-offset="true"><span data-slate-string="true"> 也是固定的。因此它们可以忽略，我们只要关注如何计算 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8976"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>q</span><span>∣</span><span>d</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8978"><span data-slate-leaf="true" data-offset-key="8978:0" data-first-offset="true"><span data-slate-string="true">。而语言模型，为我们解决了如何计算 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8979"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>q</span><span>∣</span><span>d</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="8981"><span data-slate-leaf="true" data-offset-key="8981:0" data-first-offset="true"><span data-slate-string="true"> 的问题，让 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8982"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="8984"><span data-slate-leaf="true" data-offset-key="8984:0" data-first-offset="true"><span data-slate-string="true"> 表示查询 q 里包含的 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8985"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>n</span></span></span></span></span></span><span data-slate-object="text" data-key="8987"><span data-slate-leaf="true" data-offset-key="8987:0" data-first-offset="true"><span data-slate-string="true"> 个关键词。那么根据之前的链式法则公式，可以重写为这样：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="8988"><div class="sr-rd-content-center"><img class="" src="https://static001.geekbang.org/resource/image/d7/66/d78ba0db38a575269eff4f6cb89afa66.png?wh=735*54"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8989"><span data-slate-object="text" data-key="8990"><span data-slate-leaf="true" data-offset-key="8990:0" data-first-offset="true"><span data-slate-string="true">为了提升效率，我们也使用马尔科夫假设和多元文法。假设是三元文法，那么我们可以写成这样：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="8991"><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/e9/13/e977fb4b682e67a8693f1b76f1febe13.png?wh=712*52"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="8992"><span data-slate-object="text" data-key="8993"><span data-slate-leaf="true" data-offset-key="8993:0" data-first-offset="true"><span data-slate-string="true">最终，当用户输入一个查询 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8994"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>q</span></span></span></span></span></span><span data-slate-object="text" data-key="8996"><span data-slate-leaf="true" data-offset-key="8996:0" data-first-offset="true"><span data-slate-string="true"> 之后，对于每一篇文档 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="8997"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>d</span></span></span></span></span></span><span data-slate-object="text" data-key="8999"><span data-slate-leaf="true" data-offset-key="8999:0" data-first-offset="true"><span data-slate-string="true">，我们都能获得 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9000"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>d</span><span>∣</span><span>q</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="9002"><span data-slate-leaf="true" data-offset-key="9002:0" data-first-offset="true"><span data-slate-string="true"> 的值。根据每篇文档所获得的 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9003"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>d</span><span>∣</span><span>q</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="9005"><span data-slate-leaf="true" data-offset-key="9005:0" data-first-offset="true"><span data-slate-string="true"> 这个值，由高到低对所有的文档进行排序。这就是语言模型在信息检索中的常见用法。</span></span></span></div><h3 data-slate-type="heading" data-slate-object="block" data-key="9006" id="sr-toc-6"><span data-slate-object="text" data-key="9007"><span data-slate-leaf="true" data-offset-key="9007:0" data-first-offset="true"><span data-slate-string="true">2. 中文分词</span></span></span></h3><div data-slate-type="paragraph" data-slate-object="block" data-key="9008"><span data-slate-object="text" data-key="9009"><span data-slate-leaf="true" data-offset-key="9009:0" data-first-offset="true"><span data-slate-string="true">和拉丁语系不同，中文存在分词的问题。如果想进行分词，你就可以使用语言模型。我举个例子给你解释一下，你就明白了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9010"><span data-slate-object="text" data-key="9011"><span data-slate-leaf="true" data-offset-key="9011:0" data-first-offset="true"><span data-slate-string="true">最普遍的分词方法之一是基于常用词的词典。如果一个尚未分词的句子里发现了存在于字典里的词，我们就认为找到一个新的词，并把它切分出来。这种切分不会出现完全离谱的结果，但是无法解决某些歧义。我下面来举个例子，原句是 “兵乓球拍卖完了”。我在读的时候，会有所停顿，你就能理解分词应该如何进行。可是，仅仅从书面来看，至少有以下几种分词方式：</span></span></span></div><div data-slate-type="pre" data-slate-object="block" data-key="9012"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="9013"><span data-slate-object="text" data-key="9014"><span data-slate-leaf="true" data-offset-key="9014:0" data-first-offset="true"><span data-slate-string="true">第一种，兵乓 | 球 | 拍卖 | 完了</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="9015"><span data-slate-object="text" data-key="9016"><span data-slate-leaf="true" data-offset-key="9016:0" data-first-offset="true"><span data-slate-string="true">第二种，兵乓球 | 拍卖 | 完了</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="9017"><span data-slate-object="text" data-key="9018"><span data-slate-leaf="true" data-offset-key="9018:0" data-first-offset="true"><span data-slate-string="true">第三种，兵乓 | 球拍 | 卖完 | 了</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="9019"><span data-slate-object="text" data-key="9020"><span data-slate-leaf="true" data-offset-key="9020:0" data-first-offset="true"><span data-slate-string="true">第四种，兵乓 | 球拍 | 卖 | 完了</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9021"><span data-slate-object="text" data-key="9022"><span data-slate-leaf="true" data-offset-key="9022:0" data-first-offset="true"><span data-slate-string="true">上面分词的例子，从字面来看都是合理的，所以这种歧义无法通过这句话本身来解决。那么这种情况下，语言模型能为我们做什么呢？我们知道，语言模型是基于大量的语料来统计的，所以我们可以使用这个模型来估算，哪种情况更合理。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9023"><span data-slate-object="text" data-key="9024"><span data-slate-leaf="true" data-offset-key="9024:0" data-first-offset="true"><span data-slate-string="true">假设整个文档集合是 D，要分词的句子是 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9025"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>s</span></span></span></span></span></span><span data-slate-object="text" data-key="9027"><span data-slate-leaf="true" data-offset-key="9027:0" data-first-offset="true"><span data-slate-string="true">，分词结果为 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9028"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="9030"><span data-slate-leaf="true" data-offset-key="9030:0" data-first-offset="true"><span data-slate-string="true">, … </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9031"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="9033"><span data-slate-leaf="true" data-offset-key="9033:0" data-first-offset="true"><span data-slate-string="true">，那么我们可以求 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9034"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>s</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="9036"><span data-slate-leaf="true" data-offset-key="9036:0" data-first-offset="true"><span data-slate-string="true"> 的概率为：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="9037"><div class="sr-rd-content-center"><img class="" src="https://static001.geekbang.org/resource/image/c7/34/c74b222e54b6c2a9a6385a988896f334.png?wh=809*47"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9038"><span data-slate-object="text" data-key="9039"><span data-slate-leaf="true" data-offset-key="9039:0" data-first-offset="true"><span data-slate-string="true">请注意，在信息检索中，我们关心的是每篇文章产生一个句子（也就是查询）的概率，而这里可以是整个文档集合 D 产生一个句子的概率。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9040"><span data-slate-object="text" data-key="9041"><span data-slate-leaf="true" data-offset-key="9041:0" data-first-offset="true"><span data-slate-string="true">根据链式法则和三元文法模型，那么上面的式子可以重写为：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="9042"><div class="sr-rd-content-center"><img class="" src="https://static001.geekbang.org/resource/image/d4/be/d4cfe338d5aed842cd1b366008a551be.png?wh=784*44"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9043"><span data-slate-object="text" data-key="9044"><span data-slate-leaf="true" data-offset-key="9044:0" data-first-offset="true"><span data-slate-string="true">也就是说，语言模型可以帮我们估计某种分词结果，在文档集合中出现的概率。但是由于不同的分词方法，会导致 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9045"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="9047"><span data-slate-leaf="true" data-offset-key="9047:0" data-first-offset="true"><span data-slate-string="true"> 到 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9048"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="9050"><span data-slate-leaf="true" data-offset-key="9050:0" data-first-offset="true"><span data-slate-string="true"> 的不同，因此就会产生不同的 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9051"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>s</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="9053"><span data-slate-leaf="true" data-offset-key="9053:0" data-first-offset="true"><span data-slate-string="true">。接下来，我们只要取最大的 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9054"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>s</span><span>)</span></span></span></span></span></span><span data-slate-object="text" data-key="9056"><span data-slate-leaf="true" data-offset-key="9056:0" data-first-offset="true"><span data-slate-string="true">，并假设这种分词方式是最合理的，就可以在一定程度上解决歧义。我们可以使用这个公式来求解：</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="9057"><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/1c/dc/1cfeaeb018f38f86c448b57b61525adc.png?wh=152*42"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9058"><span data-slate-object="text" data-key="9059"><span data-slate-leaf="true" data-offset-key="9059:0" data-first-offset="true"><span data-slate-string="true">其中，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9060"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="9062"><span data-slate-leaf="true" data-offset-key="9062:0" data-first-offset="true"><span data-slate-string="true"> 表示第 </span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="9063"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span>i</span></span></span></span></span></span><span data-slate-object="text" data-key="9065"><span data-slate-leaf="true" data-offset-key="9065:0" data-first-offset="true"><span data-slate-string="true"> 种分词方法。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9066"><span data-slate-object="text" data-key="9067"><span data-slate-leaf="true" data-offset-key="9067:0" data-first-offset="true"><span data-slate-string="true">回到 “兵乓球拍卖完了” 这句话，如果文档集合都是讲述的有关体育用品的销售，而不是拍卖行，那么 “兵乓 | 球拍 | 卖完 | 了” 这种分词的可能性应该更高。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="9068" id="sr-toc-7"><span data-slate-object="text" data-key="9069"><span data-slate-leaf="true" data-offset-key="9069:0" data-first-offset="true"><span data-slate-string="true">小结</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="9070"><span data-slate-object="text" data-key="9071"><span data-slate-leaf="true" data-offset-key="9071:0" data-first-offset="true"><span data-slate-string="true">这一节，我介绍了基于概率论的语言模型，以及它在信息检索和中文分词领域中的应用。这一节的公式比较多，你刚开始看可能觉得有点犯晕。不用急，我给你梳理了几个要点，你只要掌握这几个要点，依次再进行细节学习，就会事半功倍。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9072"><span data-slate-object="text" data-key="9073"><span data-slate-leaf="true" data-offset-key="9073:0" data-first-offset="true"><span data-slate-string="true">第一，使用联合概率，条件概率和边缘概率的 “三角” 关系，进行相互推导。链式法则就是很好的体现。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9074"><span data-slate-object="text" data-key="9075"><span data-slate-leaf="true" data-offset-key="9075:0" data-first-offset="true"><span data-slate-string="true">第二，使用马尔科夫假设，把受较多随机变量影响的条件概率，简化为受较少随机变量影响的条件概率，甚至是边缘概率。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9076"><span data-slate-object="text" data-key="9077"><span data-slate-leaf="true" data-offset-key="9077:0" data-first-offset="true"><span data-slate-string="true">第三，使用贝叶斯定理，通过先验概率推导后验概率。在信息检索中，给定查询的情况下推导文档的概率，就需要用到这个定理。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9078"><span data-slate-object="text" data-key="9079"><span data-slate-leaf="true" data-offset-key="9079:0" data-first-offset="true"><span data-slate-string="true">如果你记住了这几点，那么不仅能很快地理解本篇的内容，还能根据实际需求，设计出满足自己需要的语言模型。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="9080" id="sr-toc-8"><span data-slate-object="text" data-key="9081"><span data-slate-leaf="true" data-offset-key="9081:0" data-first-offset="true"><span data-slate-string="true">思考题</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="9082"><span data-slate-object="text" data-key="9083"><span data-slate-leaf="true" data-offset-key="9083:0" data-first-offset="true"><span data-slate-string="true">在中文分词的时候，我们也可以考虑文章的分类。比如，这样一句话 “使用纯净水源浇灌的大米”，正确的切分应该是：</span></span></span></div><div data-slate-type="pre" data-slate-object="block" data-key="9084"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="9085"><span data-slate-object="text" data-key="9086"><span data-slate-leaf="true" data-offset-key="9086:0" data-first-offset="true"><span data-slate-string="true">使用 | 纯净 | 水源 | 浇灌 | 的 | 大米</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9087"><span data-slate-object="text" data-key="9088"><span data-slate-leaf="true" data-offset-key="9088:0" data-first-offset="true"><span data-slate-string="true">如果我们知道这句描述来自于 “大米” 类商品，而不是 “纯净水” 类商品，那么就不会错误地切分为：</span></span></span></div><div data-slate-type="pre" data-slate-object="block" data-key="9089"><div><span></span></div><div><div data-code-line-number="1"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="9090"><span data-slate-object="text" data-key="9091"><span data-slate-leaf="true" data-offset-key="9091:0" data-first-offset="true"><span data-slate-string="true">使用 | 纯净水 | 源 | 浇灌 | 的 | 大米</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9092"><span data-slate-object="text" data-key="9093"><span data-slate-leaf="true" data-offset-key="9093:0" data-first-offset="true"><span data-slate-string="true">想想看，如何对我介绍的语言模型加以改进，把分类信息也包含进去？</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="9094"><span data-slate-object="text" data-key="9095"><span data-slate-leaf="true" data-offset-key="9095:0" data-first-offset="true"><span data-slate-type="primary" data-slate-object="mark"><span data-slate-string="true">欢迎留言和我分享，也欢迎你在留言区写下今天的学习笔记。你可以点击 “请朋友读”，把今天的内容分享给你的好友，和他一起精进。</span></span></span></span></div></div></div></div><div><div></div><div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><textarea placeholder="将学到的知识总结成笔记，方便日后快速查找及复习"></textarea></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div><div><div>确认放弃笔记？</div><div>放弃后所记笔记将不保留。</div></div><div><div>新功能上线，你的历史笔记已初始化为私密笔记，是否一键批量公开？</div><div>批量公开的笔记不会为你同步至部落</div></div><div></div></div><div><div><div>公开</div><div>同步至部落</div></div><div>取消</div><div>完成</div></div><div><span>0/2000</span></div></div><div><div><span>划线</span></div><div></div><div></div><div></div><div><span>笔记</span></div><div></div><div><span>复制</span></div></div></div><div><div><div><span></span></div><p><a href="javascript:void(0);">给文章提建议</a></p></div></div><div><span>©</span> 版权归极客邦科技所有，未经许可不得传播售卖。 页面已增加防盗追踪，如有侵权极客邦将依法追究其法律责任。 </div></div><div><div><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div><div>Geek__653581f21177</div></div><div><textarea placeholder="由作者筛选后的优质留言将会公开显示，欢迎踊跃留言。" rows="16"></textarea></div><div><div>Ctrl + Enter 发表</div><div>0/2000 字符</div><div>提交留言</div></div></div><div><h2 id="sr-toc-9">精选留言 (31)</h2><ul><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/12/4e/ef/2ad3effd.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span>枫林火山</span></div></div><div>黄老师，一直没想明白多元文法里的前面 N 个词的是否有顺序。例如：大家好， 家大好 。 这 2 种情况都符合三元文法中的 P (xn|xn-2,xn-1) 的统计条件吗？
推广下 P (x1,x2,x3,x4….xn) 等于 P (xn,xn-1,xn-2,….,x4,x3,x2,x1) 吗？
百度 - 联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。我的理解联合概率的条件是可以交换顺序的。
所以从联合概率定义的角度理解是等于的， 但是从语法模型的角度理解，语法是有顺序的，那用联合概率表示对不对。
本节讲的语法模型是把一句话当作有序队列去对待的还是无序集合对待的？ 我听您的讲解是感觉是有序的，但是我理解公式从联合概率定义又感觉公式是在说一个无序的一组条件。 没法把这两者联系起来。
以下两组公式，我只能知道当使用一元文法时，二者时相等的。二元以上有点懵！老师能不能讲解下
P (x1,x2,x3,x4….xn) = P (x1)*P (x2|x1)*P (x3|x1,x2)*…….*P (xn|x1,x2,x3,x4….xn-1) 
P (xn,xn-1,xn-2,….,x4,x3,x2,x1) = P (xn)*P (xn-1|xn)*P (xn-2|xn,xn-1)*…….*P (x1|xn,xn-1,……,x2) </div><div><p>作者回复：联合概率是不考虑顺序的，而 N 元文法一般都是要考虑一点顺序的。所谓 “一点” 就如你所提到的，这是一个条件概率 P (xn|xn-2,xn-1)，顺序是指 xn-2 和 xn-1 都是在 xn 的前面出现，但是我们并不关心 xn-2 和 xn-1 之间的顺序。而另一方面，我们之前已经考虑了 P (xn-1|xn-2, xn-3)，你可以认为 xn-1 和 xn-2 之间的关系已经在这一步考虑了。
至于你说的最后一点，P (x1,x2,x3,x4….xn) 和 P (xn,xn-1,xn-2,….,x4,x3,x2,x1) 理论上应该是一致的。但是 n 稍微大点，我们就无法直接求了，所以要使用马尔科夫假设进行近似。而马尔科夫假在一定程度上考虑了文本出现的顺序，所以不同顺序的 x1，x2...xn 就会影响近似的结果，所以有 P (x1,x2,x3,x4….xn) 约等于近似结果 a，P (xn,xn-1,xn-2,….,x4,x3,x2,x1) 约等于近似结果 b，a 和 b 都是同一个理论值的近似，但是由于马尔科夫假设的原因，两个近似值不一致。</p></div><div><div>2019-04-11</div><div><div><i></i><span>3</span></div><div><i></i><span>20</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/bc/a0/97c7679b.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span></span></div></div><div>文本分类器，对给定文本进行判断。用特征词代表该文本。应该和上篇文章分类的计算有类似之处。计算每个特征词出现在该类文章的概率。然后根据权重分类？或者根据每个词的词频。
（我也很迷糊）那中文中有时词的顺序错乱也能表达一个意思。
比如，密码是 123 和 321 是密码；蹦迪坟头和坟头蹦迪。
比如（相互和互相；代替和替代）比
如，纳税可以是一个专有名词，也可以是人名，姓纳名税。还有那遇到多音字咋办？</div><div><p>作者回复：这个问题很好，确实中文比较特殊，和拉丁文不太一样。

我觉得你的问题是：中文里的歧义或者分词错误，是不是会影响分类？
你说的这几种情况，我简单分为以下几种：

分词：如果我们能知道 123 或 321 代表一个字符串，而不是单个的数字，那么就不会切分它们。再例如 “相互” 也不会切为 “相” 和 “互”。当然中文分词本身不是件容易的事情，我这里提到概率语言模型，如果语料里有相关的信息，那么可以在一定程度上提升分词效果。

同义词：如果我们能正确切分出 “相互” 和 “互相”，那么还需要把它们关联为同义词。基本的做法是使用词典。

语义：“纳税” 的问题就更复杂一些，需要计算机理解上下文关系和语义。从统计的角度而言，那还是要看语料里 “纳税” 这个词哪种情况的概率更高。

所以，自然语言处理，尤其是中文的处理，是件相当复杂也是相当有趣的事情。“词包” 模型只是最基本的模型，如果我们想优化它在分类问题上的表现，需要解决好中文分词、消除歧义、同义词 / 近义词等问题。每个问题都是值得研究，并且可以提升的。如果每个点都能得到优化，那么最终分类的效果也会得到优化。

总结一下，文本分类涉及的面很广，不仅受到分类算法的影响，还受到其他许多自然语言处理的影响。由于这个系列专栏的主题是数学，所以我这讲只能把概率和相关分类算的核心思想体现出来。如果你对自然语言处理有兴趣，我可以在加餐或者其他专栏中和你分享。</p></div><div><div>2019-02-08</div><div><div><i></i></div><div><i></i><span>7</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>qinggeouye</span></div></div><div>思考题：

首先，利用语言模型进行中文分词，计算句子 S = 使用纯净水源浇灌的大米，属于哪种分词结果 W（“使用 | 纯净 | 水源 | 浇灌 | 的 | 大米”、“使用 | 纯净水 | 源 | 浇灌 | 的 | 大米”）的概率最大？

然后，回到上节的文本分类，再计算 分词结果 W 属于哪种分类（“大米”、“纯净水”）的概率比较大？</div><div><p>作者回复：这是一种方法👌</p></div><div><div>2019-03-05</div><div><div><i></i></div><div><i></i><span>5</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span> 唯她命</span></div></div><div>已经求得 p (q|d) = p (k1,k2,k3...,kn|d) = p (k1|d) * p (k2|k1,d) * p (k3|k2,k1,d) ....
那么我们怎么求得 p (k2|k1,d) 和  p (k3|k2,k1,d) 呢</div><div><p>作者回复：在实际项目中，可以通过大量的语料来统计，比如文档 d 中，在 k1 后面出现 k2 的次数，除以 k1 出现的次数，用来近似 P (k2|k1,d)</p></div><div><div>2019-02-26</div><div><div><i></i></div><div><i></i><span>5</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/d2/66/811970de.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span>OP_未央</span></div></div><div>思考题：
可以增加类别的先验概率，P (w1,w2...wn|C)*P (C)；或者已知大米广告的条件下，通过得到的不同分词计算所属类别的概率，选择属于大米概率大的那种分词？</div><div><p>作者回复：嗯，是的。可以对不同分类构建分类器，或者增加条件概率</p></div><div><div>2019-04-04</div><div><div><i></i><span>2</span></div><div><i></i><span>4</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://thirdwx.qlogo.cn/mmopen/vi_32/SsJajvXghPMDicSuOcx54mV6L9zv4KSKM2bKY0gsUdAH3oGCWzfRv9Q9HRljic2IvHzYFpRECp8SXGWhiaqGWFTKg/132"></div></div><div><div><div><span>seleven</span></div></div><div>换句话说：其实我是想问，如何能更好的利用全文或者说全部训练集的语义信息？</div><div><p>作者回复：如果是词包模型，确实对语义没有太多的理解。可以加入一些基于文法甚至是领域知识的语义分析，不过这个和具体的应用有关系，可能不是语法模型本身能很好解决的。例如，评论中的情感分析（sentiment analysis），我们可以考虑表达情感的词在否定句式中的表达等等。</p></div><div><div>2019-02-16</div><div><div><i></i></div><div><i></i><span>3</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://thirdwx.qlogo.cn/mmopen/vi_32/SsJajvXghPMDicSuOcx54mV6L9zv4KSKM2bKY0gsUdAH3oGCWzfRv9Q9HRljic2IvHzYFpRECp8SXGWhiaqGWFTKg/132"></div></div><div><div><div><span>seleven</span></div></div><div>这篇文章和上篇文章中介绍的分类基本都是在假定文章中的词语相互独立的情况下进行的，虽然马尔科夫模型用到前面词的概率，这可以说是结合了部分上下文之间的语义关系，只使用了和前面词的语义关系，对文本分类其实效果已经很好，尤其是在语料充足的情况下可以使用机器学习的方法让分类模型自迭代优化，目前很多机器翻译就是这么干的。但是我想问下老师，针对语料不是很充足的文本集，如何使用全篇文章的语义（不只是前面词的上下文关系）来设计一个文本分类器？</div><div><p>作者回复：我想你所说的是针对分类问题对吧？那么语料不充足，是指只有少数的标注数据（文章），是吧？如果是这样，一种做法是增大 ngram 里面的 n，因为标注数据不多，增加 n 不会增加太多的存储空间。另外，也可以使用少量数据训练得到的分类器，对新的数据进行分类，然后获得一些分类结果后，人工再进行复查，把正确的结果再次纳入训练数据。</p></div><div><div>2019-02-15</div><div><div><i></i></div><div><i></i><span>3</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span> 建强</span></div></div><div>思考题：
考虑分类信息的分词模式，只要把分词概率计算公式中的文档集合 D，改为某一分类的文档集合 T，即公式改为：arg maxP (Wi|T)，在某个文档类别下，取分词概率最大的那种切分方式，具体计算时，应在同类别下的所有文档中去计算每个分词出现的概率 (即分词出现的频度)。</div><div><p>作者回复：很好的思路</p></div><div><div>2020-05-30</div><div><div><i></i></div><div><i></i><span>2</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/1d/9a/89/babe8b52.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span>A 君</span></div></div><div>👍🏻让我茅塞顿开。通过贝叶斯公式的链式法则，将由 n 个词组成的联合概率展开成一系列条件概率，再通过马尔科夫假设 Ngram 来简化条件概率的复杂性，从而计算出一篇文章中一个句子出现的概率，或者一个句子的所有分词的可能性，亦或者是在文库中输入一个关键字，最有可能检索到的文章。</div><div><div>2020-07-03</div><div><div><i></i><span></span></div><div><i></i><span>1</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span> 罗耀龙 @坐忘</span><span><div class="sr-rd-content-center"><img class="" src="https://static001.geekbang.org/resource/image/20/30/2012c563b22c76a7f1f97c22c3ddf830.png"></div></span></div></div><div>茶艺师学编程

思考题：如何对语言模型加以改进，把分类信息也包含进去？

1、用另一纬度的信息来帮忙减少不确定性…… 我想到的是用另外一组信息表达式为 y2，原来的为 y1，求 y1＊y2=0（正交），从而敲定正确分法。

2、链式法则公式中的文档 D，直接替换成 “全局函数”，即本段文字的语境，接下来就是计算在本语境下合适的分法是哪个？</div><div><p>作者回复：两种都可以尝试</p></div><div><div>2020-04-16</div><div><div><i></i></div><div><i></i><span>1</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>J.Smile</span></div></div><div>思考题：朴素贝叶斯算法，计算哪种分词方法分类为大米或者纯净水的概率是最大的，比如分词方法 A 分为大米的概率最大，分词方法 B 分为纯净水的概率最大，那么根据实际文章分类情况选择不同的分词方法。</div><div><p>作者回复：是的</p></div><div><div>2020-02-26</div><div><div><i></i></div><div><i></i><span>1</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div></div><div><div><div><span>Paul Shan</span></div></div><div>思考题
原来中文分词的是一个句子在所有语料条件下成句的最大概率分词方法。如果语料足够多，可以计数在特定文章分类下的条件概率，然后取最大条件概率的分词方法。</div><div><div>2019-09-05</div><div><div><i></i><span></span></div><div><i></i><span>1</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div></div><div><div><div><span>Paul Shan</span></div></div><div>中文分词就是选用不同的分词，然后计算每个分词成句的概率大小来对分词作优劣判断。</div><div><div>2019-09-05</div><div><div><i></i><span></span></div><div><i></i><span>1</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span> 唯她命</span></div></div><div>老师你好，p (K2 | k1,d) 指的是 K2 | k1  和   d 的联合概率
还是指的是 在满足 k1 和 d 的条件下，出现 k2 的概率？</div><div><p>作者回复：是第二种，在满足 k1 和 d 的条件下，出现 k2 的概率</p></div><div><div>2019-02-26</div><div><div><i></i></div><div><i></i><span>1</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span> 唯她命</span></div></div><div>老师，$P (w_{1}, w_{2}, …, w_{n})$ 要在集合中找到一模一样的句子，基本是不可能的

不一定要找到一模一样的句子吧   例如 abc   难道 cab 这种打乱顺序的不行吗？</div><div><p>作者回复：如果是一元文法，词包模型，确实不需要一模一样。</p></div><div><div>2019-02-26</div><div><div><i></i></div><div><i></i><span>1</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div></div><div><div><div><span>013923</span></div></div><div>学习了！谢谢！</div><div><p>作者回复：希望对你有所帮助</p></div><div><div>2022-07-28</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/1d/9a/89/babe8b52.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span>A 君</span></div></div><div>链式法则将联合概率转成了一系列条件概率相乘，而马尔科夫假设简化了多条件概率的计算复杂度，两者结合，就可以计算一个查询、一条文本在数据集 / 文档中出现的概率，不仅可以计算查询和文档的匹配度，还可以用于进行中文分词，根据数据集特性来对文本进行分词。</div><div><p>作者回复：没错👍</p></div><div><div>2021-01-13</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/19/e4/73/74dce191.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>鼠里鼠气</span></div></div><div>“二元文法表示，某个单词出现的概率只和它前面的 1 个单词有关。”  这里的 “某个单词出现的概率” 为什么是条件概率呐？怎么看的？老师</div><div><p>作者回复：这个是二元文法的定义，它研究的是在前一个单词出现的情况下，后一个单词出现的概率，所以是条件概率。举个例子，我们统计了大量的文本，发现在 “我” 这个词后面，出现 “是” 的概率为 0.1，出现 “爱” 的概率是 0.05 等等，那么 P (是 | 我) = 0.1, P (爱 | 我)=0.05</p></div><div><div>2020-11-23</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/10/61/b8/7b23f8cb.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span>本来是亚</span></div></div><div>如果将分类的信息加入，则整个过程可以看作两个阶段：
1、中文分词。通过语料库计算各种分词序列的概率
2、分词后，计算词频，从而计算使得对应类最大概率的分词方式。</div><div><p>作者回复：你说的更像是如何利用分词来帮助自动分类的任务，这里咱们说如何利用分类信息来提升分词效果</p></div><div><div>2020-04-07</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>J.Smile</span></div></div><div>要点总结：
利用马尔科夫假设近似和多元文法 - N-Gram 模型原理进行两个工程中的应用：
①信息检索 ②中文分词</div><div><p>作者回复：是的，这两个是经典的应用场景</p></div><div><div>2020-02-26</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li></ul><div> 收起评论<span></span></div></div></sr-rd-content>
                            <toc-bg><toc style="width: initial;overflow: auto!important;"class="simpread-font simpread-theme-root" data-reactid=".25"><outline class="toc-level-h1" data-reactid=".25.0" style="width: 175px;"><active data-reactid=".25.0.0" ></active><a class="toc-outline-theme-github" href="#sr-toc-0" data-reactid=".25.0.1"><span data-reactid=".25.0.1.0">24 | 语言模型：如何使用链式法则和马尔科夫假设简化概率模型？</span></a></outline><outline class="toc-level-h2" data-reactid=".25.1" style="width: 165px;"><active data-reactid=".25.1.0"></active><a class="toc-outline-theme-github" href="#sr-toc-1" data-reactid=".25.1.1"><span data-reactid=".25.1.1.0">语言模型是什么？</span></a></outline><outline class="toc-level-h3" data-reactid=".25.2" style="width: 155px;"><active data-reactid=".25.2.0"></active><a class="toc-outline-theme-github" href="#sr-toc-2" data-reactid=".25.2.1"><span data-reactid=".25.2.1.0">1. 链式法则</span></a></outline><outline class="toc-level-h3" data-reactid=".25.3" style="width: 155px;"><active data-reactid=".25.3.0"></active><pangu> </pangu><a class="toc-outline-theme-github" href="#sr-toc-3" data-reactid=".25.3.1"><span data-reactid=".25.3.1.0">2. 马尔科夫假设</span></a></outline><outline class="toc-level-h2" data-reactid=".25.4" style="width: 165px;"><active data-reactid=".25.4.0"></active><a class="toc-outline-theme-github" href="#sr-toc-4" data-reactid=".25.4.1"><span data-reactid=".25.4.1.0">语言模型的应用</span></a></outline><outline class="toc-level-h3" data-reactid=".25.5" style="width: 155px;"><active data-reactid=".25.5.0"></active><pangu> </pangu><a class="toc-outline-theme-github" href="#sr-toc-5" data-reactid=".25.5.1"><span data-reactid=".25.5.1.0">1. 信息检索</span></a></outline><outline class="toc-level-h3" data-reactid=".25.6" style="width: 155px;"><active data-reactid=".25.6.0"></active><pangu> </pangu><a class="toc-outline-theme-github" href="#sr-toc-6" data-reactid=".25.6.1"><span data-reactid=".25.6.1.0">2. 中文分词</span></a></outline><outline class="toc-level-h2" data-reactid=".25.7" style="width: 165px;"><active data-reactid=".25.7.0"></active><a class="toc-outline-theme-github" href="#sr-toc-7" data-reactid=".25.7.1"><span data-reactid=".25.7.1.0">小结</span></a></outline><outline class="toc-level-h2" data-reactid=".25.8" style="width: 165px;"><active data-reactid=".25.8.0"></active><a class="toc-outline-theme-github" href="#sr-toc-8" data-reactid=".25.8.1"><span data-reactid=".25.8.1.0">思考题</span></a></outline><outline class="toc-level-h2" data-reactid=".25.9" style="width: 165px;"><active data-reactid=".25.9.0"></active><a class="toc-outline-theme-github" href="#sr-toc-9" data-reactid=".25.9.1"><span data-reactid=".25.9.1.0">精选留言 (31)</span></a></outline></toc></toc-bg>
                            <sr-rd-footer>
                                <sr-rd-footer-group>
                                    <sr-rd-footer-line></sr-rd-footer-line>
                                    <sr-rd-footer-text>全文完</sr-rd-footer-text>
                                    <sr-rd-footer-line></sr-rd-footer-line>
                                </sr-rd-footer-group>
                                <sr-rd-footer-copywrite>
                                    <div>本文由 <a href="http://ksria.com/simpread" target="_blank">简悦 SimpRead</a> 转码，用以提升阅读体验，<a href="https://time.geekbang.org/column/article/81105" target="_blank">原文地址 </a></div>
                                </sr-rd-footer-copywrite>
                            </sr-rd-footer>
                        </sr-read>
                    </body>
                </html>