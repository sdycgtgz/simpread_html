
                <html lang="en" class="simpread-font simpread-theme-root" style=''>
                    <head>
                        <meta charset="utf-8">
                        <meta http-equiv="content-type" content="text/html; charset=UTF-8;charset=utf-8">
                        <meta http-equiv="X-UA-Compatible" content="IE=Edge">
                        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1">
                        <meta name="author" content="Kenshin"/>
                        <meta name="description" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展" />
                        <meta name="keywords" content="Chrome extension, Chrome 扩展, 阅读模式, 沉浸式阅读, 简悦, 简阅, read mode, reading mode, reader view, firefox, firefox addon, userscript, safari, opera, tampermonkey"/>
                        <meta name="thumbnail" content="https://simpread-1254315611.cos.ap-shanghai.myqcloud.com/static/introduce-2.png"/>
                        <meta property="og:title" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展"/>
                        <meta property="og:type" content="website">
                        <meta property="og:local" content="zh_CN"/>
                        <meta property="og:url" content="http://ksria.com/simpread"/>
                        <meta property="og:image" content="https://simpread-1254315611.cos.ap-shanghai.myqcloud.com/static/introduce-2.png"/>
                        <meta property="og:image:type" content="image/png"/>
                        <meta property="og:image:width" content="960"/>
                        <meta property="og:image:height" content="355"/>
                        <meta property="og:site_name" content="http://ksria.com/simpread"/>
                        <meta property="og:description" content="简悦 SimpRead - 如杂志般沉浸式阅读体验的扩展"/>
                        <style type="text/css">.simpread-font{font:300 16px/1.8 -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#333;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased}.simpread-hidden{display:none}.simpread-read-root{display:-webkit-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;margin:0;top:-1000px;left:0;width:100%;z-index:2147483646;overflow-x:hidden;opacity:0;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}.simpread-read-root-show{top:0}.simpread-read-root-hide{top:1000px}sr-read{display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;margin:20px 20%;min-width:400px;min-height:400px;text-align:center}read-process{position:fixed;top:0;left:0;height:3px;width:100%;background-color:#64b5f6;-webkit-transition:width 2s;transition:width 2s;z-index:20000}sr-rd-content-error{display:block;position:relative;margin:0;margin-bottom:30px;padding:25px;background-color:rgba(0,0,0,.05)}sr-rd-footer{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;font-size:14px}sr-rd-footer,sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}sr-rd-footer-group{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}sr-rd-footer-text{min-width:150px}sr-rd-footer-copywrite{margin:10px 0 0;color:inherit}sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}sr-rd-footer-copywrite .second{margin:10px 0}sr-rd-footer-copywrite .third a:hover{border:none!important}sr-rd-footer-copywrite .third a:first-child{margin-right:50px}sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}sr-rd-footer-copywrite .sr-icon:hover{opacity:1}sr-rd-footer-copywrite a,sr-rd-footer-copywrite a:link,sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}sr-rd-footer-copywrite a:focus,sr-rd-footer-copywrite a:hover,sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.simpread-blocks{text-decoration:none!important}.simpread-blocks *{margin:0}.simpread-blocks a{padding:0;text-decoration:none!important}.simpread-blocks img{margin:0;padding:0;border:0;background:transparent;box-shadow:none}.simpread-focus-root{display:block;position:fixed;top:0;left:0;right:0;bottom:0;background-color:hsla(0,0%,92%,.9);z-index:2147483645;opacity:0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms}.simpread-focus-highlight{position:relative;box-shadow:0 0 0 20px #fff;background-color:#fff;overflow:visible;z-index:2147483646}.sr-controlbar-bg sr-rd-crlbar,.sr-controlbar-bg sr-rd-crlbar fab{z-index:2147483647}sr-rd-crlbar.controlbar{position:fixed;right:0;bottom:0;width:100px;height:100%;opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}sr-rd-crlbar.controlbar:hover{opacity:1}sr-rd-crlbar fap *{box-sizing:border-box}@media (max-height:620px){fab{zoom:.8}}@media (max-height:783px){dialog-gp dialog-content{max-height:580px}dialog-gp dialog-footer{border-top:1px solid #e0e0e0}}.simpread-highlight-selector{outline:3px dashed #1976d2!important;cursor:pointer!important}.simpread-highlight-controlbar,.simpread-highlight-selector{background-color:#fafafa!important;opacity:.8!important;-webkit-transition:opacity .5s ease!important;transition:opacity .5s ease!important}.simpread-highlight-controlbar{position:relative!important;border:3px dashed #1976d2!important}simpread-highlight,sr-snapshot-ctlbar{position:fixed;top:0;left:0;right:0;padding:15px;height:50px;background-color:rgba(50,50,50,.9);box-shadow:0 2px 5px rgba(0,0,0,.26);box-sizing:border-box;z-index:2147483640}simpread-highlight,sr-highlight-ctl,sr-snapshot-ctlbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-highlight-ctl{margin:0 5px;width:50px;height:20px;color:#fff;background-color:#1976d2;border-radius:4px;box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12);cursor:pointer}toc-bg{position:fixed;left:0;top:0;width:50px;height:200px;font-size:medium}toc-bg:hover{z-index:3}.toc-bg-hidden{opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}.toc-bg-hidden:hover{opacity:1;z-index:3}.toc-bg-hidden:hover toc{width:180px}toc *{all:unset}toc{position:fixed;left:0;top:100px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;padding:10px;width:0;max-width:200px;max-height:500px;overflow-x:hidden;overflow-y:hidden;cursor:pointer;border:1px solid hsla(0,0%,62%,.22);-webkit-transition:width .5s;transition:width .5s}toc:hover{overflow-y:auto}toc.mini:hover{width:200px!important}toc::-webkit-scrollbar{width:3px}toc::-webkit-scrollbar-thumb{border-radius:10px;background-color:hsla(36,2%,54%,.5)}toc outline{position:relative;display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;overflow:hidden;text-overflow:ellipsis;padding:2px 0;min-height:21px;line-height:21px;text-align:left}toc outline a,toc outline a:active,toc outline a:focus,toc outline a:visited{display:block;width:100%;color:inherit;font-size:11px;text-decoration:none!important;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}toc outline a:hover{font-weight:700!important}toc outline a.toc-outline-theme-dark,toc outline a.toc-outline-theme-night{color:#fff!important}.toc-level-h1{padding-left:5px}.toc-level-h2{padding-left:15px}.toc-level-h3{padding-left:25px}.toc-level-h4{padding-left:35px}.toc-outline-active{border-left:2px solid #f44336}toc outline active{position:absolute;left:0;top:0;bottom:0;padding:0 0 0 3px;border-left:2px solid #e8e8e8}sr-kbd{background:-webkit-gradient(linear,0 0,0 100%,from(#fff785),to(#ffc542));border:1px solid #e3be23;-o-border-image:none;border-image:none;-o-border-image:initial;border-image:initial;position:absolute;left:0;padding:1px 3px 0;font-size:11px!important;font-weight:700;box-shadow:0 3px 7px 0 rgba(0,0,0,.3);overflow:hidden;border-radius:3px}.sr-kbd-a{position:relative}kbd-mapping{position:fixed;left:5px;bottom:5px;-ms-flex-flow:row;flex-flow:row;width:250px;height:500px;background-color:#fff;border:1px solid hsla(0,0%,62%,.22);box-shadow:0 2px 5px rgba(0,0,0,.26);border-radius:3px}kbd-mapping,kbd-maps{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}kbd-maps{margin:40px 0 20px;width:100%;overflow-x:auto}kbd-maps::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}kbd-maps::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}kbd-mapping kbd-map-title{position:absolute;margin:5px 0;width:100%;font-size:14px;font-weight:700}kbd-maps-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}kbd-maps-title{margin:5px 0;padding-left:53px;font-size:12px;font-weight:700}kbd-map kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fafbfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}kbd-map kbd-name{display:inline-block;text-align:right;width:50px}kbd-map kbd-desc{padding-left:3px}sharecard-bg{position:fixed;top:0;left:0;width:100%;height:100%;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(0,0,0,.4);z-index:2147483647}sharecard{max-width:450px;background-color:#64b5f6}sharecard,sharecard-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-head{margin:25px;color:#fff;border-radius:10px;box-shadow:0 2px 6px 0 rgba(0,0,0,.2),0 25px 50px 0 rgba(0,0,0,.15)}sharecard-card{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-card,sharecard-top{display:-webkit-box;display:-ms-flexbox;display:flex}sharecard-top{-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:65px;background-color:#fff;color:#878787;font-size:25px;font-weight:500;border-top-left-radius:10px;border-top-right-radius:10px}sharecard-top span.logos{display:block;width:48px;height:48px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAMAAABg3Am1AAABU1BMVEUAAAAnNJMnNZI3Q5onNJInNJMnNJInNJMnNJI8SJ0tOZY/S55EUKAoNJI6RpwoNJNIU6InNJInNJImNJI7SJwmNJJ2fLUiMJFKVaNCTJ9faK1HUaJOWKVSXaUnNJNYY6pye7cmM5JXYKhwebMjMI8mL4719fW9vb0oNZP/UlLz8/QqN5TAwMAnNJPv7+/Pz8/q6+/p6enNzc3Kysry8vMsOJXc3env7/LU1uXo29vR0dHOzs7ExMTwjo73bW37XV3Aj1TCYELl5u3n5+fW2Obn6O7f4OrZ2+g0QJkxPpgvO5bh4uvS1OTP0ePCwsJQW6ZLVqTs7fHd3d3V1dXqv79VX6lET6A1TIxXUIBSgHxWQnpelHf+WVnopkXqbC7j5Ozi4uLDw8NGUaFATJ9SgH3r6+vGyd7BxNva2trX19ejqM2gpczHx8dze7Zha67Z2dlTgH1aXQeSAAAAJnRSTlMA6ff+497Y8NL+/fv49P379sqab/BeOiX06tzVy8m/tKqpalA7G6oKj0EAAAJlSURBVEjHndNXWxpBFIDhcS2ICRLAkt4Dx4WhLk0E6R0MYoIISrWX5P9f5cwSIRC2+T1czMV5n2FnZwn2eWONUqCAv3H2Uf5Ra1hx4+0WEXtDQW0fCPYJ1EffEfIV4CSROAE4jsePoTFsNmTJF/IeIHF2lgCIn57GodlqDWXBK7IwBYatVlMWFAildPKX7I3m74Z9fsCiQChoimoFQAz04Ad2gH1n9fv9n9hgMNDr9euLWD6fLxQKxaLfb7dTSlahbFVdEPwIQtrAihZQgyKCtCagbQe3xh0QFMgy5MR11+ewYY5/qlZ7vT2xu93ULKjbFLpiUxnIIwjgKmVTLDUFXMrAi2NJWCRLIthTBo4xyOLKpwyqU6CuDCI41hFBCVdOhyLw4FgJ1skCAiyl9BSHbCorgo6VJXTru5hrVCQS8Yr5xLzX59YJSFpVFwD9U0BGC3hGdFpATgRupTGe9R9I1b1ePBvXKDyvq/O/44LT4/E4BUbSCAwj8Evq6HlnOBprx6JhJz8Gktc7xeaP9ndY+0coQvCccFBD4JW60UIY50ciLOAODAQRVOeCHm4Q3Xks6uRDY+CQ+AR4T2wMYh6+jMCIQOp78CFoj0H7EQgIuhI3dGaHCrwgADwCPjJvA372GRigCJg49FUdk3D87pq3zp4SA5zc1Zh9DxfwkpjgUg5Mv+lbeE3McC8Lpu7SA3wk2xzcqL2tN5DfIsQC8HB7UamUy6FQOpTO5QKBQDZbKnWSyUzGjdWCwaDA8+7Le4BNgm3qQGWchYh9s5hNq6wVbBlbwhZYOp3OYOA4zmgEypnM2zj8ByIdedKrH8vDAAAAAElFTkSuQmCC");zoom:.8}sharecard-content{padding:15px;max-height:500px;font-size:20px;text-align:justify;background-color:#2196f3;overflow-x:hidden;overflow-y:auto}sharecard-via{padding:10px;font-size:10px;background-color:#2196f3}sharecard-footer{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:100px;background-color:#fff;color:#878787;font-size:15px;font-weight:500;border-bottom-left-radius:10px;border-bottom-right-radius:10px}sharecard-footer,sharecard-footer div{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sharecard-footer span.qrcode{display:block;width:100px;height:100px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,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")}sharecard-control{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:0 19px;height:80px;background-color:#fff}simpread-snapshot{width:100%;height:100%;cursor:move;z-index:2147483645}simpread-snapshot,sr-mask{position:fixed;left:0;top:0}sr-mask{background-color:rgba(0,0,0,.1)}.simpread-feedback,.simpread-urlscheme{position:fixed;right:20px;bottom:20px;z-index:2147483646}simpread-feedback,simpread-urlscheme{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:20px 20px 0;width:500px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:3px;box-shadow:0 0 2px rgba(0,0,0,.12),0 2px 2px rgba(0,0,0,.26);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease}simpread-feedback *,simpread-urlscheme *{font-size:12px!important;box-sizing:border-box}simpread-feedback.active,simpread-urlscheme.active{-webkit-animation-name:srFadeInUp;animation-name:srFadeInUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback.hide,simpread-urlscheme.hide{-webkit-animation-name:srFadeInDown;animation-name:srFadeInDown;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback sr-fb-label,simpread-urlscheme sr-urls-label{width:100%}simpread-feedback sr-fb-head,simpread-urlscheme sr-urls-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin-bottom:5px;width:100%}simpread-feedback sr-fb-content,simpread-urlscheme sr-urls-content{margin-bottom:5px;width:100%}simpread-feedback sr-urls-footer,simpread-urlscheme sr-urls-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-fb-a,simpread-urlscheme sr-urls-a{color:#2163f7;cursor:pointer}simpread-feedback text-field-state,simpread-urlscheme text-field-state{border-top:none rgba(34,101,247,.8)!important;border-left:none rgba(34,101,247,.8)!important;border-right:none rgba(34,101,247,.8)!important;border-bottom:2px solid rgba(34,101,247,.8)!important}simpread-feedback switch,simpread-urlscheme switch{margin-top:0!important}@-webkit-keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@-webkit-keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}@keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}simpread-feedback sr-fb-head{font-weight:700}simpread-feedback sr-fb-content{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}simpread-feedback sr-fb-content,simpread-feedback sr-fb-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}simpread-feedback sr-fb-footer{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-close{position:absolute;right:20px;cursor:pointer;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:200}simpread-feedback sr-close:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-top:10px}simpread-feedback sr-stars i{margin-right:10px;cursor:pointer}simpread-feedback sr-stars i svg{-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}simpread-feedback sr-stars i svg:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars i.active svg{-webkit-transform:rotate(0) scale(1);transform:rotate(0) scale(1)}simpread-feedback sr-emojis{display:block;height:100px;overflow:hidden}simpread-feedback sr-emoji{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:.3s;transition:.3s}simpread-feedback sr-emoji>svg{margin:15px 0;width:70px;height:70px;-ms-flex-negative:0;flex-shrink:0}simpread-feedback sr-stars-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin:10px 0 20px}</style>
                        <style type="text/css">.simpread-theme-root{font-size:62.5%!important}sr-rd-content,sr-rd-desc,sr-rd-title{width:100%}sr-rd-title{display:-webkit-box;margin:1em 0 .5em;overflow:hidden;text-overflow:ellipsis;text-rendering:optimizelegibility;-webkit-line-clamp:3;-webkit-box-orient:vertical}sr-rd-content{text-align:left;word-break:break-word}sr-rd-desc{text-align:justify;line-height:2.4;margin:0 0 1.2em;box-sizing:border-box}sr-rd-content{font-size:25.6px;font-size:1.6rem;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:25.6px;font-size:1.6rem}sr-rd-content div,sr-rd-content p,sr-rd-content pre,sr-rd-content sr-blockquote{margin:0 0 1.2em;word-break:break-word}sr-rd-content a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-rd-content a,sr-rd-content a:link{color:inherit;font-size:inherit;font-weight:inherit;border:none}sr-rd-content a:hover{background:transparent}sr-rd-content img{margin:10px;padding:5px;max-width:100%;background:#fff;border:1px solid #bbb;box-shadow:1px 1px 3px #d4d4d4}sr-rd-content figcaption{text-align:center;font-size:14px}sr-rd-content sr-blockquote{display:block;position:relative;padding:15px 25px;text-align:left;line-height:inherit}sr-rd-content sr-blockquote:before{position:absolute}sr-rd-content sr-blockquote *{margin:0;font-size:inherit}sr-rd-content table{width:100%;margin:0 0 1.2em;word-break:keep-all;word-break:normal;overflow:auto;border:none}sr-rd-content table td,sr-rd-content table th{border:none}sr-rd-content ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-rd-content ol{list-style:decimal;margin:0;padding:0}sr-rd-content ol li,sr-rd-content ul li{font-size:inherit;list-style:disc;margin:0 0 1.2em}sr-rd-content ol li{list-style:decimal;margin-left:1.3em}sr-rd-content ol li *,sr-rd-content ul li *{margin:0;text-align:left;text-align:initial}sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em}sr-rd-content li ul{list-style:circle}sr-rd-content pre{font-family:Consolas,Monaco,Andale Mono,Source Code Pro,Liberation Mono,Courier,monospace;display:block;padding:15px;line-height:1.5;word-break:break-all;word-wrap:break-word;white-space:pre;overflow:auto}sr-rd-content pre,sr-rd-content pre *,sr-rd-content pre div{font-size:17.6px;font-size:1.1rem}sr-rd-content li pre code,sr-rd-content p pre code,sr-rd-content pre{background-color:transparent;border:none}sr-rd-content pre code{margin:0;padding:0}sr-rd-content pre code,sr-rd-content pre code *{font-size:17.6px;font-size:1.1rem}sr-rd-content pre p{margin:0;padding:0;color:inherit;font-size:inherit;line-height:inherit}sr-rd-content li code,sr-rd-content p code{margin:0 4px;padding:2px 4px;font-size:17.6px;font-size:1.1rem}sr-rd-content mark{margin:0 5px;padding:2px;background:#fffdd1;border-bottom:1px solid #ffedce}.sr-rd-content-img{width:90%;height:auto}.sr-rd-content-img-load{width:48px;height:48px;margin:0;padding:0;border-style:none;border-width:0;background-repeat:no-repeat;background-image:url(data:image/gif;base64,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)}.sr-rd-content-center{text-align:center;display:-webkit-box;-webkit-box-align:center;-webkit-box-pack:center;-webkit-box-orient:vertical}.sr-rd-content-center-small{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.sr-rd-content-center-small img{margin:0;padding:0;border:0;box-shadow:none}img.simpread-img-broken{cursor:pointer}.sr-rd-content-nobeautify{margin:0;padding:0;border:0;box-shadow:0 0 0}sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-radius:4px;box-shadow:0 1px 2px 0 rgba(60,64,67,.3),0 2px 6px 2px rgba(60,64,67,.15)}sr-rd-mult:hover{-webkit-transition:all .45s 0ms;transition:all .45s 0ms;box-shadow:1px 1px 8px rgba(0,0,0,.16)}sr-rd-mult sr-rd-mult-content{padding:0 16px;overflow:auto}sr-rd-mult sr-rd-mult-avatar,sr-rd-mult sr-rd-mult-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sr-rd-mult sr-rd-mult-avatar{-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 15px}sr-rd-mult sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}sr-rd-mult sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}sr-rd-mult sr-rd-mult-content img{max-width:80%}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-page{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%}</style>
                        <style type="text/css">sr-rd-theme-github{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:57.6px;font-size:3.6rem;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;line-height:1.225}sr-rd-content h3{font-size:38.4px;font-size:2.4rem;line-height:1.43}sr-rd-content h4{font-size:32px;font-size:2rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}sr-rd-title{font-family:PT Sans,SF UI Display,\.PingFang SC,PingFang SC,Neue Haas Grotesk Text Pro,Arial Nova,Segoe UI,Microsoft YaHei,Microsoft JhengHei,Helvetica Neue,Source Han Sans SC,Noto Sans CJK SC,Source Han Sans CN,Noto Sans SC,Source Han Sans TC,Noto Sans CJK TC,Hiragino Sans GB,sans-serif;font-size:54.4px;font-size:3.4rem;font-weight:700;line-height:1.3}sr-rd-desc{position:relative;margin:0;margin-bottom:30px;padding:25px;padding-left:56px;font-size:28.8px;font-size:1.8rem;color:#777;background-color:rgba(0,0,0,.05);box-sizing:border-box}sr-rd-desc:before{content:"\201C";position:absolute;top:-28px;left:16px;font-size:80px;font-family:Arial;color:rgba(0,0,0,.15)}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content b *,sr-rd-content strong,sr-rd-content strong * sr-rd-content b{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}</style>
                        <style type="text/css"></style>
                        <style type="text/css">@media (pointer:coarse){sr-read{margin:20px 5%!important;min-width:0!important;max-width:90%!important}sr-rd-title{margin-top:0;font-size:2.7rem}sr-rd-content sr-blockquote,sr-rd-desc{margin:10 0!important;padding:0 0 0 10px!important;width:90%;font-size:1.8rem;font-style:normal;line-height:1.7;text-align:justify}sr-rd-content{font-size:1.75rem;font-weight:300}sr-rd-content figure{margin:0;padding:0;text-align:center}sr-rd-content a,sr-rd-content a:link,sr-rd-content li code,sr-rd-content p code{font-size:inherit}sr-rd-footer{margin-top:20px}sr-blockquote,sr-blockquote *{margin:5px!important;padding:5px!important}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6,sr-rd-title{font-family:PingFang SC,Verdana,Helvetica Neue,Microsoft Yahei,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#000;font-weight:100;line-height:1.35}sr-rd-content-h1,sr-rd-content-h2,sr-rd-content-h3,sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;line-height:1.35}sr-rd-content-h1,sr-rd-content h1{font-size:1.8em}sr-rd-content-h2,sr-rd-content h2{font-size:1.6em}sr-rd-content-h3,sr-rd-content h3{font-size:1.4em}sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:1.2em}sr-rd-content-ul,sr-rd-content ul{margin-left:1.3em!important;list-style:disc}sr-rd-content-ol,sr-rd-content ol{list-style:decimal;margin-left:1.9em!important}sr-rd-content-ol ol,sr-rd-content-ol ul,sr-rd-content-ul ol,sr-rd-content-ul ul,sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em!important}sr-rd-content img{margin:0;padding:0;border:0;max-width:100%!important;height:auto;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-rd-mult{min-width:0;background-color:#fff;box-shadow:0 1px 6px rgba(32,33,36,.28);border-radius:8px}sr-rd-mult sr-rd-mult-avatar div{margin:0}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center-small{margin:7px 0!important}sr-rd-mult sr-rd-mult-avatar span{display:block}sr-rd-mult sr-rd-mult-content{padding-left:0}@media only screen and (max-device-width:1024px){.simpread-theme-root,html.simpread-theme-root{font-size:80%!important}sr-rd-mult sr-rd-mult-avatar img{width:50px;height:50px;min-width:50px;min-height:50px}toc-bg toc{width:10px!important}toc-bg:hover toc{width:auto!important}}@media only screen and (max-device-width:414px){.simpread-theme-root,html.simpread-theme-root{font-size:70%!important}sr-rd-mult sr-rd-mult-avatar img{width:30px;height:30px;min-width:30px;min-height:30px}}@media only screen and (max-device-width:320px){.simpread-theme-root,html.simpread-theme-root{font-size:90%!important}sr-rd-content p{margin-bottom:.5em}}}</style>
                        <style type="text/css">undefinedsr-rd-content *, sr-rd-content p, sr-rd-content div {}sr-rd-content pre code, sr-rd-content pre code * {}sr-rd-desc {}sr-rd-content pre {}sr-rd-title {}</style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css"></style>
                        <style type="text/css">sr-rd-content *, sr-rd-content p, sr-rd-content div {
        font-size: 15px;
    }

    .annote-perview, .annote-perview * {
        color: rgb(85, 85, 85);
        font-weight: 400;
        line-height: 1.8;
    }</style>
                        
                        
                        <script>setTimeout(()=>{const e=location.hash.replace("#id=","");let t,a=!1;const n=t=>{for(let n of t){let t;if((t=e.length>6?n.getAttribute("data-id"):n.getAttribute("data-idx"))==e){n.scrollIntoView({behavior:"smooth",block:"start",inline:"nearest"}),a=!0;break}}};e&&(0==(t=document.getElementsByClassName("sr-unread-card")).length&&(t=document.getElementsByTagName("sr-annote")),n(t),a||n(t=document.getElementsByClassName("sr-annote")))},500);</script>
                        <script>document.addEventListener("DOMContentLoaded",function(){if("localhost"==location.hostname){const t=document.getElementsByTagName("img");for(let o of t){const t=o.src;t.startsWith("http")&&(o.src=location.origin+"/proxy?url="+t)}}},!1);</script>
                        <style>toc a {font-size: inherit!important;font-weight: 300!important;}</style><script>setTimeout(()=>{const max=document.getElementsByTagName('sr-annote-note-tip').length;for(let i=0;i<max;i++){const target=document.getElementsByTagName('sr-annote-note-tip')[i],value=target.dataset.value;value&&(target.innerText=value)}},1000);</script><title>简悦 | 13 | 优化方法：更新模型参数的方法</title>
                    </head>
                    <body>
                        <sr-read style='font-family: "LXGW WenKai Screen";'>
                            <sr-rd-title>13 | 优化方法：更新模型参数的方法</sr-rd-title>
                    <sr-rd-desc style="margin: 0;padding-top: 0;padding-bottom: 0;font-style: normal;font-size: 18px;">今天这次加餐，我们就一起来看看什么是机器学习，它是怎么分类的，都有哪些常见名词。在补充了这些基础知识之后，我还会和你聊聊模型训练的本质是什么</sr-rd-desc>
                    <sr-rd-content><h1 id="sr-toc-0"> 13 | 优化方法：更新模型参数的方法</h1><div><span>方远</span> <span> 2021-11-08</span></div><div><div><div class="sr-rd-content-center"><img class="sr-rd-content-img" src="https://static001.geekbang.org/resource/image/2b/06/2ba00a1cfbec38b031152deec96bc106.jpg"></div><div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADEAAAABCAYAAAB35kaxAAAAAXNSR0IArs4c6QAAAChJREFUGFdjfPfu3X8GKBAUFASz3r9/DxNiQBbDxcamn1yzSLEDZi8A2agny86FR+IAAAAASUVORK5CYII="></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAABCAYAAAA8TpVcAAAAAXNSR0IArs4c6QAAAB1JREFUGFdjfPfu3X9BQUEGEHj//j2YBgFCYtjkAcT9D8ti7hUOAAAAAElFTkSuQmCC"></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD0AAAABCAYAAABt2qY/AAAAAXNSR0IArs4c6QAAACdJREFUGFdjfPfu3X9BQUEGEHj//j2YBgFkMULy2PQQK0YLewiZCQDhUS3LBhDf1QAAAABJRU5ErkJggg=="></div><div class="sr-rd-content-center-small"><img class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAABCAYAAAA4u0VhAAAAAXNSR0IArs4c6QAAAB1JREFUGFdjfPfu3X9BQUEGEHj//j2YBgFkMULyAF6lD8tbqYWOAAAAAElFTkSuQmCC"></div></div></div><div><div><div></div></div><div><div><div><div><div><div>00:00</div></div><div></div></div></div><a href="javascript:;"> 1.0x <i><span></span></i></a></div><div><span>讲述：方远</span><span>大小：11.13M</span><span> 时长：12:11</span></div></div><audio title="13 | 优化方法：更新模型参数的方法" src="https://res001.geekbang.org/media/audio/70/dc/70623aa546ec197aace64bfcf4c878dc/ld/ld.m3u8"></audio></div><div><div><div><div data-slate-editor="true" data-key="7263" autocorrect="off" spellcheck="false" data-gramm="false"><div data-slate-type="paragraph" data-slate-object="block" data-key="7264"><span data-slate-object="text" data-key="7265"><span data-slate-leaf="true" data-offset-key="7265:0" data-first-offset="true"><span data-slate-string="true">你好，我是方远。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7266"><span data-slate-object="text" data-key="7267"><span data-slate-leaf="true" data-offset-key="7267:0" data-first-offset="true"><span data-slate-string="true">在上节课中，我们共同了解了前馈网络、导数、梯度、反向传播等概念。但是距离真正完全了解神经网络的学习过程，我们还差一个重要的环节，那就是优化方法。只有搞懂了优化方法，才能做到真的明白反向传播的具体过程。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7268"><span data-slate-object="text" data-key="7269"><span data-slate-leaf="true" data-offset-key="7269:0" data-first-offset="true"><span data-slate-string="true">今天我们就来学习一下优化方法，为了让你建立更深入的理解，后面我还特意为你准备了一个例子，把这三节课的所有内容串联起来。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="7270" id="sr-toc-1"><span data-slate-object="text" data-key="7271"><span data-slate-leaf="true" data-offset-key="7271:0" data-first-offset="true"><span data-slate-string="true">用下山路线规划理解优化方法</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="7272"><span data-slate-object="text" data-key="7273"><span data-slate-leaf="true" data-offset-key="7273:0" data-first-offset="true"><span data-slate-string="true">深度学习，其实包括了三个最重要的核心过程：模型表示、方法评估、优化方法。我们上节课学习的内容，都是为了优化方法做铺垫。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7274"><span data-slate-object="text" data-key="7275"><span data-slate-leaf="true" data-offset-key="7275:0" data-first-offset="true"><span data-slate-string="true">优化方法，指的是一个过程，这个过程的目的就是，寻找模型在所有可能性中达到评估效果指标最好的那一个。我们举个例子，对于函数 f (x)，它包含了一组参数。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7276"><span data-slate-object="text" data-key="7277"><span data-slate-leaf="true" data-offset-key="7277:0" data-first-offset="true"><span data-slate-string="true">这个例子中，优化方法的目的就是</span></span></span><span data-slate-object="text" data-key="7278"><span data-slate-leaf="true" data-offset-key="7278:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">找到能够使得 f (x) 的值达到最小值</span></span></span></span><span data-slate-object="text" data-key="7279"><span data-slate-leaf="true" data-offset-key="7279:0" data-first-offset="true"><span data-slate-string="true">对应的权重。换句话说，优化过程就是找到一个状态，这个状态能够让模型的损失函数最小，而这个状态就是</span></span></span><span data-slate-object="text" data-key="7280"><span data-slate-leaf="true" data-offset-key="7280:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">模型的权重</span></span></span></span><span data-slate-object="text" data-key="7281"><span data-slate-leaf="true" data-offset-key="7281:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7282"><span data-slate-object="text" data-key="7283"><span data-slate-leaf="true" data-offset-key="7283:0" data-first-offset="true"><span data-slate-string="true">常见的优化方法种类非常多，常见的有梯度下降法、牛顿法、拟牛顿法等，涉及的数学知识也更是不可胜数。同样的，PyTorch 也将优化方法进行了封装，我们在实际开发中直接使用即可，节省了大量的时间和劳动。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7284"><span data-slate-object="text" data-key="7285"><span data-slate-leaf="true" data-offset-key="7285:0" data-first-offset="true"><span data-slate-string="true">不过，为了更好地理解深度学习特别是反向传播的过程，我们还是有必要对一些重要的优化方法进行了解。我们这节课要学习的梯度下降法，也是深度学习中使用最为广泛的优化方法。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7286"><span data-slate-object="text" data-key="7287"><span data-slate-leaf="true" data-offset-key="7287:0" data-first-offset="true"><span data-slate-string="true">梯度下降其实很好理解，我给你举一个生活化的例子。假期你跟朋友去爬山，到了山顶之后忽然想上厕所，需要尽快到达半山腰的卫生间，这时候你就需要规划路线，该怎么规划呢？</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7288"><span data-slate-object="text" data-key="7289"><span data-slate-leaf="true" data-offset-key="7289:0" data-first-offset="true"><span data-slate-string="true">在不考虑生命危险的情况下，那自然是怎么快怎么走了，能跳崖我们绝不走平路，也就是说：越陡峭的地方，就越有可能快速到达目的地。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7290"><span data-slate-object="text" data-key="7291"><span data-slate-leaf="true" data-offset-key="7291:0" data-first-offset="true"><span data-slate-string="true">所以，我们就有了一个送命方案：每走几步，就改变方向，这个方向就是朝着当前最陡峭的方向，即坡度下降最快的方向行走，并不断重复这个过程。这就是梯度下降的最直观的表示了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7292"><span data-slate-object="text" data-key="7293"><span data-slate-leaf="true" data-offset-key="7293:0" data-first-offset="true"><span data-slate-string="true">在上节课中我们曾说过：</span></span></span><span data-slate-object="text" data-key="7294"><span data-slate-leaf="true" data-offset-key="7294:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">梯度向量的方向即为函数值增长最快的方向，梯度的反方向则是函数减小最快的方向</span></span></span></span><span data-slate-object="text" data-key="7295"><span data-slate-leaf="true" data-offset-key="7295:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7296"><span data-slate-object="text" data-key="7297"><span data-slate-leaf="true" data-offset-key="7297:0" data-first-offset="true"><span data-slate-string="true">梯度下降，就是梯度在深度学习中最重要的用途了。下面我们用相对严谨的方式来表述梯度下降。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7298"><span data-slate-object="text" data-key="7299"><span data-slate-leaf="true" data-offset-key="7299:0" data-first-offset="true"><span data-slate-string="true">在一个多维空间中，对于任何一个曲面，我们都能够找到一个跟它相切的超平面。这个超平面上会有无数个方向（想想这是为什么？），但是这所有的方向中，肯定有一个方向是</span></span></span><span data-slate-object="text" data-key="7300"><span data-slate-leaf="true" data-offset-key="7300:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">能够使函数下降最快</span></span></span></span><span data-slate-object="text" data-key="7301"><span data-slate-leaf="true" data-offset-key="7301:0" data-first-offset="true"><span data-slate-string="true">的方向，这个方向就是梯度的反方向。每次优化的目标就是沿着这个最快下降的方向进行，就叫做梯度下降。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7302"><span data-slate-object="text" data-key="7303"><span data-slate-leaf="true" data-offset-key="7303:0" data-first-offset="true"><span data-slate-string="true">具体来说，在一个三维空间曲线中，任何一点我们都能找到一个与之相切的平面（更高维则是超平面），这个平面上就会有无穷多个方向，</span></span></span><span data-slate-object="text" data-key="7304"><span data-slate-leaf="true" data-offset-key="7304:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">但是只有一个使曲线函数下降最快的梯度</span></span></span></span><span data-slate-object="text" data-key="7305"><span data-slate-leaf="true" data-offset-key="7305:0" data-first-offset="true"><span data-slate-string="true">。再次叨叨一遍：每次优化就沿着梯度的反方向进行，就叫做梯度下降。使什么函数下降最快呢？答案就是损失函数。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7306"><span data-slate-object="text" data-key="7307"><span data-slate-leaf="true" data-offset-key="7307:0" data-first-offset="true"><span data-slate-string="true">这下你应该将几个知识点串联起来了吧：</span></span></span><span data-slate-object="text" data-key="7308"><span data-slate-leaf="true" data-offset-key="7308:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值</span></span></span></span><span data-slate-object="text" data-key="7309"><span data-slate-leaf="true" data-offset-key="7309:0" data-first-offset="true"><span data-slate-string="true">。这两节课的最终目的，就是让你牢牢记住这句话。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7310"><span data-slate-object="text" data-key="7311"><span data-slate-leaf="true" data-offset-key="7311:0" data-first-offset="true"><span data-slate-string="true">我们继续回到刚才的例子。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="7312"><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/69/3b/697af8ec29ae8fd9d54ce07f206de83b.png?wh=1920x986"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7313"><span data-slate-object="text" data-key="7314"><span data-slate-leaf="true" data-offset-key="7314:0" data-first-offset="true"><span data-slate-string="true">图中红色的线路，是一个看上去还不错的上厕所的路线。但是我们发现，还有别的路线可选。不过，下山就算是不要命地跑，也得讲究方法。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7315"><span data-slate-object="text" data-key="7316"><span data-slate-leaf="true" data-offset-key="7316:0" data-first-offset="true"><span data-slate-string="true">就比如，步子大小很重要，太大的话你可能就按照上图中的黄色路线跑了，最后跑到了别的山谷中（函数的局部极小值而非整体最小值）或者在接近和远离卫生间的来回震荡过程中，结果可想而知。但是如果步伐太小了，则需要的时间就很久，可能你还没走到目的地，就坚持不住了（蓝色路线）。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7317"><span data-slate-object="text" data-key="7318"><span data-slate-leaf="true" data-offset-key="7318:0" data-first-offset="true"><span data-slate-string="true">在算法中，这个步子的大小，叫做学习率（learning rate）。因为步长的原因，理论上我们是不可能精确地走到目的地的，而是最后在最小值的某一个范围内不断地震荡，也会存在一定的误差，不过这个误差是我们可以接受的。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7319"><span data-slate-object="text" data-key="7320"><span data-slate-leaf="true" data-offset-key="7320:0" data-first-offset="true"><span data-slate-string="true">在实际的开发中，如果损失函数在一段时间内没有什么变化，我们就认为是到达了需要的 “最低点”，就可以认为模型已经训练收敛了，从而结束训练。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="7321" id="sr-toc-2"><span data-slate-object="text" data-key="7322"><span data-slate-leaf="true" data-offset-key="7322:0" data-first-offset="true"><span data-slate-string="true">常见的梯度下降方法</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="7323"><span data-slate-object="text" data-key="7324"><span data-slate-leaf="true" data-offset-key="7324:0" data-first-offset="true"><span data-slate-string="true">我们搞清楚了梯度下降的原理之后，下面具体来看几种最常用的梯度下降优化方法。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="7325" id="sr-toc-3"><span data-slate-object="text" data-key="7326"><span data-slate-leaf="true" data-offset-key="7326:0" data-first-offset="true"><span data-slate-string="true">1. 批量梯度下降法（Batch Gradient Descent，BGD）</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="7327"><span data-slate-object="text" data-key="7328"><span data-slate-leaf="true" data-offset-key="7328:0" data-first-offset="true"><span data-slate-string="true">线性回归模型是我们最常用的函数模型之一。假设对于一个线性回归模型，y 是真实的数据分布函数，</span></span></span><span data-slate-type="inline-katex" data-slate-object="inline" data-key="7329"><span data-slate-leaf="true"><span data-slate-string="true"><span aria-hidden="true"><span><span></span><span><span>h</span><span><span><span><span><span><span></span><span><span>θ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>x</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>θ</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>θ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>…</span><span></span><span>+</span><span></span></span><span><span></span><span><span>θ</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span><span data-slate-object="text" data-key="7331"><span data-slate-leaf="true" data-offset-key="7331:0" data-first-offset="true"><span data-slate-string="true"> 是我们通过模型训练得到的函数，其中 θ 是 h 的参数，也是我们要求的权值。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7332"><span data-slate-object="text" data-key="7333"><span data-slate-leaf="true" data-offset-key="7333:0" data-first-offset="true"><span data-slate-string="true">损失函数 J (θ) 可以表述为如下公式：</span></span></span></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-slate-type="block-katex" data-slate-object="block" data-key="7334"><span><span><span aria-hidden="true"><span><span></span><span><span>c</span><span>o</span><span>s</span><span>t</span></span><span></span><span>=</span><span></span></span><span><span></span><span>J</span><span>(</span><span>θ</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>2</span><span>m</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span>m</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span><span>(</span></span><span><span>h</span><span><span><span><span><span><span></span><span><span><span>θ</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span></span></span></span></span><span><span>)</span></span></span><span></span><span>−</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span></span></span></span></span><span><span>)</span></span></span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span></span></span></span></span></span></span></span></span></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7336"><span data-slate-object="text" data-key="7337"><span data-slate-leaf="true" data-offset-key="7337:0" data-first-offset="true"><span data-slate-string="true">在这里，m 表示样本数量。既然要想损失函数的值最小，我们就要使用到梯度，还记得我们反复说的 “</span></span></span><span data-slate-object="text" data-key="7338"><span data-slate-leaf="true" data-offset-key="7338:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">梯度向量的方向即为函数值增长最快的方向</span></span></span></span><span data-slate-object="text" data-key="7339"><span data-slate-leaf="true" data-offset-key="7339:0" data-first-offset="true"><span data-slate-string="true">” 么？让损失函数以最快的速度减小，就得用梯度的反方向。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7340"><span data-slate-object="text" data-key="7341"><span data-slate-leaf="true" data-offset-key="7341:0" data-first-offset="true"><span data-slate-string="true">首先我们对 J (θ) 中的 θ 求偏导数，这样就可以得到每个 θ 对应的梯度：</span></span></span></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-slate-type="block-katex" data-slate-object="block" data-key="7342"><span><span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>∂</span><span><span>θ</span><span><span><span><span><span><span></span><span><span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span>∂</span><span>J</span><span>(</span><span>θ</span><span>)</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>−</span><span><span></span><span><span><span><span><span><span></span><span><span>m</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span>m</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span>h</span><span><span><span><span><span><span></span><span><span><span>θ</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span></span></span></span></span><span><span>)</span></span></span><span></span><span>−</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span></span></span></span></span><span><span>)</span></span></span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>j</span></span></span></span><span><span></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7344"><span data-slate-object="text" data-key="7345"><span data-slate-leaf="true" data-offset-key="7345:0" data-first-offset="true"><span data-slate-string="true">得到了每个 θ 的梯度之后，我们就可以按照下降的方向去更新每个 θ，即：</span></span></span></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-slate-type="block-katex" data-slate-object="block" data-key="7346"><span><span><span aria-hidden="true"><span><span></span><span><span>θ</span><span><span><span><span><span><span></span><span><span><span>j</span></span></span></span><span><span></span><span><span><span>′</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>θ</span><span><span><span><span><span><span></span><span><span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span>α</span><span><span></span><span><span><span><span><span><span></span><span><span>m</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span>m</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span>h</span><span><span><span><span><span><span></span><span><span><span>θ</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span></span></span></span></span><span><span>)</span></span></span><span></span><span>−</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span></span></span></span></span><span><span>)</span></span></span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>j</span></span></span></span><span><span></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7348"><span data-slate-object="text" data-key="7349"><span data-slate-leaf="true" data-offset-key="7349:0" data-first-offset="true"><span data-slate-string="true">其中 α 就是我们刚才提到的学习率。更新 θ 之后，我们就得到了一个更新之后的损失函数，它的值肯定就会更小，那么我们的模型就更加接近于真实的数据分布了。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7350"><span data-slate-object="text" data-key="7351"><span data-slate-leaf="true" data-offset-key="7351:0" data-first-offset="true"><span data-slate-string="true">在上面的公式中，你注意到了 m 这个数了吗？没错，这个方法是当所有的数据都经过了计算之后再整体除以它，即把所有样本的误差做平均。这里我想提醒你，在实际的开发中，往往有百万甚至千万数量级的样本，那这个更新的量就很恐怖了。所以就需要另一个办法，随机梯度下降法。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="7352" id="sr-toc-4"><span data-slate-object="text" data-key="7353"><span data-slate-leaf="true" data-offset-key="7353:0" data-first-offset="true"><span data-slate-string="true">2. 随机梯度下降（Stochastic Gradient Descent，SGD）</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="7354"><span data-slate-object="text" data-key="7355"><span data-slate-leaf="true" data-offset-key="7355:0" data-first-offset="true"><span data-slate-string="true">随机梯度下降法的特点是，每计算一个样本之后就要更新一次参数，这样参数更新的频率就变高了。其公式如下：</span></span></span></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-slate-type="block-katex" data-slate-object="block" data-key="7356"><span><span><span aria-hidden="true"><span><span></span><span><span>θ</span><span><span><span><span><span><span></span><span><span><span>j</span></span></span></span><span><span></span><span><span><span>′</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>θ</span><span><span><span><span><span><span></span><span><span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span>α</span><span></span><span><span><span>(</span></span><span><span>h</span><span><span><span><span><span><span></span><span><span><span>θ</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span></span></span></span></span><span><span>)</span></span></span><span></span><span>−</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span><span>i</span></span></span></span></span></span></span></span></span><span><span>)</span></span></span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>j</span></span></span></span><span><span></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7358"><span data-slate-object="text" data-key="7359"><span data-slate-leaf="true" data-offset-key="7359:0" data-first-offset="true"><span data-slate-string="true">想想看，每训练一条数据就更新一条参数，会有什么好处呢？对，有的时候，我们只需要训练集中的一部分数据，就可以实现接近于使用全部数据训练的效果，训练速度也大大提升。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7360"><span data-slate-object="text" data-key="7361"><span data-slate-leaf="true" data-offset-key="7361:0" data-first-offset="true"><span data-slate-string="true">然而，鱼和熊掌不可兼得，SGD 虽然快，也会存在一些问题。就比如，训练数据中肯定会存在一些错误样本或者噪声数据，那么在一次用到该数据的迭代中，优化的方向肯定不是朝着最理想的方向前进的，也就会导致训练效果（比如准确率）的下降。最极端的情况下，就会导致模型无法得到全局最优，而是陷入到局部最优。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7362"><span data-slate-object="text" data-key="7363"><span data-slate-leaf="true" data-offset-key="7363:0" data-first-offset="true"><span data-slate-string="true">世间安得两全法，有的时候舍弃一些东西，我们才能获得想要的。</span></span></span><span data-slate-object="text" data-key="7364"><span data-slate-leaf="true" data-offset-key="7364:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">随机梯度下降方法选择了用损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了最终总体的优化效率的提高</span></span></span></span><span data-slate-object="text" data-key="7365"><span data-slate-leaf="true" data-offset-key="7365:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7366"><span data-slate-object="text" data-key="7367"><span data-slate-leaf="true" data-offset-key="7367:0" data-first-offset="true"><span data-slate-string="true">当然这个过程中增加的迭代次数，还是要远远小于样本的数量的。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7368"><span data-slate-object="text" data-key="7369"><span data-slate-leaf="true" data-offset-key="7369:0" data-first-offset="true"><span data-slate-string="true">那如果想尽可能折衷地去协调速度和效果，该怎么办呢？我们很自然就会想到，每次不用全部的数据，也不只用一条数据，而是用 “一些” 数据，这就是接下来我们要说的小批量梯度下降。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="7370" id="sr-toc-5"><span data-slate-object="text" data-key="7371"><span data-slate-leaf="true" data-offset-key="7371:0" data-first-offset="true"><span data-slate-string="true">3. 小批量梯度下降（Mini-Batch Gradient Descent, MBGD）</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="7372"><span data-slate-object="text" data-key="7373"><span data-slate-leaf="true" data-offset-key="7373:0" data-first-offset="true"><span data-slate-string="true">Mini-batch 的方法是目前主流使用最多的一种方式，它每次使用一个固定数量的数据进行优化。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7374"><span data-slate-object="text" data-key="7375"><span data-slate-leaf="true" data-offset-key="7375:0" data-first-offset="true"><span data-slate-string="true">这个固定数量，我们称它为 batch size。batch size 较为常见的数量一般是 2 的 n 次方，比如 32、128、512 等，越小的 batch size 对应的更新速度就越快，反之则越慢，但是更新速度慢就不容易陷入局部最优。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7376"><span data-slate-object="text" data-key="7377"><span data-slate-leaf="true" data-offset-key="7377:0" data-first-offset="true"><span data-slate-string="true">其实具体的数值设成为多少，也需要根据项目的不同特点，采用经验或不断尝试的方法去进行设置，</span></span><span data-slate-leaf="true" data-offset-key="7377:1"><span data-slate-object="annotation" data-annotation-key="gkann_7961ae78" data-attr-id="41771" data-attr-name="public" data-annotation-type="public"><span data-slate-string="true">比如图像任务 batch size 我们倾向于设置得稍微小一点，NLP 任务则可以适当的大一些。</span></span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7378"><span data-slate-object="text" data-key="7379"><span data-slate-leaf="true" data-offset-key="7379:0" data-first-offset="true"><span data-slate-string="true">基于随机梯度下降法，人们又提出了包括 momentum、nesterov momentum 等方法，这部分知识同学们有兴趣点击</span></span></span><a data-slate-type="link" data-slate-object="inline" data-key="7380"><span data-slate-object="text" data-key="7381"><span data-slate-leaf="true" data-offset-key="7381:0" data-first-offset="true"><span data-slate-string="true">这里</span></span></span></a><span data-slate-object="text" data-key="7382"><span data-slate-leaf="true" data-offset-key="7382:0" data-first-offset="true"><span data-slate-string="true">可以自行查阅。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="7383" id="sr-toc-6"><span data-slate-object="text" data-key="7384"><span data-slate-leaf="true" data-offset-key="7384:0" data-first-offset="true"><span data-slate-string="true">一个简单的抽象例子</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="7385"><span data-slate-object="text" data-key="7386"><span data-slate-leaf="true" data-offset-key="7386:0" data-first-offset="true"><span data-slate-string="true">我们通过三节课（第 11 到 13 节课），分别学习了损失函数、反向传播和优化方法（梯度下降）的概念。这三个概念也是深度学习中最为重要的内容，其核心意义在于能够让模型真正做到不断学习和完善自己的表现。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7387"><span data-slate-object="text" data-key="7388"><span data-slate-leaf="true" data-offset-key="7388:0" data-first-offset="true"><span data-slate-string="true">那么接下来我们将通过一个简单的抽样例子把三节课的内容汇总起来。需要注意的是，下面的例子不是一个能够运行的例子，而是旨在让我们更加明确一个最基本的 PyTorch 训练过程都需要哪些步骤，你可以当这是一次军训。有了这个演示例子，以后我们上战场，也就是实现真正可用的例子也会事半功倍。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7389"><span data-slate-object="text" data-key="7390"><span data-slate-leaf="true" data-offset-key="7390:0" data-first-offset="true"><span data-slate-string="true">在一个模型中，我们要设置如下几个内容：</span></span></span></div><div data-slate-type="list" data-slate-object="block" data-key="7391"><div data-slate-type="list-line" data-slate-object="block" data-key="7392"><span data-slate-object="text" data-key="7393"><span data-slate-leaf="true" data-offset-key="7393:0" data-first-offset="true"><span data-slate-string="true">模型定义。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="7394"><span data-slate-object="text" data-key="7395"><span data-slate-leaf="true" data-offset-key="7395:0" data-first-offset="true"><span data-slate-string="true">损失函数定义。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="7396"><span data-slate-object="text" data-key="7397"><span data-slate-leaf="true" data-offset-key="7397:0" data-first-offset="true"><span data-slate-string="true">优化器定义。</span></span></span></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7398"><span data-slate-object="text" data-key="7399"><span data-slate-leaf="true" data-offset-key="7399:0" data-first-offset="true"><span data-slate-string="true">通过下面的代码，我们来一块了解一下，上面三个内容在实际开发中应该怎么组合。当然，这个代码是一个抽象版本，目的是帮你快速领会思路。具体的代码填充，还是要根据实际项目来修改。</span></span></span></div><div data-code-language="python" data-slate-type="pre" data-slate-object="block" data-key="7400"><div><span></span></div><div><div data-code-line-number="1"></div><div data-code-line-number="2"></div><div data-code-line-number="3"></div><div data-code-line-number="4"></div><div data-code-line-number="5"></div><div data-code-line-number="6"></div><div data-code-line-number="7"></div><div data-code-line-number="8"></div><div data-code-line-number="9"></div><div data-code-line-number="10"></div><div data-code-line-number="11"></div><div data-code-line-number="12"></div><div data-code-line-number="13"></div><div data-code-line-number="14"></div><div data-code-line-number="15"></div><div data-code-line-number="16"></div><div data-code-line-number="17"></div><div data-code-line-number="18"></div><div data-code-line-number="19"></div><div data-code-line-number="20"></div><div data-code-line-number="21"></div><div data-code-line-number="22"></div><div data-code-line-number="23"></div><div data-code-line-number="24"></div><div data-code-line-number="25"></div><div data-code-line-number="26"></div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><div data-origin="pm_code_preview"><div data-slate-type="code-line" data-slate-object="block" data-key="7401"><span data-slate-object="text" data-key="7402"><span data-slate-leaf="true" data-offset-key="7402:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">import</span></span></span></span><span data-slate-object="text" data-key="7403"><span data-slate-leaf="true" data-offset-key="7403:0" data-first-offset="true"><span data-slate-string="true"> LeNet </span></span></span><span data-slate-object="text" data-key="7404"><span data-slate-leaf="true" data-offset-key="7404:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">#假定我们使用的模型叫做 LeNet，首先导入模型的定义类</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7405"><span data-slate-object="text" data-key="7406"><span data-slate-leaf="true" data-offset-key="7406:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">import</span></span></span></span><span data-slate-object="text" data-key="7407"><span data-slate-leaf="true" data-offset-key="7407:0" data-first-offset="true"><span data-slate-string="true"> torch.optim </span></span></span><span data-slate-object="text" data-key="7408"><span data-slate-leaf="true" data-offset-key="7408:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">as</span></span></span></span><span data-slate-object="text" data-key="7409"><span data-slate-leaf="true" data-offset-key="7409:0" data-first-offset="true"><span data-slate-string="true"> optim </span></span></span><span data-slate-object="text" data-key="7410"><span data-slate-leaf="true" data-offset-key="7410:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">#引入 PyTorch 自带的可选优化函数</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7411"><span data-slate-object="text" data-key="7412"><span data-slate-leaf="true" data-offset-key="7412:0" data-first-offset="true"><span data-slate-string="true">...</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7413"><span data-slate-object="text" data-key="7414"><span data-slate-leaf="true" data-offset-key="7414:0" data-first-offset="true"><span data-slate-string="true">net = LeNet() </span></span></span><span data-slate-object="text" data-key="7415"><span data-slate-leaf="true" data-offset-key="7415:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">#声明一个 LeNet 的实例</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7416"><span data-slate-object="text" data-key="7417"><span data-slate-leaf="true" data-offset-key="7417:0" data-first-offset="true"><span data-slate-string="true">criterion = nn.CrossEntropyLoss() </span></span></span><span data-slate-object="text" data-key="7418"><span data-slate-leaf="true" data-offset-key="7418:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">#声明模型的损失函数，使用的是交叉熵损失函数</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7419"><span data-slate-object="text" data-key="7420"><span data-slate-leaf="true" data-offset-key="7420:0" data-first-offset="true"><span data-slate-string="true">optimizer = optim.SGD(net.parameters(), lr=</span></span></span><span data-slate-object="text" data-key="7421"><span data-slate-leaf="true" data-offset-key="7421:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">0.001</span></span></span></span><span data-slate-object="text" data-key="7422"><span data-slate-leaf="true" data-offset-key="7422:0" data-first-offset="true"><span data-slate-string="true">, momentum=</span></span></span><span data-slate-object="text" data-key="7423"><span data-slate-leaf="true" data-offset-key="7423:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">0.9</span></span></span></span><span data-slate-object="text" data-key="7424"><span data-slate-leaf="true" data-offset-key="7424:0" data-first-offset="true"><span data-slate-string="true">)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7425"><span data-slate-object="text" data-key="7426"><span data-slate-leaf="true" data-offset-key="7426:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 声明优化函数，我们使用的就是之前提到的 SGD，优化的参数就是 LeNet 内部的参数，lr 即为之前提到的学习率</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7427"></div><div data-slate-type="code-line" data-slate-object="block" data-key="7428"><span data-slate-object="text" data-key="7429"><span data-slate-leaf="true" data-offset-key="7429:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">#下面开始训练</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7430"><span data-slate-object="text" data-key="7431"><span data-slate-leaf="true" data-offset-key="7431:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">for</span></span></span></span><span data-slate-object="text" data-key="7432"><span data-slate-leaf="true" data-offset-key="7432:0" data-first-offset="true"><span data-slate-string="true"> epoch </span></span></span><span data-slate-object="text" data-key="7433"><span data-slate-leaf="true" data-offset-key="7433:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">in</span></span></span></span><span data-slate-object="text" data-key="7434"><span data-slate-leaf="true" data-offset-key="7434:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="7435"><span data-slate-leaf="true" data-offset-key="7435:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">range</span></span></span></span><span data-slate-object="text" data-key="7436"><span data-slate-leaf="true" data-offset-key="7436:0" data-first-offset="true"><span data-slate-string="true">(</span></span></span><span data-slate-object="text" data-key="7437"><span data-slate-leaf="true" data-offset-key="7437:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">30</span></span></span></span><span data-slate-object="text" data-key="7438"><span data-slate-leaf="true" data-offset-key="7438:0" data-first-offset="true"><span data-slate-string="true">): </span></span></span><span data-slate-object="text" data-key="7439"><span data-slate-leaf="true" data-offset-key="7439:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">#设置要在全部数据上训练的次数</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7440"><span data-slate-object="text" data-key="7441"><span data-slate-leaf="true" data-offset-key="7441:0" data-first-offset="true"><span data-slate-string="true">  </span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7442"><span data-slate-object="text" data-key="7443"><span data-slate-leaf="true" data-offset-key="7443:0" data-first-offset="true"><span data-slate-string="true">    </span></span></span><span data-slate-object="text" data-key="7444"><span data-slate-leaf="true" data-offset-key="7444:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">for</span></span></span></span><span data-slate-object="text" data-key="7445"><span data-slate-leaf="true" data-offset-key="7445:0" data-first-offset="true"><span data-slate-string="true"> i, data </span></span></span><span data-slate-object="text" data-key="7446"><span data-slate-leaf="true" data-offset-key="7446:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">in</span></span></span></span><span data-slate-object="text" data-key="7447"><span data-slate-leaf="true" data-offset-key="7447:0" data-first-offset="true"><span data-slate-string="true"> </span></span></span><span data-slate-object="text" data-key="7448"><span data-slate-leaf="true" data-offset-key="7448:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">enumerate</span></span></span></span><span data-slate-object="text" data-key="7449"><span data-slate-leaf="true" data-offset-key="7449:0" data-first-offset="true"><span data-slate-string="true">(traindata):</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7450"><span data-slate-object="text" data-key="7451"><span data-slate-leaf="true" data-offset-key="7451:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="7452"><span data-slate-leaf="true" data-offset-key="7452:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">#data 就是我们获取的一个 batch size 大小的数据</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7453"><span data-slate-object="text" data-key="7454"><span data-slate-leaf="true" data-offset-key="7454:0" data-first-offset="true"><span data-slate-string="true">  </span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7455"><span data-slate-object="text" data-key="7456"><span data-slate-leaf="true" data-offset-key="7456:0" data-first-offset="true"><span data-slate-string="true">        inputs, labels = data </span></span></span><span data-slate-object="text" data-key="7457"><span data-slate-leaf="true" data-offset-key="7457:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true">#分别得到输入的数据及其对应的类别结果</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7458"><span data-slate-object="text" data-key="7459"><span data-slate-leaf="true" data-offset-key="7459:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="7460"><span data-slate-leaf="true" data-offset-key="7460:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 首先要通过 zero_grad () 函数把梯度清零，不然 PyTorch 每次计算梯度会累加，不清零的话第二次算的梯度等于第一次加第二次的</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7461"><span data-slate-object="text" data-key="7462"><span data-slate-leaf="true" data-offset-key="7462:0" data-first-offset="true"><span data-slate-string="true">        optimizer.zero_grad()</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7463"><span data-slate-object="text" data-key="7464"><span data-slate-leaf="true" data-offset-key="7464:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="7465"><span data-slate-leaf="true" data-offset-key="7465:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 获得模型的输出结果，也即是当前模型学到的效果</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7466"><span data-slate-object="text" data-key="7467"><span data-slate-leaf="true" data-offset-key="7467:0" data-first-offset="true"><span data-slate-string="true">        outputs = net(inputs)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7468"><span data-slate-object="text" data-key="7469"><span data-slate-leaf="true" data-offset-key="7469:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="7470"><span data-slate-leaf="true" data-offset-key="7470:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 获得输出结果和数据真正类别的损失函数</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7471"><span data-slate-object="text" data-key="7472"><span data-slate-leaf="true" data-offset-key="7472:0" data-first-offset="true"><span data-slate-string="true">        loss = criterion(outputs, labels)</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7473"><span data-slate-object="text" data-key="7474"><span data-slate-leaf="true" data-offset-key="7474:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="7475"><span data-slate-leaf="true" data-offset-key="7475:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 算完 loss 之后进行反向梯度传播，这个过程之后梯度会记录在变量中</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7476"><span data-slate-object="text" data-key="7477"><span data-slate-leaf="true" data-offset-key="7477:0" data-first-offset="true"><span data-slate-string="true">        loss.backward()</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7478"><span data-slate-object="text" data-key="7479"><span data-slate-leaf="true" data-offset-key="7479:0" data-first-offset="true"><span data-slate-string="true">        </span></span></span><span data-slate-object="text" data-key="7480"><span data-slate-leaf="true" data-offset-key="7480:0" data-first-offset="true"><span data-slate-type="mark-class" data-slate-object="mark"><span data-slate-string="true"># 用计算的梯度去做优化</span></span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7481"><span data-slate-object="text" data-key="7482"><span data-slate-leaf="true" data-offset-key="7482:0" data-first-offset="true"><span data-slate-string="true">        optimizer.step()</span></span></span></div><div data-slate-type="code-line" data-slate-object="block" data-key="7483"><span data-slate-object="text" data-key="7484"><span data-slate-leaf="true" data-offset-key="7484:0" data-first-offset="true"><span data-slate-string="true">...</span></span></span></div></div></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7485"><span data-slate-object="text" data-key="7486"><span data-slate-leaf="true" data-offset-key="7486:0" data-first-offset="true"><span data-slate-string="true">这个抽象框架是不是非常清晰？我们先设置好模型、损失函数和优化函数。然后针对每一批（batch）数据，求得输出结果，接着计算损失函数值，再把这个值进行反向传播，并利用优化函数进行优化。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7487"><span data-slate-object="text" data-key="7488"><span data-slate-leaf="true" data-offset-key="7488:0" data-first-offset="true"><span data-slate-string="true">别看这个过程非常简单，但它是深度学习最根本、最关键的过程了，也是我们通过三节课学习到的最核心内容了。</span></span></span></div><h2 data-slate-type="heading" data-slate-object="block" data-key="7489" id="sr-toc-7"><span data-slate-object="text" data-key="7490"><span data-slate-leaf="true" data-offset-key="7490:0" data-first-offset="true"><span data-slate-string="true">总结</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="7491"><span data-slate-object="text" data-key="7492"><span data-slate-leaf="true" data-offset-key="7492:0" data-first-offset="true"><span data-slate-string="true">这节课，我们学习了优化方法以及梯度下降法，并通过一个例子将损失函数、反向传播、梯度下降做了串联。至此，我们就能够在给定一个模型的情况下，训练属于我们自己的深度学习模型了，恭喜你耐心看完。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7493"><span data-slate-object="text" data-key="7494"><span data-slate-leaf="true" data-offset-key="7494:0" data-first-offset="true"><span data-slate-string="true">当你想不起来梯度下降原理的时候，不妨回顾一下我们下山路线规划的例子。我们的目标就是设置合理的学习率（步伐），尽可能接近咱们的目的地（达到较理想的拟合效果）。用严谨点的表达说，就是正文里咱们反复强调的：</span></span></span><span data-slate-object="text" data-key="7495"><span data-slate-leaf="true" data-offset-key="7495:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值</span></span></span></span><span data-slate-object="text" data-key="7496"><span data-slate-leaf="true" data-offset-key="7496:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="image" data-slate-object="block" data-key="7497"><div class="sr-rd-content-center"><img class="sr-rd-content-img-load" src="https://static001.geekbang.org/resource/image/69/3b/697af8ec29ae8fd9d54ce07f206de83b.png?wh=1920x986"></div></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7498"><span data-slate-object="text" data-key="7499"><span data-slate-leaf="true" data-offset-key="7499:0" data-first-offset="true"><span data-slate-string="true">这里我再带你回顾一下这节课的要点：</span></span></span></div><div data-slate-type="list" data-slate-object="block" data-key="7500"><div data-slate-type="list-line" data-slate-object="block" data-key="7501"><span data-slate-object="text" data-key="7502"><span data-slate-leaf="true" data-offset-key="7502:0" data-first-offset="true"><span data-slate-string="true">模型之所以使用梯度下降，其实是通过优化方法不断的去修正模型和真实数据的拟合差距。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="7503"><span data-slate-object="text" data-key="7504"><span data-slate-leaf="true" data-offset-key="7504:0" data-first-offset="true"><span data-slate-string="true">常用的三种梯度方法包括批量、随机和小批量，一般来说我们更多采用</span></span></span><span data-slate-object="text" data-key="7505"><span data-slate-leaf="true" data-offset-key="7505:0" data-first-offset="true"><span data-slate-type="bold" data-slate-object="mark"><span data-slate-string="true">小批量梯度下降</span></span></span></span><span data-slate-object="text" data-key="7506"><span data-slate-leaf="true" data-offset-key="7506:0" data-first-offset="true"><span data-slate-string="true">。</span></span></span></div><div data-slate-type="list-line" data-slate-object="block" data-key="7507"><span data-slate-object="text" data-key="7508"><span data-slate-leaf="true" data-offset-key="7508:0" data-first-offset="true"><span data-slate-string="true">最后我们通过一个抽象的框架，汇总了训练一个模型所需要的几个关键内容，如损失函数、优化函数等，这部分内容是深度学习最关键的过程，建议你重点关注。</span></span></span></div></div><h2 data-slate-type="heading" data-slate-object="block" data-key="7509" id="sr-toc-8"><span data-slate-object="text" data-key="7510"><span data-slate-leaf="true" data-offset-key="7510:0" data-first-offset="true"><span data-slate-string="true">每课一练</span></span></span></h2><div data-slate-type="paragraph" data-slate-object="block" data-key="7511"><span data-slate-object="text" data-key="7512"><span data-slate-leaf="true" data-offset-key="7512:0" data-first-offset="true"><span data-slate-string="true">batch size 越大越好吗？</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7513"><span data-slate-object="text" data-key="7514"><span data-slate-leaf="true" data-offset-key="7514:0" data-first-offset="true"><span data-slate-string="true">欢迎你在留言区记录你的疑问或收获，也推荐你把这节课分享给更多的同事、朋友。</span></span></span></div><div data-slate-type="paragraph" data-slate-object="block" data-key="7515"><span data-slate-object="text" data-key="7516"><span data-slate-leaf="true" data-offset-key="7516:0" data-first-offset="true"><span data-slate-string="true">我是方远，我们下节课见！</span></span></span></div></div></div></div><div><div></div><div></div><div><div data-simplebar="init"><div><div><div></div></div><div><div><div><div><textarea placeholder="将学到的知识总结成笔记，方便日后快速查找及复习"></textarea></div></div></div></div><div></div></div><div><div></div></div><div><div></div></div></div><div><div>确认放弃笔记？</div><div>放弃后所记笔记将不保留。</div></div><div><div>新功能上线，你的历史笔记已初始化为私密笔记，是否一键批量公开？</div><div>批量公开的笔记不会为你同步至部落</div></div><div></div></div><div><div><div>公开</div><div>同步至部落</div></div><div>取消</div><div>完成</div></div><div><span>0/2000</span></div></div><div><div><span>划线</span></div><div></div><div></div><div></div><div><span>笔记</span></div><div></div><div><span>复制</span></div></div></div><div><div><div><span></span></div><p><a href="javascript:void(0);">给文章提建议</a></p></div></div><div><span>©</span> 版权归极客邦科技所有，未经许可不得传播售卖。 页面已增加防盗追踪，如有侵权极客邦将依法追究其法律责任。 </div></div><div><div><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div><div>Geek__653581f21177</div></div><div><textarea placeholder="由作者筛选后的优质留言将会公开显示，欢迎踊跃留言。" rows="16"></textarea></div><div><div>Ctrl + Enter 发表</div><div>0/2000 字符</div><div>提交留言</div></div></div><div><h2 id="sr-toc-9">精选留言 (11)</h2><ul><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="http://thirdwx.qlogo.cn/mmopen/vi_32/j24oyxHcpB5AMR9pMO6fITqnOFVOncnk2T1vdu1rYLfq1cN6Sj7xVrBVbCvHXUad2MpfyBcE4neBguxmjIxyiaQ/132"></div></div><div><div><div><span>vcjmhg</span></div></div><div>不是
1. batch_size 越大显存占用会越多，可能会造成内存溢出问题，此外由于一次读取太多的样本，可能会造成迭代速度慢的问题。
2. batch_size 较大容易使模型收敛在局部最优点
3. 此外过大的 batch_size 的可能会导致模型泛化能力较差的问题</div><div><p>作者回复: 👍🏻👍🏻👍🏻^^，厉害。</p></div><div><div>2021-11-09</div><div><div><i></i></div><div><i></i><span>8</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/17/dc/ce/03fdeb60.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span> 白色纯度</span></div></div><div>batch_size 理解：
1) Batch_size 设为 2 的幂，是为了符合 CPU、GPU 的内存要求，利于并行化处理
2) Batch_size 设置的大（视数据集而定，一般上千就算比较大了），意味着守旧派系
a) 优点是：梯度震荡小，训练结果更稳定，训练时间相对更短。
b) 缺点：泛化能力差，因为相同 epoch 的情况下，参数更新的次数减少了，增大迭代次数可以适当提高模型表现；内存可能会溢出，训练速度变慢；容易陷入局部最优（毕竟全局最优只有一个，局部最优有很多，收敛太过平稳，一旦陷入某个极小值便很难逃离）
3）batch_size 设置的小，意味着创新派
a) 更新频繁，会受到噪声数据的影响，容易逃离局部最优
b) 泛化能力往往更强，在数据标注没有大范围出错的情况下，参数更新次数频繁，更新方向整体都是对的，方向错的次数占比更低，容易被纠正

按照极限法的思路，batch_size 为 1，代表着参数更新极不稳定；batch_size 为全体数据样本量，更新非常稳定。但参数更新本就是个追逐最优的过程，遇到极值前需要稳定更新，遇到极值后需要不稳定的更新逃离马鞍面。所以衍生出 mini-batch_size 的办法。权衡之下的取舍吧。</div><div><div>2021-11-30</div><div><div><i></i><span></span></div><div><i></i><span>7</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div></div><div><div><div><span>clee</span></div></div><div>你好，最近 3 节课是理论偏多，实际动手部分偏少，那这部分内容应该怎么实际动手练习才能和理论想结合的更好，这方面有什么建议呢？</div><div><p>作者回复：你好，clee，感谢你的留言。
先恭喜你坚持到这了 ^^。在 14 节课里会把前面的知识串起来，讲讲如何自己构建一个模型，然后训练它。
在后续的实战篇里，会讲解在工业中一些 CV 与 NLP 的应用，在实战篇里都能看到这些理论知识的影子。</p></div><div><div>2021-11-08</div><div><div><i></i></div><div><i></i><span>5</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span> 马克图布</span></div></div><div>文章里说：

&gt; 越小的 batch size 对应的更新速度就越快，反之则越慢，但是更新速度慢就不容易陷入局部最优。

「更新速度慢就不容易陷入局部最优」这不是说「越大的 batch size 越不容易陷入局部最优」么？看了其他人和老师的留言被整晕了……😂</div><div><p>作者回复: hello。
batch size 太小的话，那么每个 batch 之间的差异就会很大，迭代的时候梯度震荡就会严重，不利于收敛。

batch size 越大，那么 batch 之间的差异越小，梯度震荡小，利于模型收敛。


但是凡事有个限度，如果 batch size 太大了，训练过程就会一直沿着一个方向走，从而陷入局部最优。

这也就是为什么我们要不断的尝试一个相对合理的 mini batch</p></div><div><div>2021-11-12</div><div><div><i></i></div><div><i></i><span>2</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/19/e3/d7/d7b3505f.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span> 官</span></div></div><div>并不是，batchsize 越大更新越慢，而且对显存要求变高，在配置较低的机器会出现显存不足的问题</div><div><p>作者回复: ^^，👍🏻👍🏻。是的，除此之外，较大的 batch_size 容易使模型收敛在局部最优点，特别小则容易受噪声影响。</p></div><div><div>2021-11-08</div><div><div><i></i></div><div><i></i><span>1</span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/resource/image/eb/56/eb5afbf568af2917033e5a860de0b756.png?x-oss-process=image/resize,w_28"></div></div><div><div><div><span>John (易筋)</span></div></div><div>为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值。
为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值。
为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值。
重要的事情 3 遍</div><div><div>2022-08-06</div><div><div><i></i><span></span></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>亚林</span></div></div><div>太大了，硬件受不了</div><div><div>2022-05-19</div><div><div><i></i><span></span></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="data:image/jpeg;base64,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"></div></div><div><div><div><span>人间失格</span></div></div><div>老师，请问 "迭代速度慢" 这个问题
1、随机梯度下降和批量梯度下降都是对每个样本求损失，理论上是不是计算次数一样呢，这个慢是指的参数更新慢吗，总体上是不是批量下降更快一点。
2、小批量梯度下降和随机梯度下降的 epoch 理论上次数差别大吗，随机梯度下降每个样本更新一次，epoch 会不会少一点也能达到相同的收敛效果。</div><div><p>作者回复: hello，你好。谢谢你的留言。
1. 随机梯度下降是每个样本计算之后都要更新模型参数，批量梯度则是一个 batch 之后再更新模型参数。2. 有区别的，建议采用批量梯度</p></div><div><div>2021-11-22</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/0f/63/cb/7c004188.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>和你一起搬砖的胡大爷</span></div></div><div>老师，吴恩达老师的 ml 课上说不管什么 gd 都没法确定打到全局最优，在高维数据下几乎必然只能到局部最优，所以初始化的技巧就比较重要。
老师您说的就无法得到全局最优我不大理解</div><div><p>作者回复: hello, 其实这跟 Andrew Ng 大神的说法不冲突的，咱们在这里讨论的是一种相对简单（特征没有那么多）的情况，形象化的比喻的话，还是下山的问题，高维数据相当于不是一座山，而是一片片山脉，模型在寻找最优的过程中因为步长的原因，极大概率就会进入到局部最优。这也就是为什么有时候会加入一个 momentum 的原因，让模型再冲出去到别的山谷试试。</p></div><div><div>2021-11-17</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/27/fc/ca/c1b8d9ca.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>IUniverse</span></div></div><div>batchsize 越大，更新模型的也会越慢。同时太大了也会爆显存</div><div><p>作者回复：你好，IUniverse，谢谢你的留言。
是的，
除此之外，较大的 batch_size 容易使模型收敛在局部最优点，特别小则容易受噪声影响。^^</p></div><div><div>2021-11-08</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li><li><div><div data-v-3d2acdda=""><div class="sr-rd-content-center-small"><img class="" src="https://static001.geekbang.org/account/avatar/00/2a/ce/1d/74fc7790.jpg?x-oss-process=image/resize,m_fill,h_68,w_68"></div></div><div><div><div><span>tom</span></div></div><div>batch size 过大，有可能会爆显存。。。😂</div><div><p>作者回复：你好，tom，谢谢你的留言。
是的，batch size 太大显存会爆掉。
除此之外，较大的 batch_size 容易使模型收敛在局部最优点，特别小则容易受噪声影响。^^</p></div><div><div>2021-11-08</div><div><div><i></i></div><div><i></i><span></span></div></div></div></div></div></li></ul><div> 收起评论<span></span></div></div></sr-rd-content>
                            <toc-bg><toc style="width: initial;overflow: auto!important;"class="simpread-font simpread-theme-root" data-reactid=".14"><outline class="toc-level-h1" data-reactid=".14.0" style="width: 175px;"><active data-reactid=".14.0.0" ></active><a class="toc-outline-theme-github" href="#sr-toc-0" data-reactid=".14.0.1"><span data-reactid=".14.0.1.0"> 13 | 优化方法：更新模型参数的方法</span></a></outline><outline class="toc-level-h2" data-reactid=".14.1" style="width: 165px;"><active data-reactid=".14.1.0"></active><a class="toc-outline-theme-github" href="#sr-toc-1" data-reactid=".14.1.1"><span data-reactid=".14.1.1.0">用下山路线规划理解优化方法</span></a></outline><outline class="toc-level-h2" data-reactid=".14.2" style="width: 165px;"><active data-reactid=".14.2.0"></active><a class="toc-outline-theme-github" href="#sr-toc-2" data-reactid=".14.2.1"><span data-reactid=".14.2.1.0">常见的梯度下降方法</span></a></outline><outline class="toc-level-h2" data-reactid=".14.3" style="width: 165px;"><active data-reactid=".14.3.0"></active><pangu> </pangu><a class="toc-outline-theme-github" href="#sr-toc-3" data-reactid=".14.3.1"><span data-reactid=".14.3.1.0">1. 批量梯度下降法（Batch Gradient Descent，BGD）</span></a></outline><outline class="toc-level-h2" data-reactid=".14.4" style="width: 165px;"><active data-reactid=".14.4.0"></active><a class="toc-outline-theme-github" href="#sr-toc-4" data-reactid=".14.4.1"><span data-reactid=".14.4.1.0">2. 随机梯度下降（Stochastic Gradient Descent，SGD）</span></a></outline><outline class="toc-level-h2" data-reactid=".14.5" style="width: 165px;"><active data-reactid=".14.5.0"></active><a class="toc-outline-theme-github" href="#sr-toc-5" data-reactid=".14.5.1"><span data-reactid=".14.5.1.0">3. 小批量梯度下降（Mini-Batch Gradient Descent, MBGD）</span></a></outline><outline class="toc-level-h2" data-reactid=".14.6" style="width: 165px;"><active data-reactid=".14.6.0"></active><a class="toc-outline-theme-github" href="#sr-toc-6" data-reactid=".14.6.1"><span data-reactid=".14.6.1.0">一个简单的抽象例子</span></a></outline><outline class="toc-level-h2" data-reactid=".14.7" style="width: 165px;"><active data-reactid=".14.7.0"></active><a class="toc-outline-theme-github" href="#sr-toc-7" data-reactid=".14.7.1"><span data-reactid=".14.7.1.0">总结</span></a></outline><outline class="toc-level-h2" data-reactid=".14.8" style="width: 165px;"><active data-reactid=".14.8.0"></active><a class="toc-outline-theme-github" href="#sr-toc-8" data-reactid=".14.8.1"><span data-reactid=".14.8.1.0">每课一练</span></a></outline><outline class="toc-level-h2" data-reactid=".14.9" style="width: 165px;"><active data-reactid=".14.9.0"></active><a class="toc-outline-theme-github" href="#sr-toc-9" data-reactid=".14.9.1"><span data-reactid=".14.9.1.0">精选留言 (11)</span></a></outline></toc></toc-bg>
                            <sr-rd-footer>
                                <sr-rd-footer-group>
                                    <sr-rd-footer-line></sr-rd-footer-line>
                                    <sr-rd-footer-text>全文完</sr-rd-footer-text>
                                    <sr-rd-footer-line></sr-rd-footer-line>
                                </sr-rd-footer-group>
                                <sr-rd-footer-copywrite>
                                    <div>本文由 <a href="http://ksria.com/simpread" target="_blank">简悦 SimpRead</a> 转码，用以提升阅读体验，<a href="https://time.geekbang.org/column/article/438639" target="_blank">原文地址 </a></div>
                                </sr-rd-footer-copywrite>
                            </sr-rd-footer>
                        </sr-read>
                    </body>
                </html>